{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13a7081d0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, Features, Value, DatasetDict\n",
    "\n",
    "seed = 6\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset('csv', data_files='dataset/recipes_df_r.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.dataset_dict.DatasetDict'>\n"
     ]
    }
   ],
   "source": [
    "# see datatype of df\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cooking_method': Value(dtype='string', id=None),\n",
       " 'ingredients': Value(dtype='string', id=None),\n",
       " 'recipe_name': Value(dtype='string', id=None),\n",
       " 'tags': Value(dtype='string', id=None),\n",
       " 'Vegetarian&Desserts': Value(dtype='int64', id=None),\n",
       " 'Others&D': Value(dtype='int64', id=None),\n",
       " 'Vegetarian': Value(dtype='int64', id=None),\n",
       " 'Others': Value(dtype='int64', id=None),\n",
       " 'Dairy Free': Value(dtype='int64', id=None),\n",
       " 'Gluten Free': Value(dtype='int64', id=None),\n",
       " 'Low Carb': Value(dtype='int64', id=None),\n",
       " 'Low Fat': Value(dtype='int64', id=None),\n",
       " 'Low Sodium': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cooking_method': [\"['Remove the small side muscle from the scallops, rinse with cold water and thoroughly pat dry.', 'Add the butter and oil to a 12 to 14-inch saute pan on high heat. Salt and pepper the scallops. Once the fat begins to smoke, gently add the scallops, making sure they are not touching each other. Sear the scallops for 1 1/2 minutes on each side. The scallops should have a 1/4-inch golden crust on each side while still being translucent in the center. Serve immediately.']\",\n",
       "  '[\\'With a sharp knife, slit the sausage skins lengthways and pop all the meat out. Using wet hands, roll little balls of sausage meat about the size of large marbles and set aside.\\', \"Heat a large frying pan and add a good splash of olive oil. Gently fry the sausage balls until golden brown all over, then add the pancetta and continue cooking for a couple of minutes, until it\\'s golden. While this is cooking, bring a pan of salted water to the boil, add the linguine, and cook according to the packet instructions.\", \\'In a large bowl, whip up the egg yolks, cream, half the Parmesan, the lemon zest and parsley. When the pasta is cooked, drain it in a colander, reserving a little of the cooking water, and immediately toss it quickly with the egg mixture back in the pasta pan. Add the hot sausage meatballs and toss everything together. The egg will cook delicately from the heat of the linguine, just enough for it to thicken and not scramble. The sauce should be smooth and silky. If the pasta becomes a little claggy, add a few spoonfuls of the reserved cooking water to loosen it slightly. Sprinkle over the rest of the Parmesan, season, if necessary, with the salt and pepper, and drizzle with extra virgin olive oil. Eat immediately!\\']',\n",
       "  \"['Preheat the oven to 450 degrees F.', 'Cut an X into the round end of each chestnut, place on a rimmed baking sheet, put in the oven and add 1/4 cup water to the pan. Roast for 10 minutes; the shells should peel back where cut. Remove from the oven, let cool for a minute or two, and then peel while the chestnuts are still warm. Chop coarsely.', 'Decrease the oven temperature to 350 degrees F. Spread the breadcrumbs on a baking sheet and bake until dry and golden, about 15 minutes. Put in a large bowl with the chestnuts and cornbread.', 'In a large saute pan over medium heat, melt the butter, and then add the celery, onions, and chopped liver, and cook until the vegetables are soft, about 8 minutes. Add to the bowl with the chestnuts.', 'Stir in the chicken stock, parsley, sage, salt, pepper and eggs. Spoon into a buttered 3-quart baking dish and bake, covered, for 30 minutes. Uncover and bake until browned, about 30 minutes.']\",\n",
       "  \"['Coat pan with 2 teaspoons peanut oil.', 'Combine beans, salsa, green onions, peppers, cilantro, bread crumbs, garlic, and chipotle sauce in a large bowl and mix well, mashing up the beans. Form 12 small patties (about the diameter of a silver dollar and 3/4 to 1-inch thick) and roll in tortilla breading.', 'Heat 2 teaspoons peanut oil in a large skillet over medium heat. Fry patties, in batches as necessary, until golden brown and heated through.', 'Serve on bed of salsa or cover with salsa and fresh cilantro. Add a small dollop of Peppered Lime Mayonnaise on top and garnish with a twist on lime.', 'Combine all ingredients in a bowl. Season with salt and pepper, to taste. Refrigerate if not using immediately.']\",\n",
       "  '[\\'For the buttermilk strawberry skillet cake: Preheat the oven to 375 degrees F. Butter a 10-inch cast-iron skillet and dust with flour.\\', \\'Beat the butter and 3/4 cup of the sugar in a large bowl with an electric mixer on medium speed. Beat in the vanilla and egg. Whisk together the flour, baking powder, baking soda and salt in a medium bowl until combined. With the mixer running, add about a third of the flour mixture to the butter-sugar mixture, then add about half of the buttermilk. Continue adding the flour and buttermilk in this manner, ending with the flour, mixing just until the batter comes together.\\', \\'Pour the batter into the prepared skillet and smooth the top. Place the strawberries on top of the cake batter, pressing down gently. Sprinkle the remaining 2 teaspoons sugar over top. Bake until golden brown, 35 to 40 minutes. Let cool slightly.\\', \\'For the strawberry whipped cream: Place a mixing bowl and whisk attachment into the freezer 15 minutes prior to using.\\', \\'In the bowl of an electric mixer fitted with the cold whisk attachment, whip the cream until soft peaks form, 3 to 4 minutes. Add the jam and vanilla and continue whipping until stiff peaks form, about 2 minutes more. (If not using immediately, cover and refrigerate.)\\', \"Serve the cake with the whipped cream and Jerry\\'s Sugared Pecans.\", \\'Preheat the oven to 350 degrees F.\\', \\'Line a large baking sheet with sides with aluminum foil. Pour the melted butter onto the lined sheet.\\', \\'In a large bowl, mix the sugar, cinnamon and egg whites. Add the pecan halves and toss until they are fully coated.\\', \\'Spread the pecans onto the prepared baking sheet. Bake for 30 minutes, stirring the pecans every 10 minutes. Cool on the baking sheet for 10 to 15 minutes before serving.\\']'],\n",
       " 'ingredients': [\"['1 to 1 1/4 pounds dry sea scallops, approximately 16', '2 teaspoons unsalted butter', '2 teaspoons olive oil', 'Kosher salt', 'Freshly ground black pepper']\",\n",
       "  \"['4 good-quality organic Italian sausages', 'Olive oil', '4 slices thickly cut pancetta, chopped', '455g/1 pound dried linguine', '4 large egg yolks, preferably organic', '100ml/3 1/2 fluid ounces double cream', '100g/3 1/2 ounces freshly grated Parmesan', '1 lemon, zested', 'Sprig fresh flat-leaf parsley, chopped', 'Sea salt and freshly ground black pepper', 'Extra-virgin olive oil']\",\n",
       "  \"['1 pound fresh chestnuts', '8 cups coarse fresh breadcrumbs, from crusty bread', '4 cups diced cornbread', '1 cup (2 sticks) unsalted butter, plus more for baking dish', '2 celery ribs, chopped', '1 extra-large yellow onion, chopped', '1 turkey liver, cleaned and finely chopped', '1 cup chicken or turkey stock, preferably homemade or low-sodium store bought', '1/2 cup fresh flat-leaf parsley leaves, chopped', '1/4 cup fresh sage leaves, chopped', '1 teaspoon kosher salt', '1/4 teaspoon freshly ground black pepper', '2 large eggs, lightly beaten']\",\n",
       "  '[\\'2 (15-ounce) cans black beans, drained and rinsed well\\', \\'2 tablespoons salsa, plus more for serving\\', \\'2 tablespoons finely diced green onions\\', \\'3/4 cup diced red bell peppers\\', \\'1 tablespoon chopped fresh cilantro leaves, plus more for garnish\\', \\'1 1/2 cups bread crumbs\\', \\'3 large cloves garlic\\', \"2 teaspoons bacon chipotle sauce (recommended: Bob\\'s Bacon Chipotle Sauce)\", \\'2 teaspoons peanut oil, plus more as needed\\', \\'1 1/2 cups finely crushed tortilla chips\\', \\'1 teaspoon ground cumin\\', \\'Store bought salsa\\', \\'Key Lime Peppered Mayonnaise, recipe follows\\', \\'Lime twists, for garnish\\', \\'1 cup mayonnaise\\', \\'1 1/2 teaspoons fresh key lime juice\\', \\'1 jalapeno pepper, minced\\', \\'Salt and pepper\\']',\n",
       "  '[\\'5 tablespoons salted butter, at room temperature, plus additional for the skillet\\', \\'1 1/4 cups all-purpose flour, plus additional for the skillet\\', \\'3/4 cup plus 2 teaspoons sugar\\', \\'1 1/2 teaspoons vanilla extract\\', \\'1 large egg\\', \\'1/2 teaspoon baking powder\\', \\'1/4 teaspoon baking soda\\', \\'1/4 teaspoon salt\\', \\'1/2 cup buttermilk\\', \\'2 cups strawberries, tops removed, sliced in half lengthwise\\', \\'3/4 cup cold heavy whipping cream\\', \\'1/4 cup strawberry jam\\', \\'1/2 teaspoon vanilla extract\\', \"Jerry\\'s Sugared Pecans, recipe follows, for serving\", \\'1/2 cup (1 stick) butter, melted\\', \\'1 cup sugar\\', \\'1 teaspoon ground cinnamon\\', \\'3 large egg whites\\', \\'4 cups pecan halves\\']'],\n",
       " 'recipe_name': ['Seared Scallops',\n",
       "  'Sausage Carbonara: Linguine alla Carbonara di Salsiccia',\n",
       "  'Chestnut Stuffing',\n",
       "  'Southwestern Black Bean Cakes with Salsa, Fresh Cilantro, and Key Lime Peppered Mayonnaise',\n",
       "  \"Buttermilk Strawberry Skillet Cake with Strawberry Whipped Cream and Jerry's Sugared Pecans\"],\n",
       " 'tags': ['Scallop Recipes,Shellfish Recipes,Main Dish,Lunch,Gluten Free',\n",
       "  'Italian,Pasta Recipes,Cheese,Sausage Recipes,Meat,Dairy Recipes,Main Dish,Sauteing Recipes',\n",
       "  'Thanksgiving Stuffing and Dressing,Holiday,Stuffing,Thanksgiving,Thanksgiving Side Dishes,Side Dish,Nut Recipes',\n",
       "  'American,Southwestern,Salsa,Meat,Bacon Recipes,Beans and Legumes,Nut Recipes,Fruit,Lime Recipes,Jalapeno Recipes,Vegetable,Side Dish,Appetizer,Cinco de Mayo,Holiday',\n",
       "  'Baking,Dessert,Skillet Recipes,Cake,Nut Recipes,Fruit,Strawberry,Low Sodium'],\n",
       " 'Vegetarian&Desserts': [0, 0, 0, 0, 1],\n",
       " 'Others&D': [1, 1, 1, 1, 0],\n",
       " 'Vegetarian': [0, 0, 0, 0, 0],\n",
       " 'Others': [1, 1, 1, 1, 1],\n",
       " 'Dairy Free': [0, 0, 0, 0, 0],\n",
       " 'Gluten Free': [1, 0, 0, 0, 0],\n",
       " 'Low Carb': [0, 0, 0, 0, 0],\n",
       " 'Low Fat': [0, 0, 0, 0, 0],\n",
       " 'Low Sodium': [0, 0, 0, 0, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 5 instances of dataset\n",
    "df['train'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Casting\n",
    "As we can see a few cells above, the first 4 features are already in the wanted data type: `string`. Let's cast the others into `bool` (... and make other adjustments). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cooking_method': Value(dtype='string', id=None),\n",
       " 'ingredients': Value(dtype='string', id=None),\n",
       " 'recipe_name': Value(dtype='string', id=None),\n",
       " 'tags': Value(dtype='string', id=None),\n",
       " 'Vegetarian&Desserts': Value(dtype='int64', id=None),\n",
       " 'Others&D': Value(dtype='int64', id=None),\n",
       " 'Dairy Free': Value(dtype='int64', id=None),\n",
       " 'Gluten Free': Value(dtype='int64', id=None),\n",
       " 'Low Carb': Value(dtype='int64', id=None),\n",
       " 'Low Fat': Value(dtype='int64', id=None),\n",
       " 'Low Sodium': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REMOVE columns Vegetarian, Others\n",
    "df = df.remove_columns(['Vegetarian', 'Others'])\n",
    "\n",
    "df['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583d3391ea9c4a71887775ae3fbd19c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cooking_method': Value(dtype='string', id=None),\n",
       " 'ingredients': Value(dtype='string', id=None),\n",
       " 'recipe_name': Value(dtype='string', id=None),\n",
       " 'tags': Value(dtype='string', id=None),\n",
       " 'Dairy Free': Value(dtype='int64', id=None),\n",
       " 'Gluten Free': Value(dtype='int64', id=None),\n",
       " 'Low Carb': Value(dtype='int64', id=None),\n",
       " 'Low Fat': Value(dtype='int64', id=None),\n",
       " 'Low Sodium': Value(dtype='int64', id=None),\n",
       " 'Veg': Value(dtype='int64', id=None),\n",
       " 'NonVeg': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RENAME columns Vegetarian&Dessert to Veg, Others&Dessert to NonVeg\n",
    "\n",
    "# Define a function to rename columns\n",
    "def rename_columns(example):\n",
    "    # Rename 'Vegetarian&Dessert' to 'Veg'\n",
    "    if 'Vegetarian&Desserts' in example:\n",
    "        example['Veg'] = example.pop('Vegetarian&Desserts')\n",
    "    # Rename 'Others&Dessert' to 'NonVeg'\n",
    "    if 'Others&D' in example:\n",
    "        example['NonVeg'] = example.pop('Others&D')\n",
    "    return example\n",
    "\n",
    "# Apply the rename_columns function to each example in the dataset\n",
    "for split in df.keys():\n",
    "    df[split] = df[split].map(rename_columns)\n",
    "\n",
    "df['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6293eb127214b04b1b109b6f05e03a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cooking_method': Value(dtype='string', id=None),\n",
       " 'ingredients': Value(dtype='string', id=None),\n",
       " 'recipe_name': Value(dtype='string', id=None),\n",
       " 'tags': Value(dtype='string', id=None),\n",
       " 'Dairy Free': Value(dtype='bool', id=None),\n",
       " 'Gluten Free': Value(dtype='bool', id=None),\n",
       " 'Low Carb': Value(dtype='bool', id=None),\n",
       " 'Low Fat': Value(dtype='bool', id=None),\n",
       " 'Low Sodium': Value(dtype='bool', id=None),\n",
       " 'Veg': Value(dtype='bool', id=None),\n",
       " 'NonVeg': Value(dtype='bool', id=None)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CAST variables to boolean\n",
    "df['train'] = df['train'].cast(Features({\n",
    "    'cooking_method' : Value('string'),\n",
    "    'ingredients' : Value('string'),\n",
    "    'recipe_name' : Value('string'),\n",
    "    'tags' : Value('string'),\n",
    "    'Dairy Free': Value('bool'),\n",
    "    'Gluten Free': Value('bool'),\n",
    "    'Low Carb': Value('bool'),\n",
    "    'Low Fat': Value('bool'),\n",
    "    'Low Sodium': Value('bool'),\n",
    "    'Veg': Value('bool'),\n",
    "    'NonVeg': Value('bool')\n",
    "}))\n",
    "\n",
    "df['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: stratify ?!?!?!?!?!??!?!?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n"
     ]
    }
   ],
   "source": [
    "# count how many recipes have 'cooking_method' of more than 512 tokens\n",
    "count = 0\n",
    "for i in range(len(df['train'])):\n",
    "    if len(df['train']['cooking_method'][i].split()) > 512:\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8249290e571a4110b7097c68213e0456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# drop these instances\n",
    "df['train'] = df['train'].filter(lambda x: len(x['cooking_method'].split()) <= 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# count how many recipes have 'cooking_method' of more than 512 tokens\n",
    "count = 0\n",
    "for i in range(len(df['train'])):\n",
    "    if len(df['train']['cooking_method'][i].split()) > 512:\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': (2949, 11), 'train': (5502, 11), 'validation': (1376, 11)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df['train'].train_test_split(test_size=0.3, seed = seed)\n",
    "train_validation = df['train'].train_test_split(test_size=0.2, seed = seed)\n",
    "\n",
    "dataset = DatasetDict()\n",
    "dataset['test'] = df['test']\n",
    "dataset['train'] = train_validation['train']\n",
    "dataset['validation'] = train_validation['test']\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Notice that from now on it is immediate to switch to the multi-labels case !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cooking_method', 'ingredients', 'recipe_name', 'tags', 'Dairy Free', 'Gluten Free', 'Low Carb', 'Low Fat', 'Low Sodium', 'Veg', 'NonVeg'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Veg']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for label in dataset['train'].features.keys() if label not in ['cooking_method','ingredients','recipe_name','tags','Dairy Free','Gluten Free','Low Carb','Low Fat','Low Sodium','NonVeg']]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 'Veg'}, {'Veg': 0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "\n",
    "id2label, label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = examples[\"cooking_method\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(text), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "  \n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53edbcf832f64850a311c3f7eae5b9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2949 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddebde548cc47bab5c1219161641164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34d7fbf42b948b5b693f1dda15dfd2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1376 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'labels': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = encoded_dataset['train'][0]['input_ids']\n",
    "len(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] ['Preheat the oven to 350 degrees F. Grease six 12 - ounce ramekins with 2 tablespoons of the butter and set aside on a rimmed baking sheet. ','Spread the brioche on a second rimmed baking sheet and bake, tossing halfway through, until lightly browned, about 15 minutes. ','Melt the remaining 2 tablespoons butter in a large skillet over medium - high heat. Add the sausage and squash and cook, breaking up the meat with the back of a spoon, until the sausage is browned and the squash is tender, about 8 minutes. Stir in the leeks, thyme, 1 teaspoon salt and 1 / 2 teaspoon pepper and continue to cook, stirring often, until the leeks are wilted and softened, about 5 minutes. ','Whisk together the chicken broth, cream, parsley, chives and eggs in a medium bowl. Add half the brioche to the ramekins. Divide the sausage mixture among the ramekins ; top with 1 cup of the Gruyere and the remaining brioche. Pour the egg mixture over and sprinkle with the remaining 1 cup Gruyere. ','Set aside at room temperature for 15 minutes to allow the bread to absorb the egg mixture. Bake on the baking sheet until the custard is set and the bread pudding is puffed and golden, about 20 minutes. Sprinkle with chives before serving.'] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train'][0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!! useful for multi-labels case: see which labels it has\n",
    "[id2label[idx] for idx, label in enumerate(encoded_dataset['train'][0]['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8a2fefdb434feb92b8783e83362def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# NOTE we need to change problem_type to multi_label_classification.....\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", \n",
    "                                                           problem_type=\"binary_classification\",\n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id\n",
    "                                                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle-bert/bert-base-cased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#push_to_hub=True,\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:124\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules)\u001b[0m\n",
      "File \u001b[0;32m~/VSCodeProjects/HLT-Project/.venv/lib/python3.11/site-packages/transformers/training_args.py:1551\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(version\u001b[38;5;241m.\u001b[39mparse(torch\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mbase_version) \u001b[38;5;241m==\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16:\n\u001b[1;32m   1546\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1549\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1550\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m-> 1551\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1554\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (get_xla_device_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1555\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1556\u001b[0m ):\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1560\u001b[0m     )\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1572\u001b[0m ):\n",
      "File \u001b[0;32m~/VSCodeProjects/HLT-Project/.venv/lib/python3.11/site-packages/transformers/training_args.py:2027\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2024\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2025\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2026\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 2027\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[0;32m~/VSCodeProjects/HLT-Project/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:63\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     61\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/VSCodeProjects/HLT-Project/.venv/lib/python3.11/site-packages/transformers/training_args.py:1937\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 1937\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   1938\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1939\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1940\u001b[0m         )\n\u001b[1;32m   1941\u001b[0m     AcceleratorState\u001b[38;5;241m.\u001b[39m_reset_state(reset_partial_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"google-bert/bert-base-cased\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
