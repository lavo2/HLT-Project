{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolapitzalis/anaconda3/envs/hlt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.neural_network import BernoulliRBM, MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = '/Users/nicolapitzalis/Documents/uni-ai/HLT/HLT-Project/dataset/distilbert_results/ner_recipes.csv'\n",
    "PATH_LABELS = '/Users/nicolapitzalis/Documents/uni-ai/HLT/HLT-Project/dataset/distilbert_results/ner_labels.csv'\n",
    "PATH_VOCABULARY = '/Users/nicolapitzalis/Documents/uni-ai/HLT/HLT-Project/dataset/distilbert_results/ner_ingredients.csv'\n",
    "# PATH_RECIPES = '/Users/nicolapitzalis/Documents/uni-ai/HLT/HLT-Project/dataset/recipes_df_r.csv'\n",
    "\n",
    "data = pd.read_csv(PATH_DATA)\n",
    "vocabulary = pd.read_csv(PATH_VOCABULARY, header=None)\n",
    "labels = pd.read_csv(PATH_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vocabulary.fillna('Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels['Vegetarian&Desserts'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = data.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Word2Vec model\n",
    "model = Word2Vec(data_list, vector_size=768, window=20, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5984, 768)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "# Loop over each recipe in the data\n",
    "for index, recipe in data.iterrows():\n",
    "    embedding_recipe = np.zeros(model.wv.vectors.shape[1])\n",
    "    \n",
    "    for ingredient in recipe:\n",
    "        if pd.isnull(ingredient):\n",
    "            continue\n",
    "        \n",
    "        if ingredient in model.wv.key_to_index:\n",
    "            embedding_recipe += model.wv[ingredient]\n",
    "        \n",
    "    embedding_recipe = embedding_recipe / recipe.dropna().shape[0]\n",
    "    embeddings.append(embedding_recipe)\n",
    "\n",
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9770, 768)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, test_size=0.2, random_state=42, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4837, 1: 4933}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 3870, 1: 3946}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 967, 1: 987}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.83      0.78       967\n",
      "           1       0.81      0.71      0.75       987\n",
      "\n",
      "    accuracy                           0.77      1954\n",
      "   macro avg       0.77      0.77      0.77      1954\n",
      "weighted avg       0.77      0.77      0.77      1954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(criterion='gini', n_estimators=300, random_state=42, max_depth=None, min_samples_split=2, min_samples_leaf=1, n_jobs=-1)\n",
    "random_forest.fit(X_train, y_train)\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65534707\n",
      "Iteration 2, loss = 0.55600702\n",
      "Iteration 3, loss = 0.55159904\n",
      "Iteration 4, loss = 0.54054722\n",
      "Iteration 5, loss = 0.54172864\n",
      "Iteration 6, loss = 0.53382748\n",
      "Iteration 7, loss = 0.53220847\n",
      "Iteration 8, loss = 0.53488956\n",
      "Iteration 9, loss = 0.52752857\n",
      "Iteration 10, loss = 0.53359101\n",
      "Iteration 11, loss = 0.52205374\n",
      "Iteration 12, loss = 0.52581901\n",
      "Iteration 13, loss = 0.52227006\n",
      "Iteration 14, loss = 0.52749048\n",
      "Iteration 15, loss = 0.52972348\n",
      "Iteration 16, loss = 0.52328026\n",
      "Iteration 17, loss = 0.52056665\n",
      "Iteration 18, loss = 0.52993007\n",
      "Iteration 19, loss = 0.52168191\n",
      "Iteration 20, loss = 0.52316473\n",
      "Iteration 21, loss = 0.51687956\n",
      "Iteration 22, loss = 0.51711567\n",
      "Iteration 23, loss = 0.51669433\n",
      "Iteration 24, loss = 0.51513952\n",
      "Iteration 25, loss = 0.51253431\n",
      "Iteration 26, loss = 0.51664358\n",
      "Iteration 27, loss = 0.51319170\n",
      "Iteration 28, loss = 0.51358868\n",
      "Iteration 29, loss = 0.51415516\n",
      "Iteration 30, loss = 0.51193500\n",
      "Iteration 31, loss = 0.51826869\n",
      "Iteration 32, loss = 0.50671913\n",
      "Iteration 33, loss = 0.50993920\n",
      "Iteration 34, loss = 0.51044113\n",
      "Iteration 35, loss = 0.50882743\n",
      "Iteration 36, loss = 0.50038878\n",
      "Iteration 37, loss = 0.50560936\n",
      "Iteration 38, loss = 0.50722966\n",
      "Iteration 39, loss = 0.50799807\n",
      "Iteration 40, loss = 0.50084692\n",
      "Iteration 41, loss = 0.50595276\n",
      "Iteration 42, loss = 0.50074593\n",
      "Iteration 43, loss = 0.50171259\n",
      "Iteration 44, loss = 0.50957691\n",
      "Iteration 45, loss = 0.50048694\n",
      "Iteration 46, loss = 0.49860255\n",
      "Iteration 47, loss = 0.49397838\n",
      "Iteration 48, loss = 0.49346187\n",
      "Iteration 49, loss = 0.49379909\n",
      "Iteration 50, loss = 0.49602068\n",
      "Iteration 51, loss = 0.49180473\n",
      "Iteration 52, loss = 0.49508573\n",
      "Iteration 53, loss = 0.49191284\n",
      "Iteration 54, loss = 0.48910815\n",
      "Iteration 55, loss = 0.48925133\n",
      "Iteration 56, loss = 0.49878978\n",
      "Iteration 57, loss = 0.49259590\n",
      "Iteration 58, loss = 0.48534734\n",
      "Iteration 59, loss = 0.48633734\n",
      "Iteration 60, loss = 0.48524566\n",
      "Iteration 61, loss = 0.50107533\n",
      "Iteration 62, loss = 0.48838080\n",
      "Iteration 63, loss = 0.48367541\n",
      "Iteration 64, loss = 0.48188701\n",
      "Iteration 65, loss = 0.48060742\n",
      "Iteration 66, loss = 0.47925351\n",
      "Iteration 67, loss = 0.48806636\n",
      "Iteration 68, loss = 0.47627474\n",
      "Iteration 69, loss = 0.47416205\n",
      "Iteration 70, loss = 0.47825664\n",
      "Iteration 71, loss = 0.47182278\n",
      "Iteration 72, loss = 0.47192080\n",
      "Iteration 73, loss = 0.47293445\n",
      "Iteration 74, loss = 0.47112606\n",
      "Iteration 75, loss = 0.46993504\n",
      "Iteration 76, loss = 0.46725194\n",
      "Iteration 77, loss = 0.46888684\n",
      "Iteration 78, loss = 0.46805100\n",
      "Iteration 79, loss = 0.46410609\n",
      "Iteration 80, loss = 0.46542949\n",
      "Iteration 81, loss = 0.46274618\n",
      "Iteration 82, loss = 0.46038818\n",
      "Iteration 83, loss = 0.46564001\n",
      "Iteration 84, loss = 0.46074015\n",
      "Iteration 85, loss = 0.45885181\n",
      "Iteration 86, loss = 0.45467289\n",
      "Iteration 87, loss = 0.46310640\n",
      "Iteration 88, loss = 0.46065054\n",
      "Iteration 89, loss = 0.45561342\n",
      "Iteration 90, loss = 0.45817915\n",
      "Iteration 91, loss = 0.45519017\n",
      "Iteration 92, loss = 0.45130354\n",
      "Iteration 93, loss = 0.44961974\n",
      "Iteration 94, loss = 0.46216792\n",
      "Iteration 95, loss = 0.44867249\n",
      "Iteration 96, loss = 0.44584802\n",
      "Iteration 97, loss = 0.44674565\n",
      "Iteration 98, loss = 0.45094146\n",
      "Iteration 99, loss = 0.44360007\n",
      "Iteration 100, loss = 0.43917379\n",
      "Iteration 101, loss = 0.43792948\n",
      "Iteration 102, loss = 0.44651097\n",
      "Iteration 103, loss = 0.43579467\n",
      "Iteration 104, loss = 0.44072889\n",
      "Iteration 105, loss = 0.46538085\n",
      "Iteration 106, loss = 0.43639589\n",
      "Iteration 107, loss = 0.43892703\n",
      "Iteration 108, loss = 0.43302004\n",
      "Iteration 109, loss = 0.43055034\n",
      "Iteration 110, loss = 0.42636732\n",
      "Iteration 111, loss = 0.43278370\n",
      "Iteration 112, loss = 0.42596746\n",
      "Iteration 113, loss = 0.42077279\n",
      "Iteration 114, loss = 0.42972719\n",
      "Iteration 115, loss = 0.42548965\n",
      "Iteration 116, loss = 0.42638382\n",
      "Iteration 117, loss = 0.41881338\n",
      "Iteration 118, loss = 0.41948116\n",
      "Iteration 119, loss = 0.41851864\n",
      "Iteration 120, loss = 0.42508833\n",
      "Iteration 121, loss = 0.42064813\n",
      "Iteration 122, loss = 0.41917890\n",
      "Iteration 123, loss = 0.41951283\n",
      "Iteration 124, loss = 0.41166353\n",
      "Iteration 125, loss = 0.40945606\n",
      "Iteration 126, loss = 0.41643918\n",
      "Iteration 127, loss = 0.41047558\n",
      "Iteration 128, loss = 0.40441551\n",
      "Iteration 129, loss = 0.41407728\n",
      "Iteration 130, loss = 0.40338128\n",
      "Iteration 131, loss = 0.39900657\n",
      "Iteration 132, loss = 0.39824918\n",
      "Iteration 133, loss = 0.40615487\n",
      "Iteration 134, loss = 0.39865944\n",
      "Iteration 135, loss = 0.40382055\n",
      "Iteration 136, loss = 0.39424925\n",
      "Iteration 137, loss = 0.39270169\n",
      "Iteration 138, loss = 0.39510973\n",
      "Iteration 139, loss = 0.40124430\n",
      "Iteration 140, loss = 0.38819827\n",
      "Iteration 141, loss = 0.40611402\n",
      "Iteration 142, loss = 0.39476976\n",
      "Iteration 143, loss = 0.38810081\n",
      "Iteration 144, loss = 0.38483702\n",
      "Iteration 145, loss = 0.38342885\n",
      "Iteration 146, loss = 0.38678593\n",
      "Iteration 147, loss = 0.38166293\n",
      "Iteration 148, loss = 0.37584815\n",
      "Iteration 149, loss = 0.37474294\n",
      "Iteration 150, loss = 0.37570509\n",
      "Iteration 151, loss = 0.36845025\n",
      "Iteration 152, loss = 0.38081141\n",
      "Iteration 153, loss = 0.37094172\n",
      "Iteration 154, loss = 0.37344569\n",
      "Iteration 155, loss = 0.37080510\n",
      "Iteration 156, loss = 0.37351495\n",
      "Iteration 157, loss = 0.37480532\n",
      "Iteration 158, loss = 0.36414992\n",
      "Iteration 159, loss = 0.36623512\n",
      "Iteration 160, loss = 0.36406119\n",
      "Iteration 161, loss = 0.35873322\n",
      "Iteration 162, loss = 0.35504589\n",
      "Iteration 163, loss = 0.36257361\n",
      "Iteration 164, loss = 0.36539787\n",
      "Iteration 165, loss = 0.35995239\n",
      "Iteration 166, loss = 0.34882018\n",
      "Iteration 167, loss = 0.35309142\n",
      "Iteration 168, loss = 0.35364596\n",
      "Iteration 169, loss = 0.35607196\n",
      "Iteration 170, loss = 0.35550598\n",
      "Iteration 171, loss = 0.37610534\n",
      "Iteration 172, loss = 0.34317915\n",
      "Iteration 173, loss = 0.34412567\n",
      "Iteration 174, loss = 0.34788917\n",
      "Iteration 175, loss = 0.35201809\n",
      "Iteration 176, loss = 0.34206765\n",
      "Iteration 177, loss = 0.34319930\n",
      "Iteration 178, loss = 0.33381716\n",
      "Iteration 179, loss = 0.33303567\n",
      "Iteration 180, loss = 0.34152414\n",
      "Iteration 181, loss = 0.33246175\n",
      "Iteration 182, loss = 0.33585651\n",
      "Iteration 183, loss = 0.33558564\n",
      "Iteration 184, loss = 0.33435413\n",
      "Iteration 185, loss = 0.32727928\n",
      "Iteration 186, loss = 0.32289895\n",
      "Iteration 187, loss = 0.32259357\n",
      "Iteration 188, loss = 0.33421871\n",
      "Iteration 189, loss = 0.32364310\n",
      "Iteration 190, loss = 0.32400475\n",
      "Iteration 191, loss = 0.32078462\n",
      "Iteration 192, loss = 0.31900276\n",
      "Iteration 193, loss = 0.31836663\n",
      "Iteration 194, loss = 0.32228985\n",
      "Iteration 195, loss = 0.31128764\n",
      "Iteration 196, loss = 0.31715523\n",
      "Iteration 197, loss = 0.30852036\n",
      "Iteration 198, loss = 0.31797465\n",
      "Iteration 199, loss = 0.30548939\n",
      "Iteration 200, loss = 0.30450569\n",
      "Iteration 201, loss = 0.30304311\n",
      "Iteration 202, loss = 0.30781549\n",
      "Iteration 203, loss = 0.30733527\n",
      "Iteration 204, loss = 0.31307096\n",
      "Iteration 205, loss = 0.29637076\n",
      "Iteration 206, loss = 0.30636826\n",
      "Iteration 207, loss = 0.30644070\n",
      "Iteration 208, loss = 0.29822331\n",
      "Iteration 209, loss = 0.29978104\n",
      "Iteration 210, loss = 0.29658280\n",
      "Iteration 211, loss = 0.28806343\n",
      "Iteration 212, loss = 0.28593185\n",
      "Iteration 213, loss = 0.30043506\n",
      "Iteration 214, loss = 0.28713055\n",
      "Iteration 215, loss = 0.29804830\n",
      "Iteration 216, loss = 0.30901935\n",
      "Iteration 217, loss = 0.30408177\n",
      "Iteration 218, loss = 0.29027373\n",
      "Iteration 219, loss = 0.28164227\n",
      "Iteration 220, loss = 0.29117081\n",
      "Iteration 221, loss = 0.28086926\n",
      "Iteration 222, loss = 0.28063530\n",
      "Iteration 223, loss = 0.27583474\n",
      "Iteration 224, loss = 0.28116640\n",
      "Iteration 225, loss = 0.26994329\n",
      "Iteration 226, loss = 0.28598506\n",
      "Iteration 227, loss = 0.26757324\n",
      "Iteration 228, loss = 0.26781224\n",
      "Iteration 229, loss = 0.27283159\n",
      "Iteration 230, loss = 0.28133248\n",
      "Iteration 231, loss = 0.28436151\n",
      "Iteration 232, loss = 0.27186094\n",
      "Iteration 233, loss = 0.28078400\n",
      "Iteration 234, loss = 0.28784901\n",
      "Iteration 235, loss = 0.26809828\n",
      "Iteration 236, loss = 0.26366138\n",
      "Iteration 237, loss = 0.26388279\n",
      "Iteration 238, loss = 0.26091189\n",
      "Iteration 239, loss = 0.25175259\n",
      "Iteration 240, loss = 0.25161192\n",
      "Iteration 241, loss = 0.26026560\n",
      "Iteration 242, loss = 0.25292310\n",
      "Iteration 243, loss = 0.25983270\n",
      "Iteration 244, loss = 0.25477644\n",
      "Iteration 245, loss = 0.24993696\n",
      "Iteration 246, loss = 0.25486074\n",
      "Iteration 247, loss = 0.24587980\n",
      "Iteration 248, loss = 0.24941024\n",
      "Iteration 249, loss = 0.24413294\n",
      "Iteration 250, loss = 0.25222575\n",
      "Iteration 251, loss = 0.24462911\n",
      "Iteration 252, loss = 0.24369621\n",
      "Iteration 253, loss = 0.24158666\n",
      "Iteration 254, loss = 0.23749472\n",
      "Iteration 255, loss = 0.23331984\n",
      "Iteration 256, loss = 0.23011200\n",
      "Iteration 257, loss = 0.24996314\n",
      "Iteration 258, loss = 0.23312356\n",
      "Iteration 259, loss = 0.25403462\n",
      "Iteration 260, loss = 0.24023673\n",
      "Iteration 261, loss = 0.23162385\n",
      "Iteration 262, loss = 0.23293414\n",
      "Iteration 263, loss = 0.23211342\n",
      "Iteration 264, loss = 0.23652669\n",
      "Iteration 265, loss = 0.24918533\n",
      "Iteration 266, loss = 0.22917936\n",
      "Iteration 267, loss = 0.22804406\n",
      "Iteration 268, loss = 0.23008573\n",
      "Iteration 269, loss = 0.22491513\n",
      "Iteration 270, loss = 0.22492780\n",
      "Iteration 271, loss = 0.22244636\n",
      "Iteration 272, loss = 0.22367147\n",
      "Iteration 273, loss = 0.21250639\n",
      "Iteration 274, loss = 0.21656810\n",
      "Iteration 275, loss = 0.21714391\n",
      "Iteration 276, loss = 0.21256747\n",
      "Iteration 277, loss = 0.21802356\n",
      "Iteration 278, loss = 0.22514860\n",
      "Iteration 279, loss = 0.22093324\n",
      "Iteration 280, loss = 0.21718393\n",
      "Iteration 281, loss = 0.20828151\n",
      "Iteration 282, loss = 0.21187296\n",
      "Iteration 283, loss = 0.23463353\n",
      "Iteration 284, loss = 0.21755274\n",
      "Iteration 285, loss = 0.21386904\n",
      "Iteration 286, loss = 0.20126720\n",
      "Iteration 287, loss = 0.20788108\n",
      "Iteration 288, loss = 0.21217329\n",
      "Iteration 289, loss = 0.20662501\n",
      "Iteration 290, loss = 0.20186972\n",
      "Iteration 291, loss = 0.21099719\n",
      "Iteration 292, loss = 0.20505095\n",
      "Iteration 293, loss = 0.19990615\n",
      "Iteration 294, loss = 0.21214543\n",
      "Iteration 295, loss = 0.20027125\n",
      "Iteration 296, loss = 0.19643832\n",
      "Iteration 297, loss = 0.19252825\n",
      "Iteration 298, loss = 0.19200947\n",
      "Iteration 299, loss = 0.19830765\n",
      "Iteration 300, loss = 0.18762170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolapitzalis/anaconda3/envs/hlt/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7318321392016377\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.77      0.74       967\n",
      "           1       0.75      0.70      0.72       987\n",
      "\n",
      "    accuracy                           0.73      1954\n",
      "   macro avg       0.73      0.73      0.73      1954\n",
      "weighted avg       0.73      0.73      0.73      1954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(4096), max_iter=300, alpha=0, activation='tanh',\n",
    "                    solver='sgd', verbose=10, random_state=21, learning_rate_init=0.01)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=200)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
