{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./dataset/recipes_82k.csv')\n",
    "df = pd.read_csv('./dataset/ner_ingredients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9820 entries, 0 to 9819\n",
      "Data columns (total 1 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   ingredients  9820 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 76.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# change the column name to `ingredients`\n",
    "df.rename(columns={'red chicory': 'ingredients'}, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new instance to df\n",
    "df = df._append({'ingredients': 'red chicory'}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9816</th>\n",
       "      <td>Potato chips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9817</th>\n",
       "      <td>fish flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9818</th>\n",
       "      <td>dry beef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9819</th>\n",
       "      <td>cauliflower head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9820</th>\n",
       "      <td>red chicory</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ingredients\n",
       "9816      Potato chips\n",
       "9817       fish flavor\n",
       "9818          dry beef\n",
       "9819  cauliflower head\n",
       "9820       red chicory"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We strt processingt his dtaaframe: eliminate words of 2 chars, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    s = s.lower()\n",
    "    # if you encounter a - or ' (or something else) in the text, replace it with a space\n",
    "    #TODO: right?\n",
    "    s1 = re.sub(r'[^a-z\\s]', ' ', s)\n",
    "    s1 = ' '.join([w for w in s1.split() if len(w) > 2])\n",
    "    # remove multiple spaces and starting and ending spaces\n",
    "    s2 = re.sub(r'\\s+', ' ', s1).strip()\n",
    "    return s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mango juice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pinch salt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tooth coriander</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sticks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vegetable stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ingredients\n",
       "0      mango juice\n",
       "1       pinch salt\n",
       "2  tooth coriander\n",
       "3           sticks\n",
       "4  vegetable stock"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the function to the ingredients column\n",
    "df['ingredients'] = df['ingredients'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 1)\n"
     ]
    }
   ],
   "source": [
    "# count empty strings\n",
    "print(df[df['ingredients'] == ''].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1240, 1)\n"
     ]
    }
   ],
   "source": [
    "# duplicates\n",
    "print(df[df.duplicated()].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9821, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8580, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eliminate both empty strings and duplicates\n",
    "df = df.drop_duplicates()\n",
    "df = df[df['ingredients'] != '']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 21:26:30 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110edb2149f844f2abb9553908b0ea56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 21:26:30 INFO: Downloaded file to /Users/irene/stanza_resources/resources.json\n",
      "2024-04-29 21:26:30 WARNING: Language en package default expects mwt, which has been added\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d44a99824a40629440a55a0754f565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.8.0/models/tokenize/combined.pt:   0%|    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5023ecdbc64c4a73aaf126113ccfaf01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.8.0/models/mwt/combined.pt:   0%|         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d86cdf077724f3299857bc064a7e310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.8.0/models/pos/combined_charlm.pt:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115104a67bf243e1ab0d2f974aa477d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.8.0/models/lemma/combined_nocharlm.pt:   0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef175adb89d411eba99ad96ce329a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.8.0/models/forward_charlm/1billion.pt:   0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83fdf26a69814f46afad4a8ca0f2113c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.8.0/models/backward_charlm/1billion.pt:   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c2d4a3ea4a46fe8521889090a1a351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.8.0/models/pretrain/conll17.pt:   0%|     …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 21:27:15 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2024-04-29 21:27:15 INFO: Using device: cpu\n",
      "2024-04-29 21:27:15 INFO: Loading: tokenize\n",
      "2024-04-29 21:27:15 INFO: Loading: mwt\n",
      "2024-04-29 21:27:15 INFO: Loading: pos\n",
      "2024-04-29 21:27:16 INFO: Loading: lemma\n",
      "2024-04-29 21:27:16 INFO: Done loading processors!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stanza.pipeline.core.Pipeline at 0x31f231ad0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize, pos, lemma')\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how these tools work!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Ingredient: bones fish bones\n",
      "Tokens: ['bones', 'fish', 'bones']\n",
      "POS Tags: ['NOUN', 'NOUN', 'NOUN']\n",
      "Lemmas: ['bone', 'fish', 'bone']\n",
      "\n",
      "Original Ingredient: eyed peas\n",
      "Tokens: ['eyed', 'peas']\n",
      "POS Tags: ['ADJ', 'NOUN']\n",
      "Lemmas: ['eye', 'peas']\n",
      "\n",
      "Original Ingredient: sliced pepperoni\n",
      "Tokens: ['sliced', 'pepperoni']\n",
      "POS Tags: ['VERB', 'NOUN']\n",
      "Lemmas: ['slice', 'pepperoni']\n",
      "\n",
      "Original Ingredient: carrot fronds\n",
      "Tokens: ['carrot', 'fronds']\n",
      "POS Tags: ['NOUN', 'NOUN']\n",
      "Lemmas: ['carrot', 'frond']\n",
      "\n",
      "Original Ingredient: chocolate sprinkles\n",
      "Tokens: ['chocolate', 'sprinkles']\n",
      "POS Tags: ['NOUN', 'NOUN']\n",
      "Lemmas: ['chocolate', 'sprinkle']\n",
      "\n",
      "Original Ingredient: lash milk\n",
      "Tokens: ['lash', 'milk']\n",
      "POS Tags: ['NOUN', 'NOUN']\n",
      "Lemmas: ['lash', 'milk']\n",
      "\n",
      "Original Ingredient: goose\n",
      "Tokens: ['goose']\n",
      "POS Tags: ['NOUN']\n",
      "Lemmas: ['goose']\n",
      "\n",
      "Original Ingredient: bucatini pasta\n",
      "Tokens: ['bucatini', 'pasta']\n",
      "POS Tags: ['NOUN', 'NOUN']\n",
      "Lemmas: ['bucatini', 'pasta']\n",
      "\n",
      "Original Ingredient: lowfat yogurt\n",
      "Tokens: ['lowfat', 'yogurt']\n",
      "POS Tags: ['NOUN', 'NOUN']\n",
      "Lemmas: ['lowfat', 'yogurt']\n",
      "\n",
      "Original Ingredient: russet potato\n",
      "Tokens: ['russet', 'potato']\n",
      "POS Tags: ['NOUN', 'NOUN']\n",
      "Lemmas: ['russet', 'potato']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process each ingredient\n",
    "for ingredient in df[7000:7010]['ingredients']:\n",
    "    # Process ingredient through the pipeline\n",
    "    doc = nlp(ingredient)\n",
    "    \n",
    "    # Extract tokenized forms, part-of-speech tags, and lemmatized forms\n",
    "    tokens = [word.text for sent in doc.sentences for word in sent.words]\n",
    "    pos_tags = [word.upos for sent in doc.sentences for word in sent.words]\n",
    "    lemmas = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "    \n",
    "    # Print the processed information\n",
    "    print(\"Original Ingredient:\", ingredient)\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print(\"POS Tags:\", pos_tags)\n",
    "    print(\"Lemmas:\", lemmas)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Ingredient: bones fish bones\n",
      "Tokens: ['bone', 'fish', 'bone']\n",
      "POS Tags: ['NOUN', 'NOUN', 'NOUN']\n",
      "Lemmas: ['bone', 'fish', 'bone']\n",
      "\n",
      "Original Ingredient: eyed peas\n",
      "Tokens: ['peas']\n",
      "POS Tags: ['ADJ', 'NOUN']\n",
      "Lemmas: ['eye', 'peas']\n",
      "\n",
      "Original Ingredient: sliced pepperoni\n",
      "Tokens: ['slice', 'pepperoni']\n",
      "POS Tags: ['VERB', 'NOUN']\n",
      "Lemmas: ['slice', 'pepperoni']\n",
      "\n",
      "Original Ingredient: carrot fronds\n",
      "Tokens: ['carrot', 'frond']\n",
      "POS Tags: ['NOUN', 'NOUN']\n",
      "Lemmas: ['carrot', 'frond']\n",
      "\n",
      "Original Ingredient: chocolate sprinkles\n",
      "Tokens: ['chocolate', 'sprinkle']\n",
      "POS Tags: ['NOUN', 'NOUN']\n",
      "Lemmas: ['chocolate', 'sprinkle']\n",
      "\n",
      "Original Ingredient: lash milk\n",
      "Tokens: ['lash', 'milk']\n",
      "POS Tags: ['NOUN', 'NOUN']\n",
      "Lemmas: ['lash', 'milk']\n",
      "\n",
      "Original Ingredient: goose\n",
      "Tokens: ['goose']\n",
      "POS Tags: ['NOUN']\n",
      "Lemmas: ['goose']\n",
      "\n",
      "Original Ingredient: bucatini pasta\n",
      "Tokens: ['bucatini', 'pasta']\n",
      "POS Tags: ['NOUN', 'NOUN']\n",
      "Lemmas: ['bucatini', 'pasta']\n",
      "\n",
      "Original Ingredient: lowfat yogurt\n",
      "Tokens: ['lowfat', 'yogurt']\n",
      "POS Tags: ['NOUN', 'NOUN']\n",
      "Lemmas: ['lowfat', 'yogurt']\n",
      "\n",
      "Original Ingredient: russet potato\n",
      "Tokens: ['russet', 'potato']\n",
      "POS Tags: ['NOUN', 'NOUN']\n",
      "Lemmas: ['russet', 'potato']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INTERNAL PROCESSING: ELIMINATE ADJ AND PROPN\n",
    "\n",
    "for ingredient in df[7000:7010]['ingredients']:\n",
    "    # Process ingredient through the pipeline\n",
    "    doc = nlp(ingredient)\n",
    "    \n",
    "    # Extract tokenized forms, part-of-speech tags, and lemmatized forms\n",
    "    tokens = [word.text for sent in doc.sentences for word in sent.words]\n",
    "    pos_tags = [word.upos for sent in doc.sentences for word in sent.words]\n",
    "    lemmas = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "    \n",
    "    ### NOTICE THAT WE ARE USING `lemmas` INSTEAD OF `tokens`, so we will define our clean dictionary with the pure form of the words (their lemmatization!!!) ###\n",
    "    # eliminate the tokens in `tokens` that are ADJ in `pos_tags`\n",
    "    tokens = [lemmas[i] for i in range(len(tokens)) if pos_tags[i] != 'ADJ' and pos_tags[i] != 'PROPN']\n",
    "    \n",
    "    # Print the processed information\n",
    "    print(\"Original Ingredient:\", ingredient)\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print(\"POS Tags:\", pos_tags)\n",
    "    print(\"Lemmas:\", lemmas)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: valutare se togliere anche VERB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, now we will have new empty strings!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of new dictionary\n",
    "cleaned_ingredients = []\n",
    "\n",
    "for ingredient in df['ingredients']:\n",
    "    # Process ingredient through the pipeline\n",
    "    doc = nlp(ingredient)\n",
    "    \n",
    "    # Extract tokenized forms, part-of-speech tags, and lemmatized forms\n",
    "    tokens = [word.text for sent in doc.sentences for word in sent.words]\n",
    "    pos_tags = [word.upos for sent in doc.sentences for word in sent.words]\n",
    "    lemmas = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "    \n",
    "    ### NOTICE THAT WE ARE USING `lemmas` INSTEAD OF `tokens`, so we will define our clean dictionary with the pure form of the words (their lemmatization!!!) ###\n",
    "    # eliminate the tokens in `tokens` that are ADJ in `pos_tags`\n",
    "    tokens = [lemmas[i] for i in range(len(tokens)) if pos_tags[i] != 'ADJ' and pos_tags[i] != 'PROPN']\n",
    "\n",
    "    # reconvert tokens to a string\n",
    "    cleaned_ingredient = ' '.join(tokens)\n",
    "\n",
    "    # append to the list\n",
    "    cleaned_ingredients.append(cleaned_ingredient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mango juice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pinch salt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tooth coriander</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vegetable stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ingredients\n",
       "0      mango juice\n",
       "1       pinch salt\n",
       "2  tooth coriander\n",
       "3            stick\n",
       "4  vegetable stock"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# switch cleaned_ingredients to a DataFrame\n",
    "cleaned_df = pd.DataFrame(cleaned_ingredients, columns=['ingredients'])\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(498, 1)\n"
     ]
    }
   ],
   "source": [
    "# see how many empty strings we have\n",
    "print(cleaned_df[cleaned_df['ingredients'] == ''].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2517, 1)\n"
     ]
    }
   ],
   "source": [
    "# see duplicates\n",
    "print(cleaned_df[cleaned_df.duplicated()].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>clam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>plum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>pepper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>potato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>onion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>liqueur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>lobster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>pepper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>plum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>pepper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>lettuce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>mustard seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ingredients\n",
       "33               \n",
       "42           clam\n",
       "44               \n",
       "60               \n",
       "74               \n",
       "80           rice\n",
       "92               \n",
       "95               \n",
       "105              \n",
       "110              \n",
       "128          plum\n",
       "139        pepper\n",
       "144        potato\n",
       "146         onion\n",
       "151              \n",
       "162              \n",
       "164       liqueur\n",
       "173     chocolate\n",
       "178              \n",
       "187       lobster\n",
       "198        pepper\n",
       "202              \n",
       "225          plum\n",
       "246        pepper\n",
       "257              \n",
       "261              \n",
       "264       lettuce\n",
       "267              \n",
       "304  mustard seed\n",
       "331          rice"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the duplicates \n",
    "cleaned_df[cleaned_df.duplicated()].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6062, 1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eliminate both empty strings and duplicates\n",
    "cleaned_df = cleaned_df.drop_duplicates()\n",
    "cleaned_df = cleaned_df[cleaned_df['ingredients'] != '']\n",
    "cleaned_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
