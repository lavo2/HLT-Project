{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification by standard ML approach\n",
    "In the following, we will run some experiments using some of the standard ML approaches, such as Random Forest, MLP, SVMs. These na√Øve methods will exploit the list of ingredients, rather than the structure of the entire recipe itself. This reasoning comes up naturally when considering such standards ML practices, since they do not keep in consideration the semantic structure of the sentences and do not share any weight in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolapitzalis/anaconda3/envs/hlt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.neural_network import BernoulliRBM, MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = '/Users/nicolapitzalis/Documents/uni-ai/HLT/HLT-Project/dataset/ingredients_list_stanza_10k.csv'\n",
    "PATH_VOCABULARY = '/Users/nicolapitzalis/Documents/uni-ai/HLT/HLT-Project/dataset/vocabulary_stanza_10k.csv'\n",
    "PATH_RECIPES = '/Users/nicolapitzalis/Documents/uni-ai/HLT/HLT-Project/dataset/dataset_balanced_10k.csv'\n",
    "\n",
    "data = pd.read_csv(PATH_DATA, low_memory=False)\n",
    "vocabulary = pd.read_csv(PATH_VOCABULARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = pd.read_csv(PATH_RECIPES)\n",
    "labels = recipes['Vegetarian']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "We need to create the one_hot_encoded version of the ingredients, both for the ingredient's vocabulary and for the matrix of recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vocabulary.fillna('Missing')\n",
    "ohe_vocabulary = pd.get_dummies(vocabulary, prefix='category').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_matrix = []\n",
    "\n",
    "# Loop over each recipe in the data\n",
    "for index, recipe in data.iterrows():\n",
    "    ohe_recipe = np.zeros(len(ohe_vocabulary.columns))\n",
    "    \n",
    "    for ingredient in recipe:\n",
    "        if pd.isnull(ingredient):\n",
    "            continue\n",
    "        \n",
    "        # Check if the ingredient is in the one-hot vocabulary\n",
    "        if f'category_{ingredient}' in ohe_vocabulary.columns:\n",
    "            # Find the index for the ingredient\n",
    "            ingredient_index = ohe_vocabulary.columns.get_loc(f'category_{ingredient}')\n",
    "            \n",
    "            # Set the corresponding position in ohe_recipe to 1\n",
    "            ohe_recipe[ingredient_index] = 1\n",
    "\n",
    "    ohe_matrix.append(ohe_recipe)\n",
    "\n",
    "ohe_matrix = np.array(ohe_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(ohe_matrix, labels, test_size=0.3, random_state=42, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5000, 1: 5000}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 3500, 1: 3500}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1500, 1: 1500}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8706666666666667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.85      0.87      1500\n",
      "           1       0.86      0.89      0.87      1500\n",
      "\n",
      "    accuracy                           0.87      3000\n",
      "   macro avg       0.87      0.87      0.87      3000\n",
      "weighted avg       0.87      0.87      0.87      3000\n",
      "\n",
      "Accuracy Score: 0.8706666666666667\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(criterion='gini', n_estimators=100, random_state=42, max_depth=None, min_samples_split=5, min_samples_leaf=1, n_jobs=-1)\n",
    "random_forest.fit(X_train, y_train)\n",
    "y_pred = random_forest.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1274  226]\n",
      " [ 162 1338]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=np.unique(labels), yticklabels=np.unique(labels))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]  \n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8726666666666667\n",
      "Best parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best cross-validation score: 0.88\n"
     ]
    }
   ],
   "source": [
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8726666666666667\n",
      "Confusion Matrix:\n",
      "[[1266  234]\n",
      " [ 148 1352]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=np.unique(labels), yticklabels=np.unique(labels))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.37016492\n",
      "Iteration 2, loss = 0.12697363\n",
      "Iteration 3, loss = 0.06797402\n",
      "Iteration 4, loss = 0.04568066\n",
      "Iteration 5, loss = 0.03622758\n",
      "Iteration 6, loss = 0.02636126\n",
      "Iteration 7, loss = 0.01939708\n",
      "Iteration 8, loss = 0.01433541\n",
      "Iteration 9, loss = 0.01070834\n",
      "Iteration 10, loss = 0.00817415\n",
      "Iteration 11, loss = 0.00681514\n",
      "Iteration 12, loss = 0.00589878\n",
      "Iteration 13, loss = 0.00541609\n",
      "Iteration 14, loss = 0.00521912\n",
      "Iteration 15, loss = 0.00490313\n",
      "Iteration 16, loss = 0.00469806\n",
      "Iteration 17, loss = 0.00460096\n",
      "Iteration 18, loss = 0.00446057\n",
      "Iteration 19, loss = 0.00422173\n",
      "Iteration 20, loss = 0.00414082\n",
      "Iteration 21, loss = 0.00409968\n",
      "Iteration 22, loss = 0.00399767\n",
      "Iteration 23, loss = 0.00398628\n",
      "Iteration 24, loss = 0.00390211\n",
      "Iteration 25, loss = 0.00385261\n",
      "Iteration 26, loss = 0.00385868\n",
      "Iteration 27, loss = 0.00394725\n",
      "Iteration 28, loss = 0.00383929\n",
      "Iteration 29, loss = 0.00383734\n",
      "Iteration 30, loss = 0.00381639\n",
      "Iteration 31, loss = 0.00365802\n",
      "Iteration 32, loss = 0.00359239\n",
      "Iteration 33, loss = 0.00356811\n",
      "Iteration 34, loss = 0.00361912\n",
      "Iteration 35, loss = 0.00369754\n",
      "Iteration 36, loss = 0.00356756\n",
      "Iteration 37, loss = 0.00356583\n",
      "Iteration 38, loss = 0.00363091\n",
      "Iteration 39, loss = 0.00363327\n",
      "Iteration 40, loss = 0.00343941\n",
      "Iteration 41, loss = 0.00348756\n",
      "Iteration 42, loss = 0.00347325\n",
      "Iteration 43, loss = 0.00348192\n",
      "Iteration 44, loss = 0.00343708\n",
      "Iteration 45, loss = 0.00345274\n",
      "Iteration 46, loss = 0.00344615\n",
      "Iteration 47, loss = 0.00344106\n",
      "Iteration 48, loss = 0.00339650\n",
      "Iteration 49, loss = 0.00346733\n",
      "Iteration 50, loss = 0.00338732\n",
      "Iteration 51, loss = 0.00350485\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.86      0.89      1500\n",
      "           1       0.87      0.93      0.90      1500\n",
      "\n",
      "    accuracy                           0.90      3000\n",
      "   macro avg       0.90      0.90      0.90      3000\n",
      "weighted avg       0.90      0.90      0.90      3000\n",
      "\n",
      "Accuracy Score: 0.8976666666666666\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, alpha=0.0001,\n",
    "                    solver='adam', verbose=10, random_state=21, learning_rate_init=0.01)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1292  208]\n",
      " [  99 1401]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=np.unique(labels), yticklabels=np.unique(labels))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "[CV 2/3; 2/36] START alpha=0.0001, hidden_layer_sizes=(50,), learning_rate_init=0.01\n",
      "[CV 3/3; 2/36] START alpha=0.0001, hidden_layer_sizes=(50,), learning_rate_init=0.01\n",
      "[CV 1/3; 2/36] START alpha=0.0001, hidden_layer_sizes=(50,), learning_rate_init=0.01\n",
      "[CV 2/3; 1/36] START alpha=0.0001, hidden_layer_sizes=(50,), learning_rate_init=0.001\n",
      "[CV 1/3; 3/36] START alpha=0.0001, hidden_layer_sizes=(50,), learning_rate_init=0.1\n",
      "[CV 1/3; 1/36] START alpha=0.0001, hidden_layer_sizes=(50,), learning_rate_init=0.001\n",
      "[CV 3/3; 1/36] START alpha=0.0001, hidden_layer_sizes=(50,), learning_rate_init=0.001\n",
      "[CV 2/3; 3/36] START alpha=0.0001, hidden_layer_sizes=(50,), learning_rate_init=0.1\n",
      "[CV 3/3; 3/36] START alpha=0.0001, hidden_layer_sizes=(50,), learning_rate_init=0.1\n",
      "[CV 1/3; 4/36] START alpha=0.0001, hidden_layer_sizes=(100,), learning_rate_init=0.001\n",
      "[CV 2/3; 4/36] START alpha=0.0001, hidden_layer_sizes=(100,), learning_rate_init=0.001\n",
      "[CV 3/3; 4/36] START alpha=0.0001, hidden_layer_sizes=(100,), learning_rate_init=0.001\n",
      "Iteration 1, loss = 0.40826221\n",
      "Iteration 1, loss = 0.45401639\n",
      "Iteration 2, loss = 0.12317211\n",
      "Iteration 2, loss = 0.17153984\n",
      "Iteration 1, loss = 0.44692471\n",
      "Iteration 1, loss = 0.42384349\n",
      "Iteration 1, loss = 0.65251549\n",
      "Iteration 1, loss = 0.44459636\n",
      "Iteration 1, loss = 0.41289426\n",
      "Iteration 1, loss = 0.65297703\n",
      "Iteration 1, loss = 0.65251717\n",
      "Iteration 3, loss = 0.05275884\n",
      "Iteration 3, loss = 0.08553495\n",
      "Iteration 2, loss = 0.16069264\n",
      "Iteration 2, loss = 0.12840423\n",
      "Iteration 1, loss = 0.64355830\n",
      "Iteration 2, loss = 0.52981200\n",
      "Iteration 2, loss = 0.16074989\n",
      "Iteration 1, loss = 0.64304743\n",
      "Iteration 1, loss = 0.64350254\n",
      "Iteration 2, loss = 0.12270925\n",
      "Iteration 2, loss = 0.53161785\n",
      "Iteration 2, loss = 0.53033151\n",
      "Iteration 4, loss = 0.02990619\n",
      "Iteration 4, loss = 0.05455647\n",
      "Iteration 3, loss = 0.07896768\n",
      "Iteration 3, loss = 0.05080059\n",
      "Iteration 3, loss = 0.41861647\n",
      "Iteration 3, loss = 0.08346731\n",
      "Iteration 3, loss = 0.05613026\n",
      "Iteration 3, loss = 0.42135818\n",
      "Iteration 3, loss = 0.41927502\n",
      "Iteration 5, loss = 0.01959417\n",
      "Iteration 2, loss = 0.48062323\n",
      "Iteration 5, loss = 0.03754994\n",
      "Iteration 4, loss = 0.04861103\n",
      "Iteration 2, loss = 0.48173755\n",
      "Iteration 2, loss = 0.48119981\n",
      "Iteration 4, loss = 0.02704424\n",
      "Iteration 4, loss = 0.05567911\n",
      "Iteration 4, loss = 0.33813619\n",
      "Iteration 4, loss = 0.03041498\n",
      "Iteration 4, loss = 0.34111711\n",
      "Iteration 4, loss = 0.33946686\n",
      "Iteration 6, loss = 0.01471496\n",
      "Iteration 6, loss = 0.02823208\n",
      "Iteration 5, loss = 0.03543616\n",
      "Iteration 5, loss = 0.01851610\n",
      "Iteration 5, loss = 0.04065790\n",
      "Iteration 5, loss = 0.28011086\n",
      "Iteration 5, loss = 0.28394176\n",
      "Iteration 5, loss = 0.01803350\n",
      "Iteration 5, loss = 0.28222437\n",
      "Iteration 3, loss = 0.35491561\n",
      "Iteration 3, loss = 0.35729958\n",
      "Iteration 3, loss = 0.35472308\n",
      "Iteration 7, loss = 0.01326571\n",
      "Iteration 7, loss = 0.02349245\n",
      "Iteration 6, loss = 0.02711213\n",
      "Iteration 6, loss = 0.01455237\n",
      "Iteration 6, loss = 0.03154761\n",
      "Iteration 6, loss = 0.23650966\n",
      "Iteration 6, loss = 0.24041836\n",
      "Iteration 6, loss = 0.23847256\n",
      "Iteration 6, loss = 0.01438954\n",
      "Iteration 8, loss = 0.01199982\n",
      "Iteration 8, loss = 0.01889182\n",
      "Iteration 7, loss = 0.02146643\n",
      "Iteration 7, loss = 0.01367123\n",
      "Iteration 7, loss = 0.02691112\n",
      "Iteration 4, loss = 0.27238371\n",
      "Iteration 7, loss = 0.20321169\n",
      "Iteration 4, loss = 0.27250161\n",
      "Iteration 4, loss = 0.27624694\n",
      "Iteration 7, loss = 0.20707721\n",
      "Iteration 7, loss = 0.20553941\n",
      "Iteration 7, loss = 0.01184132\n",
      "Iteration 9, loss = 0.01578966\n",
      "Iteration 9, loss = 0.01103376\n",
      "Iteration 8, loss = 0.01716192\n",
      "Iteration 8, loss = 0.01180654\n",
      "Iteration 8, loss = 0.02052716\n",
      "Iteration 8, loss = 0.17686842\n",
      "Iteration 8, loss = 0.18129973\n",
      "Iteration 8, loss = 0.17907332\n",
      "Iteration 8, loss = 0.01039974\n",
      "Iteration 10, loss = 0.01050196\n",
      "Iteration 10, loss = 0.01313989\n",
      "Iteration 5, loss = 0.21818714\n",
      "Iteration 9, loss = 0.01405494\n",
      "Iteration 9, loss = 0.01169136\n",
      "Iteration 5, loss = 0.22223884\n",
      "Iteration 5, loss = 0.21556965\n",
      "Iteration 9, loss = 0.01829418\n",
      "Iteration 9, loss = 0.15575680\n",
      "Iteration 9, loss = 0.15965609\n",
      "Iteration 9, loss = 0.15790967\n",
      "Iteration 9, loss = 0.00919811\n",
      "Iteration 11, loss = 0.00972391\n",
      "Iteration 11, loss = 0.01157539\n",
      "Iteration 10, loss = 0.01216030\n",
      "Iteration 10, loss = 0.01083977\n",
      "Iteration 10, loss = 0.01485727\n",
      "Iteration 10, loss = 0.13954340\n",
      "Iteration 10, loss = 0.14239992\n",
      "Iteration 10, loss = 0.14077159\n",
      "Iteration 10, loss = 0.00867343\n",
      "Iteration 6, loss = 0.17710827\n",
      "Iteration 6, loss = 0.18148817\n",
      "Iteration 6, loss = 0.17628229\n",
      "Iteration 12, loss = 0.00925601\n",
      "Iteration 11, loss = 0.01022012\n",
      "Iteration 12, loss = 0.00970130\n",
      "Iteration 11, loss = 0.01480161\n",
      "Iteration 11, loss = 0.01303293\n",
      "Iteration 11, loss = 0.12491816\n",
      "Iteration 11, loss = 0.12769384\n",
      "Iteration 11, loss = 0.12648933\n",
      "Iteration 11, loss = 0.00864324\n",
      "Iteration 13, loss = 0.00891525\n",
      "Iteration 13, loss = 0.00831228\n",
      "Iteration 12, loss = 0.00868612\n",
      "Iteration 12, loss = 0.03697013\n",
      "Iteration 12, loss = 0.01084090\n",
      "Iteration 7, loss = 0.14869520\n",
      "Iteration 12, loss = 0.11308752\n",
      "Iteration 12, loss = 0.11558481\n",
      "Iteration 7, loss = 0.14742210\n",
      "Iteration 7, loss = 0.15230220\n",
      "Iteration 12, loss = 0.11507510\n",
      "Iteration 12, loss = 0.00804378\n",
      "Iteration 14, loss = 0.00856489\n",
      "Iteration 14, loss = 0.00755052\n",
      "Iteration 13, loss = 0.00754915\n",
      "Iteration 13, loss = 0.12709652\n",
      "Iteration 13, loss = 0.00948818\n",
      "Iteration 13, loss = 0.10277149\n",
      "Iteration 13, loss = 0.10534158\n",
      "Iteration 13, loss = 0.10471506\n",
      "Iteration 13, loss = 0.00778192\n",
      "Iteration 15, loss = 0.00810727\n",
      "Iteration 15, loss = 0.00669967\n",
      "Iteration 14, loss = 0.00693856\n",
      "Iteration 14, loss = 0.09891122\n",
      "Iteration 8, loss = 0.12719828\n",
      "Iteration 8, loss = 0.12973641\n",
      "Iteration 14, loss = 0.00912738\n",
      "Iteration 8, loss = 0.12536627\n",
      "Iteration 14, loss = 0.09487076\n",
      "Iteration 14, loss = 0.09691909\n",
      "Iteration 14, loss = 0.09663865\n",
      "Iteration 14, loss = 0.00737104\n",
      "Iteration 16, loss = 0.00793302\n",
      "Iteration 16, loss = 0.00623712\n",
      "Iteration 15, loss = 0.00627785\n",
      "Iteration 15, loss = 0.05748346\n",
      "Iteration 15, loss = 0.00853081\n",
      "Iteration 15, loss = 0.08716100\n",
      "Iteration 15, loss = 0.08927419\n",
      "Iteration 15, loss = 0.08949273\n",
      "Iteration 15, loss = 0.00732567\n",
      "Iteration 9, loss = 0.11082402\n",
      "Iteration 17, loss = 0.00778388\n",
      "Iteration 9, loss = 0.11323300\n",
      "Iteration 9, loss = 0.10908862\n",
      "Iteration 16, loss = 0.00572961\n",
      "Iteration 17, loss = 0.00588092\n",
      "Iteration 16, loss = 0.03746138\n",
      "Iteration 16, loss = 0.00741223\n",
      "Iteration 16, loss = 0.08070455\n",
      "Iteration 16, loss = 0.08275050\n",
      "Iteration 16, loss = 0.08391625\n",
      "Iteration 16, loss = 0.00693646\n",
      "Iteration 18, loss = 0.00762730\n",
      "Iteration 18, loss = 0.00545802\n",
      "Iteration 17, loss = 0.00534571\n",
      "Iteration 17, loss = 0.02956986\n",
      "Iteration 17, loss = 0.00775261\n",
      "Iteration 17, loss = 0.07550375\n",
      "Iteration 17, loss = 0.07686683\n",
      "Iteration 10, loss = 0.09787611\n",
      "Iteration 10, loss = 0.09904684\n",
      "Iteration 10, loss = 0.09578666\n",
      "Iteration 17, loss = 0.00677054\n",
      "Iteration 17, loss = 0.07785017\n",
      "Iteration 19, loss = 0.00744720\n",
      "Iteration 19, loss = 0.00511311\n",
      "Iteration 18, loss = 0.00504566\n",
      "Iteration 18, loss = 0.02458299\n",
      "Iteration 18, loss = 0.00716677\n",
      "Iteration 18, loss = 0.07082523\n",
      "Iteration 18, loss = 0.07198236\n",
      "Iteration 18, loss = 0.00647920\n",
      "Iteration 18, loss = 0.07277475\n",
      "Iteration 20, loss = 0.00697063\n",
      "Iteration 19, loss = 0.00480899\n",
      "Iteration 20, loss = 0.00488099\n",
      "Iteration 11, loss = 0.08683049\n",
      "Iteration 19, loss = 0.02701685\n",
      "Iteration 11, loss = 0.08520365\n",
      "Iteration 19, loss = 0.00612086\n",
      "Iteration 11, loss = 0.08842130\n",
      "Iteration 19, loss = 0.06625729\n",
      "Iteration 19, loss = 0.06748034\n",
      "Iteration 19, loss = 0.00644204\n",
      "Iteration 19, loss = 0.06842550\n",
      "Iteration 21, loss = 0.00687616\n",
      "Iteration 20, loss = 0.00459247\n",
      "Iteration 21, loss = 0.00475934\n",
      "Iteration 20, loss = 0.02571520\n",
      "Iteration 20, loss = 0.00522183\n",
      "Iteration 20, loss = 0.06214270\n",
      "Iteration 20, loss = 0.06352910\n",
      "Iteration 20, loss = 0.00644526\n",
      "Iteration 20, loss = 0.06463796\n",
      "Iteration 22, loss = 0.00665912\n",
      "Iteration 12, loss = 0.07877317\n",
      "Iteration 12, loss = 0.07633042\n",
      "Iteration 12, loss = 0.07917343\n",
      "Iteration 21, loss = 0.00439925\n",
      "Iteration 22, loss = 0.00451871\n",
      "Iteration 21, loss = 0.03256831\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 21, loss = 0.00493476\n",
      "[CV 1/3; 3/36] END alpha=0.0001, hidden_layer_sizes=(50,), learning_rate_init=0.1;, score=0.895 total time=  38.9s\n",
      "Iteration 21, loss = 0.05887263\n",
      "Iteration 21, loss = 0.05980108\n",
      "[CV 1/3; 5/36] START alpha=0.0001, hidden_layer_sizes=(100,), learning_rate_init=0.01\n",
      "Iteration 21, loss = 0.00617522\n",
      "Iteration 21, loss = 0.06133192\n",
      "Iteration 23, loss = 0.00652165\n",
      "Iteration 22, loss = 0.00423267\n",
      "Iteration 23, loss = 0.00447061\n",
      "Iteration 22, loss = 0.00460014\n",
      "Iteration 22, loss = 0.05554158\n",
      "Iteration 22, loss = 0.05678308\n",
      "Iteration 13, loss = 0.07191406\n",
      "Iteration 13, loss = 0.06904203\n",
      "Iteration 22, loss = 0.01546705\n",
      "Iteration 13, loss = 0.07222846\n",
      "Iteration 22, loss = 0.05838295\n",
      "Iteration 24, loss = 0.00621851\n",
      "Iteration 23, loss = 0.00411572\n",
      "Iteration 24, loss = 0.00431971\n",
      "Iteration 23, loss = 0.00435505\n",
      "Iteration 23, loss = 0.05412106\n",
      "Iteration 23, loss = 0.05296107\n",
      "Iteration 23, loss = 0.25712690\n",
      "Iteration 23, loss = 0.05563235\n",
      "Iteration 25, loss = 0.00607688\n",
      "Iteration 24, loss = 0.00398580\n",
      "Iteration 25, loss = 0.00417620\n",
      "Iteration 14, loss = 0.06589646\n",
      "Iteration 24, loss = 0.00425426\n",
      "Iteration 14, loss = 0.06333475\n",
      "Iteration 14, loss = 0.06562334\n",
      "Iteration 24, loss = 0.05182168\n",
      "Iteration 24, loss = 0.05037047\n",
      "Iteration 1, loss = 0.42222668\n",
      "Iteration 24, loss = 0.16421292\n",
      "Iteration 24, loss = 0.05324205\n",
      "Iteration 26, loss = 0.00603114\n",
      "Iteration 25, loss = 0.00380628\n",
      "Iteration 26, loss = 0.00407610\n",
      "Iteration 25, loss = 0.00424258\n",
      "Iteration 25, loss = 0.04902691\n",
      "Iteration 25, loss = 0.04801379\n",
      "Iteration 25, loss = 0.08096061\n",
      "Iteration 25, loss = 0.05088803\n",
      "Iteration 27, loss = 0.00607278\n",
      "Iteration 15, loss = 0.06142346\n",
      "Iteration 15, loss = 0.05857210\n",
      "Iteration 26, loss = 0.00370020\n",
      "Iteration 15, loss = 0.06020561\n",
      "Iteration 27, loss = 0.00407266\n",
      "Iteration 2, loss = 0.14505056\n",
      "Iteration 26, loss = 0.04692398\n",
      "Iteration 26, loss = 0.00395990\n",
      "Iteration 26, loss = 0.04597932\n",
      "Iteration 26, loss = 0.05223912\n",
      "Iteration 26, loss = 0.04884819\n",
      "Iteration 28, loss = 0.00591458\n",
      "Iteration 27, loss = 0.00359950\n",
      "Iteration 28, loss = 0.00399080\n",
      "Iteration 27, loss = 0.04505638\n",
      "Iteration 27, loss = 0.00386174\n",
      "Iteration 27, loss = 0.04396798\n",
      "Iteration 16, loss = 0.05644958\n",
      "Iteration 16, loss = 0.05373288\n",
      "Iteration 27, loss = 0.04507398\n",
      "Iteration 16, loss = 0.05568344\n",
      "Iteration 27, loss = 0.04703109\n",
      "Iteration 29, loss = 0.11421323\n",
      "Iteration 3, loss = 0.07170694\n",
      "Iteration 28, loss = 0.00353695\n",
      "Iteration 29, loss = 0.00401595\n",
      "Iteration 28, loss = 0.00377900\n",
      "Iteration 28, loss = 0.04308075\n",
      "Iteration 28, loss = 0.04215940\n",
      "Iteration 28, loss = 0.03582840\n",
      "Iteration 28, loss = 0.04547984\n",
      "Iteration 30, loss = 0.38815562\n",
      "Iteration 29, loss = 0.00347907\n",
      "Iteration 30, loss = 0.00389770\n",
      "Iteration 17, loss = 0.05297708\n",
      "Iteration 17, loss = 0.05001755\n",
      "Iteration 29, loss = 0.00372333\n",
      "Iteration 29, loss = 0.04146690\n",
      "Iteration 29, loss = 0.04064038\n",
      "Iteration 17, loss = 0.05186166\n",
      "Iteration 4, loss = 0.04295675\n",
      "Iteration 29, loss = 0.03290435\n",
      "Iteration 31, loss = 0.17441381\n",
      "Iteration 29, loss = 0.04386474\n",
      "Iteration 30, loss = 0.00342313\n",
      "Iteration 31, loss = 0.00388824\n",
      "Iteration 30, loss = 0.00368649\n",
      "Iteration 30, loss = 0.03993974\n",
      "Iteration 30, loss = 0.03903359\n",
      "Iteration 30, loss = 0.02874127\n",
      "Iteration 30, loss = 0.04223934\n",
      "Iteration 32, loss = 0.07515518\n",
      "Iteration 18, loss = 0.04998894\n",
      "Iteration 18, loss = 0.04667579\n",
      "Iteration 31, loss = 0.00338141\n",
      "Iteration 18, loss = 0.04883513\n",
      "Iteration 32, loss = 0.00375656\n",
      "Iteration 31, loss = 0.00363312\n",
      "Iteration 31, loss = 0.03880478\n",
      "Iteration 5, loss = 0.02736491\n",
      "Iteration 31, loss = 0.03776363\n",
      "Iteration 31, loss = 0.02824791\n",
      "Iteration 31, loss = 0.04088716\n",
      "Iteration 33, loss = 0.05314289\n",
      "Iteration 32, loss = 0.00338144\n",
      "Iteration 33, loss = 0.00367649\n",
      "Iteration 32, loss = 0.00360270\n",
      "Iteration 32, loss = 0.03731720\n",
      "Iteration 32, loss = 0.03637987\n",
      "Iteration 32, loss = 0.02638987\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 19, loss = 0.04379684\n",
      "Iteration 19, loss = 0.04691697\n",
      "Iteration 32, loss = 0.03982065\n",
      "Iteration 19, loss = 0.04568314\n",
      "Iteration 34, loss = 0.04798889\n",
      "[CV 2/3; 3/36] END alpha=0.0001, hidden_layer_sizes=(50,), learning_rate_init=0.1;, score=0.887 total time=  59.1s\n",
      "Iteration 33, loss = 0.00333673\n",
      "Iteration 6, loss = 0.02116097\n",
      "[CV 2/3; 5/36] START alpha=0.0001, hidden_layer_sizes=(100,), learning_rate_init=0.01\n",
      "Iteration 34, loss = 0.00364160\n",
      "Iteration 33, loss = 0.00355308\n",
      "Iteration 33, loss = 0.03602562\n",
      "Iteration 33, loss = 0.03517448\n",
      "Iteration 33, loss = 0.03856926\n",
      "Iteration 35, loss = 0.03994998\n",
      "Iteration 34, loss = 0.00328367\n",
      "Iteration 35, loss = 0.00353341\n",
      "Iteration 34, loss = 0.00348600\n",
      "Iteration 34, loss = 0.03484312\n",
      "Iteration 20, loss = 0.04166661\n",
      "Iteration 20, loss = 0.04480604\n",
      "Iteration 34, loss = 0.03381353\n",
      "Iteration 20, loss = 0.04295803\n",
      "Iteration 7, loss = 0.01550422\n",
      "Iteration 34, loss = 0.03722643\n",
      "Iteration 36, loss = 0.03652060\n",
      "Iteration 35, loss = 0.00322612\n",
      "Iteration 36, loss = 0.00358528\n",
      "Iteration 35, loss = 0.03386566\n",
      "Iteration 35, loss = 0.00345402\n",
      "Iteration 35, loss = 0.03274926\n",
      "Iteration 35, loss = 0.03664998\n",
      "Iteration 37, loss = 0.03483382\n",
      "Iteration 36, loss = 0.00322162\n",
      "Iteration 1, loss = 0.41800207\n",
      "Iteration 21, loss = 0.04223470\n",
      "Iteration 21, loss = 0.03895901\n",
      "Iteration 37, loss = 0.00364812\n",
      "Iteration 21, loss = 0.04047595\n",
      "Iteration 36, loss = 0.03294718\n",
      "Iteration 36, loss = 0.00342481\n",
      "Iteration 8, loss = 0.01151888\n",
      "Iteration 36, loss = 0.03189472\n",
      "Iteration 36, loss = 0.03527407\n",
      "Iteration 38, loss = 0.03382981\n",
      "Iteration 37, loss = 0.00320265\n",
      "Iteration 38, loss = 0.00368504\n",
      "Iteration 37, loss = 0.03202795\n",
      "Iteration 37, loss = 0.00341498\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 37, loss = 0.03091557\n",
      "[CV 2/3; 2/36] END alpha=0.0001, hidden_layer_sizes=(50,), learning_rate_init=0.01;, score=0.899 total time= 1.1min\n",
      "Iteration 37, loss = 0.03419188\n",
      "Iteration 2, loss = 0.13599597\n",
      "[CV 3/3; 5/36] START alpha=0.0001, hidden_layer_sizes=(100,), learning_rate_init=0.01\n",
      "Iteration 22, loss = 0.04086692\n",
      "Iteration 22, loss = 0.03697396\n",
      "Iteration 39, loss = 0.03282693\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 0.03857187\n",
      "Iteration 38, loss = 0.00319206\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/3; 3/36] END alpha=0.0001, hidden_layer_sizes=(50,), learning_rate_init=0.1;, score=0.878 total time= 1.1min\n",
      "Iteration 9, loss = 0.01021812\n",
      "[CV 1/3; 2/36] END alpha=0.0001, hidden_layer_sizes=(50,), learning_rate_init=0.01;, score=0.904 total time= 1.1min\n",
      "Iteration 39, loss = 0.00360951\n",
      "Iteration 38, loss = 0.03111876\n",
      "[CV 1/3; 6/36] START alpha=0.0001, hidden_layer_sizes=(100,), learning_rate_init=0.1\n",
      "[CV 2/3; 6/36] START alpha=0.0001, hidden_layer_sizes=(100,), learning_rate_init=0.1\n",
      "Iteration 38, loss = 0.03009895\n",
      "Iteration 38, loss = 0.03335900\n",
      "Iteration 40, loss = 0.00364820\n",
      "Iteration 39, loss = 0.03023171\n",
      "Iteration 3, loss = 0.07376389\n",
      "Iteration 39, loss = 0.02920168\n",
      "Iteration 23, loss = 0.03839575\n",
      "Iteration 23, loss = 0.03540374\n",
      "Iteration 23, loss = 0.03655849\n",
      "Iteration 39, loss = 0.03266354\n",
      "Iteration 10, loss = 0.00821343\n",
      "Iteration 41, loss = 0.00379630\n",
      "Iteration 40, loss = 0.02935295\n",
      "Iteration 40, loss = 0.02822893\n",
      "Iteration 1, loss = 0.42256421\n",
      "Iteration 40, loss = 0.03192982\n",
      "Iteration 1, loss = 0.49023053\n",
      "Iteration 1, loss = 0.42784788\n",
      "Iteration 42, loss = 0.00358420\n",
      "Iteration 4, loss = 0.05095769\n",
      "Iteration 41, loss = 0.02873167\n",
      "Iteration 24, loss = 0.03690454\n",
      "Iteration 24, loss = 0.03377455\n",
      "Iteration 24, loss = 0.03488863\n",
      "Iteration 41, loss = 0.02740162\n",
      "Iteration 11, loss = 0.00684240\n",
      "Iteration 41, loss = 0.03098624\n",
      "Iteration 43, loss = 0.00349530\n",
      "Iteration 42, loss = 0.02785554\n",
      "Iteration 2, loss = 0.15531134\n",
      "Iteration 42, loss = 0.02678823\n",
      "Iteration 2, loss = 0.17069920\n",
      "Iteration 42, loss = 0.03031636\n",
      "Iteration 2, loss = 0.12858459\n",
      "Iteration 5, loss = 0.03595658\n",
      "Iteration 25, loss = 0.03516118\n",
      "Iteration 25, loss = 0.03209148\n",
      "Iteration 25, loss = 0.03335420\n",
      "Iteration 44, loss = 0.00337678\n",
      "Iteration 43, loss = 0.02732520\n",
      "Iteration 12, loss = 0.00595400\n",
      "Iteration 43, loss = 0.02593455\n",
      "Iteration 43, loss = 0.02956951\n",
      "Iteration 3, loss = 0.07585152\n",
      "Iteration 45, loss = 0.00342789\n",
      "Iteration 44, loss = 0.02659956\n",
      "Iteration 3, loss = 0.07372690\n",
      "Iteration 44, loss = 0.02515954\n",
      "Iteration 6, loss = 0.02731862\n",
      "Iteration 3, loss = 0.05853713\n",
      "Iteration 26, loss = 0.03076992\n",
      "Iteration 26, loss = 0.03371355\n",
      "Iteration 44, loss = 0.02867218\n",
      "Iteration 26, loss = 0.03208141\n",
      "Iteration 13, loss = 0.00536259\n",
      "Iteration 46, loss = 0.00343049\n",
      "Iteration 45, loss = 0.02588919\n",
      "Iteration 45, loss = 0.02477351\n",
      "Iteration 45, loss = 0.02815381\n",
      "Iteration 4, loss = 0.04395916\n",
      "Iteration 46, loss = 0.02535852\n",
      "Iteration 4, loss = 0.04161883\n",
      "Iteration 47, loss = 0.00338532\n",
      "Iteration 7, loss = 0.02168907\n",
      "Iteration 27, loss = 0.02945871\n",
      "Iteration 4, loss = 0.03410084\n",
      "Iteration 27, loss = 0.03248397\n",
      "Iteration 46, loss = 0.02396193\n",
      "Iteration 27, loss = 0.03070251\n",
      "Iteration 46, loss = 0.02750071\n",
      "Iteration 14, loss = 0.00495442\n",
      "Iteration 47, loss = 0.02474304\n",
      "Iteration 48, loss = 0.00335764\n",
      "Iteration 47, loss = 0.02339807\n",
      "Iteration 5, loss = 0.02878816\n",
      "Iteration 47, loss = 0.02678607\n",
      "Iteration 5, loss = 0.03196143\n",
      "Iteration 48, loss = 0.02422146\n",
      "Iteration 8, loss = 0.01721239\n",
      "Iteration 49, loss = 0.00341559\n",
      "Iteration 28, loss = 0.02827405\n",
      "Iteration 28, loss = 0.03127109\n",
      "Iteration 5, loss = 0.02047757\n",
      "Iteration 28, loss = 0.02950801\n",
      "Iteration 15, loss = 0.00474451\n",
      "Iteration 48, loss = 0.02268044\n",
      "Iteration 48, loss = 0.02618714\n",
      "Iteration 49, loss = 0.02364641\n",
      "Iteration 6, loss = 0.02318872\n",
      "Iteration 50, loss = 0.00327183\n",
      "Iteration 49, loss = 0.02214723\n",
      "Iteration 6, loss = 0.02303241\n",
      "Iteration 49, loss = 0.02560564\n",
      "Iteration 9, loss = 0.01365195\n",
      "Iteration 29, loss = 0.03034932\n",
      "Iteration 29, loss = 0.02713297\n",
      "Iteration 6, loss = 0.01655084\n",
      "Iteration 29, loss = 0.02863199\n",
      "Iteration 50, loss = 0.02314854\n",
      "Iteration 51, loss = 0.00336548\n",
      "Iteration 16, loss = 0.00448893\n",
      "Iteration 50, loss = 0.02156019\n",
      "Iteration 50, loss = 0.02504568\n",
      "Iteration 7, loss = 0.01774434\n",
      "Iteration 51, loss = 0.02278387\n",
      "Iteration 52, loss = 0.00326089\n",
      "Iteration 7, loss = 0.01903432\n",
      "Iteration 51, loss = 0.02104556\n",
      "Iteration 10, loss = 0.01154407\n",
      "Iteration 30, loss = 0.02911835\n",
      "Iteration 30, loss = 0.02606020\n",
      "Iteration 51, loss = 0.02461024\n",
      "Iteration 7, loss = 0.01540429\n",
      "Iteration 30, loss = 0.02743278\n",
      "Iteration 17, loss = 0.00426669\n",
      "Iteration 52, loss = 0.02223987\n",
      "Iteration 53, loss = 0.00326872\n",
      "Iteration 52, loss = 0.02048322\n",
      "Iteration 8, loss = 0.01348688\n",
      "Iteration 52, loss = 0.02420515\n",
      "Iteration 8, loss = 0.01625871\n",
      "Iteration 53, loss = 0.02183024\n",
      "Iteration 54, loss = 0.00332273\n",
      "Iteration 11, loss = 0.00950132\n",
      "Iteration 31, loss = 0.02535123\n",
      "Iteration 31, loss = 0.02874250\n",
      "Iteration 31, loss = 0.02671259\n",
      "Iteration 8, loss = 0.01584624\n",
      "Iteration 53, loss = 0.02010476\n",
      "Iteration 18, loss = 0.00408408\n",
      "Iteration 53, loss = 0.02374650\n",
      "Iteration 54, loss = 0.02128683\n",
      "Iteration 55, loss = 0.00328508\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 0.01034410\n",
      "[CV 3/3; 2/36] END alpha=0.0001, hidden_layer_sizes=(50,), learning_rate_init=0.01;, score=0.897 total time= 1.6min\n",
      "Iteration 54, loss = 0.01950574\n",
      "[CV 3/3; 6/36] START alpha=0.0001, hidden_layer_sizes=(100,), learning_rate_init=0.1\n",
      "Iteration 54, loss = 0.02296681\n",
      "Iteration 9, loss = 0.01486577\n",
      "Iteration 12, loss = 0.00793585\n",
      "Iteration 32, loss = 0.02427266\n",
      "Iteration 55, loss = 0.02077337\n",
      "Iteration 32, loss = 0.02730857\n",
      "Iteration 32, loss = 0.02569554\n",
      "Iteration 9, loss = 0.01437837\n",
      "Iteration 19, loss = 0.00392131\n",
      "Iteration 55, loss = 0.01900645\n",
      "Iteration 55, loss = 0.02263360\n",
      "Iteration 10, loss = 0.00857996\n",
      "Iteration 56, loss = 0.02039606\n",
      "Iteration 10, loss = 0.01365740\n",
      "Iteration 56, loss = 0.01866433\n",
      "Iteration 56, loss = 0.02212389\n",
      "Iteration 13, loss = 0.00663823\n",
      "Iteration 33, loss = 0.02345800\n",
      "Iteration 33, loss = 0.02656364\n",
      "Iteration 33, loss = 0.02465175\n",
      "Iteration 20, loss = 0.00381260\n",
      "Iteration 57, loss = 0.01998472\n",
      "Iteration 10, loss = 0.02849090\n",
      "Iteration 57, loss = 0.01819558\n",
      "Iteration 11, loss = 0.00714730\n",
      "Iteration 1, loss = 0.44649810\n",
      "Iteration 57, loss = 0.02178698\n",
      "Iteration 58, loss = 0.01968693\n",
      "Iteration 11, loss = 0.01275621\n",
      "Iteration 34, loss = 0.02570763\n",
      "Iteration 34, loss = 0.02268620\n",
      "Iteration 14, loss = 0.00613652\n",
      "Iteration 58, loss = 0.01774796\n",
      "Iteration 34, loss = 0.02397557\n",
      "Iteration 58, loss = 0.02132091\n",
      "Iteration 21, loss = 0.00371695\n",
      "Iteration 11, loss = 0.04796676\n",
      "Iteration 59, loss = 0.01919448\n",
      "Iteration 2, loss = 0.14157884\n",
      "Iteration 12, loss = 0.00657008\n",
      "Iteration 59, loss = 0.01731150\n",
      "Iteration 59, loss = 0.02090284\n",
      "Iteration 12, loss = 0.01198100\n",
      "Iteration 60, loss = 0.01892206\n",
      "Iteration 35, loss = 0.02483666\n",
      "Iteration 15, loss = 0.00557364\n",
      "Iteration 35, loss = 0.02193358\n",
      "Iteration 35, loss = 0.02343486\n",
      "Iteration 22, loss = 0.00361908\n",
      "Iteration 60, loss = 0.01692772\n",
      "Iteration 12, loss = 0.07294253\n",
      "Iteration 60, loss = 0.02073775\n",
      "Iteration 13, loss = 0.00581899\n",
      "Iteration 3, loss = 0.05593068\n",
      "Iteration 61, loss = 0.01855525\n",
      "Iteration 61, loss = 0.01649857\n",
      "Iteration 13, loss = 0.01130758\n",
      "Iteration 61, loss = 0.02014677\n",
      "Iteration 16, loss = 0.00530571\n",
      "Iteration 36, loss = 0.02416906\n",
      "Iteration 36, loss = 0.02140200\n",
      "Iteration 62, loss = 0.01834569\n",
      "Iteration 36, loss = 0.02264792\n",
      "Iteration 23, loss = 0.00355739\n",
      "Iteration 13, loss = 0.06727784\n",
      "Iteration 62, loss = 0.01620891\n",
      "Iteration 62, loss = 0.01993807\n",
      "Iteration 14, loss = 0.00542092\n",
      "Iteration 4, loss = 0.02796180\n",
      "Iteration 63, loss = 0.01778117\n",
      "Iteration 14, loss = 0.01069975\n",
      "Iteration 63, loss = 0.01584770\n",
      "Iteration 63, loss = 0.01931496\n",
      "Iteration 37, loss = 0.02349041\n",
      "Iteration 17, loss = 0.00501324\n",
      "Iteration 37, loss = 0.02046950\n",
      "Iteration 37, loss = 0.02193249\n",
      "Iteration 24, loss = 0.00353407\n",
      "Iteration 64, loss = 0.01755307\n",
      "Iteration 14, loss = 0.05707527\n",
      "Iteration 5, loss = 0.01898015\n",
      "Iteration 15, loss = 0.00501861\n",
      "Iteration 64, loss = 0.01542484\n",
      "Iteration 64, loss = 0.01923920\n",
      "Iteration 15, loss = 0.01013295\n",
      "Iteration 65, loss = 0.01724048\n",
      "Iteration 38, loss = 0.02307643\n",
      "Iteration 18, loss = 0.00478139\n",
      "Iteration 38, loss = 0.01994251\n",
      "Iteration 38, loss = 0.02135800\n",
      "Iteration 65, loss = 0.01516742\n",
      "Iteration 25, loss = 0.00347936\n",
      "Iteration 65, loss = 0.01880774\n",
      "Iteration 15, loss = 0.05631255\n",
      "Iteration 66, loss = 0.01684511\n",
      "Iteration 6, loss = 0.01635926\n",
      "Iteration 16, loss = 0.00490828\n",
      "Iteration 66, loss = 0.01482270\n",
      "Iteration 16, loss = 0.00965376\n",
      "Iteration 66, loss = 0.01833974\n",
      "Iteration 67, loss = 0.01652535\n",
      "Iteration 39, loss = 0.02212714\n",
      "Iteration 39, loss = 0.01930711\n",
      "Iteration 19, loss = 0.00456325\n",
      "Iteration 39, loss = 0.02068397\n",
      "Iteration 26, loss = 0.00339143\n",
      "Iteration 67, loss = 0.01453101\n",
      "Iteration 67, loss = 0.01798563\n",
      "Iteration 16, loss = 0.04408400\n",
      "Iteration 17, loss = 0.00461256\n",
      "Iteration 7, loss = 0.01582403\n",
      "Iteration 68, loss = 0.01629283\n",
      "Iteration 17, loss = 0.00921081\n",
      "Iteration 68, loss = 0.01421870\n",
      "Iteration 68, loss = 0.01782576\n",
      "Iteration 20, loss = 0.00437116\n",
      "Iteration 40, loss = 0.02150619\n",
      "Iteration 40, loss = 0.01897548\n",
      "Iteration 40, loss = 0.02004019\n",
      "Iteration 69, loss = 0.01599162\n",
      "Iteration 27, loss = 0.00335783\n",
      "Iteration 17, loss = 0.03574175\n",
      "Iteration 69, loss = 0.01391229\n",
      "Iteration 18, loss = 0.00443748\n",
      "Iteration 8, loss = 0.01377913\n",
      "Iteration 69, loss = 0.01737965\n",
      "Iteration 70, loss = 0.01571875\n",
      "Iteration 18, loss = 0.00885685\n",
      "Iteration 70, loss = 0.01355695\n",
      "Iteration 41, loss = 0.01813494\n",
      "Iteration 21, loss = 0.00421941\n",
      "Iteration 41, loss = 0.02083850\n",
      "Iteration 41, loss = 0.01954075\n",
      "Iteration 28, loss = 0.00330814\n",
      "Iteration 70, loss = 0.01707966\n",
      "Iteration 71, loss = 0.01540528\n",
      "Iteration 18, loss = 0.03963608\n",
      "Iteration 9, loss = 0.01299397\n",
      "Iteration 19, loss = 0.00427380\n",
      "Iteration 71, loss = 0.01331445\n",
      "Iteration 71, loss = 0.01672895\n",
      "Iteration 19, loss = 0.00856651\n",
      "Iteration 72, loss = 0.01523120\n",
      "Iteration 42, loss = 0.02030690\n",
      "Iteration 42, loss = 0.01744508\n",
      "Iteration 22, loss = 0.00414060\n",
      "Iteration 42, loss = 0.01903610\n",
      "Iteration 29, loss = 0.00327004\n",
      "Iteration 72, loss = 0.01300424\n",
      "Iteration 72, loss = 0.01658774\n",
      "Iteration 73, loss = 0.01491955\n",
      "Iteration 20, loss = 0.00420024\n",
      "Iteration 10, loss = 0.01203102\n",
      "Iteration 19, loss = 0.03568727\n",
      "Iteration 73, loss = 0.01288600\n",
      "Iteration 20, loss = 0.00872250\n",
      "Iteration 73, loss = 0.01617643\n",
      "Iteration 74, loss = 0.01465307\n",
      "Iteration 43, loss = 0.01966453\n",
      "Iteration 23, loss = 0.00398601\n",
      "Iteration 43, loss = 0.01700605\n",
      "Iteration 43, loss = 0.01852602\n",
      "Iteration 30, loss = 0.00323327\n",
      "Iteration 74, loss = 0.01249183\n",
      "Iteration 21, loss = 0.00411780\n",
      "Iteration 74, loss = 0.01605273\n",
      "Iteration 11, loss = 0.01133368\n",
      "Iteration 20, loss = 0.04327612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 75, loss = 0.01439377\n",
      "[CV 2/3; 6/36] END alpha=0.0001, hidden_layer_sizes=(100,), learning_rate_init=0.1;, score=0.887 total time= 1.1min\n",
      "Iteration 21, loss = 0.02489117\n",
      "[CV 1/3; 7/36] START alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate_init=0.001\n",
      "Iteration 75, loss = 0.01229908\n",
      "Iteration 75, loss = 0.01565377\n",
      "Iteration 44, loss = 0.01944779\n",
      "Iteration 44, loss = 0.01661134\n",
      "Iteration 24, loss = 0.00391156\n",
      "Iteration 44, loss = 0.01819261\n",
      "Iteration 31, loss = 0.00321670\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 76, loss = 0.01416519\n",
      "[CV 1/3; 5/36] END alpha=0.0001, hidden_layer_sizes=(100,), learning_rate_init=0.01;, score=0.905 total time= 1.6min\n",
      "[CV 2/3; 7/36] START alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate_init=0.001\n",
      "Iteration 22, loss = 0.00403656\n",
      "Iteration 76, loss = 0.01206914\n",
      "Iteration 12, loss = 0.01138704\n",
      "Iteration 76, loss = 0.01549027\n",
      "Iteration 77, loss = 0.01391089\n",
      "Iteration 22, loss = 0.23070204\n",
      "Iteration 45, loss = 0.01884201\n",
      "Iteration 77, loss = 0.01174528\n",
      "Iteration 25, loss = 0.00391888\n",
      "Iteration 45, loss = 0.01607013\n",
      "Iteration 45, loss = 0.01769436\n",
      "Iteration 77, loss = 0.01517384\n",
      "Iteration 78, loss = 0.01368166\n",
      "Iteration 1, loss = 0.63795698\n",
      "Iteration 23, loss = 0.00398495\n",
      "Iteration 13, loss = 0.01033417\n",
      "Iteration 78, loss = 0.01160110\n",
      "Iteration 78, loss = 0.01509962\n",
      "Iteration 23, loss = 0.15723106\n",
      "Iteration 79, loss = 0.01366420\n",
      "Iteration 1, loss = 0.64059904\n",
      "Iteration 46, loss = 0.01843951\n",
      "Iteration 26, loss = 0.00418816\n",
      "Iteration 46, loss = 0.01555400\n",
      "Iteration 46, loss = 0.01712064\n",
      "Iteration 79, loss = 0.01124921\n",
      "Iteration 79, loss = 0.01466444\n",
      "Iteration 2, loss = 0.40966519\n",
      "Iteration 24, loss = 0.00398277\n",
      "Iteration 14, loss = 0.00995924\n",
      "Iteration 80, loss = 0.01326174\n",
      "Iteration 24, loss = 0.10473773\n",
      "Iteration 80, loss = 0.01113783\n",
      "Iteration 80, loss = 0.01434054\n",
      "Iteration 2, loss = 0.40975830\n",
      "Iteration 47, loss = 0.01801497\n",
      "Iteration 81, loss = 0.01314388\n",
      "Iteration 47, loss = 0.01527571\n",
      "Iteration 27, loss = 0.00374451\n",
      "Iteration 47, loss = 0.01679864\n",
      "Iteration 81, loss = 0.01092539\n",
      "Iteration 3, loss = 0.22636295\n",
      "Iteration 25, loss = 0.00395826\n",
      "Iteration 15, loss = 0.00920419\n",
      "Iteration 81, loss = 0.01430828\n",
      "Iteration 82, loss = 0.01283243\n",
      "Iteration 25, loss = 0.08693128\n",
      "Iteration 82, loss = 0.01067290\n",
      "Iteration 48, loss = 0.01743556\n",
      "Iteration 3, loss = 0.22708994\n",
      "Iteration 82, loss = 0.01400872\n",
      "Iteration 48, loss = 0.01470339\n",
      "Iteration 28, loss = 0.00361534\n",
      "Iteration 48, loss = 0.01634062\n",
      "Iteration 83, loss = 0.01257241\n",
      "Iteration 4, loss = 0.13431807\n",
      "Iteration 26, loss = 0.00509949\n",
      "Iteration 16, loss = 0.00937125\n",
      "Iteration 83, loss = 0.01045799\n",
      "Iteration 83, loss = 0.01383124\n",
      "Iteration 26, loss = 0.05593588\n",
      "Iteration 84, loss = 0.01239026\n",
      "Iteration 49, loss = 0.01716402\n",
      "Iteration 4, loss = 0.13401126\n",
      "Iteration 49, loss = 0.01596345\n",
      "Iteration 29, loss = 0.00356598\n",
      "Iteration 49, loss = 0.01429435\n",
      "Iteration 84, loss = 0.01033673\n",
      "Iteration 84, loss = 0.01346288\n",
      "Iteration 85, loss = 0.01215366\n",
      "Iteration 5, loss = 0.08897695\n",
      "Iteration 27, loss = 0.00405595\n",
      "Iteration 17, loss = 0.00883391\n",
      "Iteration 85, loss = 0.01010665\n",
      "Iteration 27, loss = 0.04865452\n",
      "Iteration 85, loss = 0.01325549\n",
      "Iteration 86, loss = 0.01196677\n",
      "Iteration 50, loss = 0.01649951\n",
      "Iteration 5, loss = 0.08895409\n",
      "Iteration 50, loss = 0.01393836\n",
      "Iteration 30, loss = 0.00350087\n",
      "Iteration 50, loss = 0.01558964\n",
      "Iteration 86, loss = 0.00996344\n",
      "Iteration 86, loss = 0.01305835\n",
      "Iteration 6, loss = 0.06194028\n",
      "Iteration 18, loss = 0.00847947\n",
      "Iteration 28, loss = 0.00373498\n",
      "Iteration 87, loss = 0.01184586\n",
      "Iteration 28, loss = 0.04122517\n",
      "Iteration 87, loss = 0.00974420\n",
      "Iteration 87, loss = 0.01285323\n",
      "Iteration 51, loss = 0.01612105\n",
      "Iteration 6, loss = 0.06343880\n",
      "Iteration 51, loss = 0.01358365\n",
      "Iteration 51, loss = 0.01523806\n",
      "Iteration 31, loss = 0.00347275\n",
      "Iteration 88, loss = 0.01161753\n",
      "Iteration 7, loss = 0.04657300\n",
      "Iteration 19, loss = 0.00795448\n",
      "Iteration 88, loss = 0.00953872\n",
      "Iteration 29, loss = 0.00373546\n",
      "Iteration 88, loss = 0.01268959\n",
      "Iteration 89, loss = 0.01154127\n",
      "Iteration 29, loss = 0.03787351\n",
      "Iteration 52, loss = 0.01580481\n",
      "Iteration 89, loss = 0.00939592\n",
      "Iteration 7, loss = 0.04970103\n",
      "Iteration 52, loss = 0.01320377\n",
      "Iteration 52, loss = 0.01483043\n",
      "Iteration 32, loss = 0.00342125\n",
      "Iteration 89, loss = 0.01252935\n",
      "Iteration 90, loss = 0.01125975\n",
      "Iteration 8, loss = 0.03659978\n",
      "Iteration 20, loss = 0.00782237\n",
      "Iteration 30, loss = 0.00376662\n",
      "Iteration 90, loss = 0.00924237\n",
      "Iteration 90, loss = 0.01232905\n",
      "Iteration 30, loss = 0.03554124\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 91, loss = 0.01113208\n",
      "[CV 1/3; 6/36] END alpha=0.0001, hidden_layer_sizes=(100,), learning_rate_init=0.1;, score=0.883 total time= 1.5min\n",
      "Iteration 53, loss = 0.01531817\n",
      "[CV 3/3; 7/36] START alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate_init=0.001\n",
      "Iteration 53, loss = 0.01286121\n",
      "Iteration 8, loss = 0.03958852\n",
      "Iteration 53, loss = 0.01453314\n",
      "Iteration 33, loss = 0.00344341\n",
      "Iteration 91, loss = 0.00908253\n",
      "Iteration 91, loss = 0.01210331\n",
      "Iteration 9, loss = 0.03017082\n",
      "Iteration 92, loss = 0.01091142\n",
      "Iteration 21, loss = 0.00751183\n",
      "Iteration 31, loss = 0.00370988\n",
      "Iteration 92, loss = 0.00892766\n",
      "Iteration 92, loss = 0.01187681\n",
      "Iteration 54, loss = 0.01502538\n",
      "Iteration 93, loss = 0.01074590\n",
      "Iteration 54, loss = 0.01254531\n",
      "Iteration 9, loss = 0.03480907\n",
      "Iteration 54, loss = 0.01411639\n",
      "Iteration 34, loss = 0.00338994\n",
      "Iteration 93, loss = 0.00876550\n",
      "Iteration 10, loss = 0.02458836\n",
      "Iteration 93, loss = 0.01177861\n",
      "Iteration 22, loss = 0.00721769\n",
      "Iteration 32, loss = 0.00357720\n",
      "Iteration 94, loss = 0.01057417\n",
      "Iteration 1, loss = 0.64074692\n",
      "Iteration 94, loss = 0.00859430\n",
      "Iteration 55, loss = 0.01451846\n",
      "Iteration 94, loss = 0.01161810\n",
      "Iteration 55, loss = 0.01206291\n",
      "Iteration 10, loss = 0.02894857\n",
      "Iteration 55, loss = 0.01371934\n",
      "Iteration 35, loss = 0.00334813\n",
      "Iteration 95, loss = 0.01050391\n",
      "Iteration 23, loss = 0.00744819\n",
      "Iteration 11, loss = 0.02084307\n",
      "Iteration 33, loss = 0.00355057\n",
      "Iteration 95, loss = 0.00844838\n",
      "Iteration 2, loss = 0.41583231\n",
      "Iteration 95, loss = 0.01139100\n",
      "Iteration 96, loss = 0.01028239\n",
      "Iteration 56, loss = 0.01439051\n",
      "Iteration 56, loss = 0.01186551\n",
      "Iteration 11, loss = 0.02433692\n",
      "Iteration 96, loss = 0.00833734\n",
      "Iteration 56, loss = 0.01344706\n",
      "Iteration 36, loss = 0.00332802\n",
      "Iteration 96, loss = 0.01117374\n",
      "Iteration 97, loss = 0.01011791\n",
      "Iteration 12, loss = 0.01784963\n",
      "Iteration 24, loss = 0.11218051\n",
      "Iteration 34, loss = 0.00348840\n",
      "Iteration 3, loss = 0.23443309\n",
      "Iteration 97, loss = 0.00813202\n",
      "Iteration 97, loss = 0.01099638\n",
      "Iteration 98, loss = 0.01001551\n",
      "Iteration 57, loss = 0.01397828\n",
      "Iteration 57, loss = 0.01151755\n",
      "Iteration 12, loss = 0.02122350\n",
      "Iteration 57, loss = 0.01315291\n",
      "Iteration 37, loss = 0.00332089\n",
      "Iteration 98, loss = 0.00803873\n",
      "Iteration 98, loss = 0.01090153\n",
      "Iteration 13, loss = 0.01497046\n",
      "Iteration 25, loss = 0.29602871\n",
      "Iteration 35, loss = 0.00359130\n",
      "Iteration 99, loss = 0.00979384\n",
      "Iteration 4, loss = 0.14018293\n",
      "Iteration 99, loss = 0.00793706\n",
      "Iteration 99, loss = 0.01080065\n",
      "Iteration 58, loss = 0.01362176\n",
      "Iteration 100, loss = 0.00966508\n",
      "Iteration 58, loss = 0.01125445\n",
      "Iteration 13, loss = 0.01872841\n",
      "Iteration 58, loss = 0.01286898\n",
      "Iteration 38, loss = 0.00327604\n",
      "Iteration 100, loss = 0.00779624\n",
      "Iteration 14, loss = 0.01287551\n",
      "Iteration 26, loss = 0.18169969\n",
      "Iteration 100, loss = 0.01059835\n",
      "Iteration 36, loss = 0.00344660\n",
      "Iteration 5, loss = 0.09122502\n",
      "Iteration 101, loss = 0.00965449\n",
      "Iteration 101, loss = 0.00773619\n",
      "Iteration 59, loss = 0.01344709\n",
      "Iteration 59, loss = 0.01099871\n",
      "Iteration 101, loss = 0.01038544\n",
      "Iteration 14, loss = 0.01631307\n",
      "Iteration 59, loss = 0.01259153\n",
      "Iteration 39, loss = 0.00325071\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 102, loss = 0.00942124\n",
      "[CV 2/3; 5/36] END alpha=0.0001, hidden_layer_sizes=(100,), learning_rate_init=0.01;, score=0.895 total time= 2.0min\n",
      "[CV 1/3; 8/36] START alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate_init=0.01\n",
      "Iteration 15, loss = 0.01082474\n",
      "Iteration 27, loss = 0.09348351\n",
      "Iteration 37, loss = 0.00379558\n",
      "Iteration 102, loss = 0.00758112\n",
      "Iteration 6, loss = 0.06376197\n",
      "Iteration 102, loss = 0.01031004\n",
      "Iteration 103, loss = 0.00923943\n",
      "Iteration 60, loss = 0.01328291\n",
      "Iteration 60, loss = 0.01061906\n",
      "Iteration 15, loss = 0.01407412\n",
      "Iteration 103, loss = 0.00743557\n",
      "Iteration 60, loss = 0.01234630\n",
      "Iteration 103, loss = 0.01011168\n",
      "Iteration 104, loss = 0.00909977\n",
      "Iteration 16, loss = 0.00900021\n",
      "Iteration 28, loss = 0.07424700\n",
      "Iteration 38, loss = 0.00353816\n",
      "Iteration 7, loss = 0.04896497\n",
      "Iteration 104, loss = 0.00731065\n",
      "Iteration 104, loss = 0.00998045\n",
      "Iteration 105, loss = 0.00896130\n",
      "Iteration 61, loss = 0.01301069\n",
      "Iteration 61, loss = 0.01040425\n",
      "Iteration 16, loss = 0.01214772\n",
      "Iteration 61, loss = 0.01204033\n",
      "Iteration 1, loss = 0.41692611\n",
      "Iteration 105, loss = 0.00724256\n",
      "Iteration 105, loss = 0.00984205\n",
      "Iteration 29, loss = 0.05341453\n",
      "Iteration 39, loss = 0.00363316\n",
      "Iteration 17, loss = 0.00789098\n",
      "Iteration 106, loss = 0.00890602\n",
      "Iteration 8, loss = 0.03775275\n",
      "Iteration 106, loss = 0.00713142\n",
      "Iteration 106, loss = 0.00970309\n",
      "Iteration 62, loss = 0.01240298\n",
      "Iteration 62, loss = 0.01013625\n",
      "Iteration 107, loss = 0.00872685\n",
      "Iteration 62, loss = 0.01169672\n",
      "Iteration 17, loss = 0.01034295\n",
      "Iteration 2, loss = 0.12911958\n",
      "Iteration 107, loss = 0.00699840\n",
      "Iteration 30, loss = 0.04718169\n",
      "Iteration 40, loss = 0.00351690\n",
      "Iteration 18, loss = 0.00682073\n",
      "Iteration 107, loss = 0.00956624\n",
      "Iteration 108, loss = 0.00859299\n",
      "Iteration 9, loss = 0.03053906\n",
      "Iteration 63, loss = 0.00992820\n",
      "Iteration 63, loss = 0.01221408\n",
      "Iteration 108, loss = 0.00689631\n",
      "Iteration 108, loss = 0.00945428\n",
      "Iteration 18, loss = 0.00939623\n",
      "Iteration 63, loss = 0.01163435\n",
      "Iteration 3, loss = 0.06274290\n",
      "Iteration 109, loss = 0.00866028\n",
      "Iteration 31, loss = 0.04406906\n",
      "Iteration 41, loss = 0.00355766\n",
      "Iteration 19, loss = 0.00630213\n",
      "Iteration 109, loss = 0.00683215\n",
      "Iteration 109, loss = 0.00931052\n",
      "Iteration 10, loss = 0.02498647\n",
      "Iteration 110, loss = 0.00849309\n",
      "Iteration 64, loss = 0.00966071\n",
      "Iteration 64, loss = 0.01191900\n",
      "Iteration 64, loss = 0.01121558\n",
      "Iteration 19, loss = 0.00840129\n",
      "Iteration 4, loss = 0.02691823\n",
      "Iteration 110, loss = 0.00675389\n",
      "Iteration 110, loss = 0.00918623\n",
      "Iteration 32, loss = 0.04105549\n",
      "Iteration 111, loss = 0.00830215\n",
      "Iteration 42, loss = 0.00338784\n",
      "Iteration 20, loss = 0.00556048\n",
      "Iteration 11, loss = 0.02134332\n",
      "Iteration 111, loss = 0.00664433\n",
      "Iteration 111, loss = 0.00915975\n",
      "Iteration 65, loss = 0.00948929\n",
      "Iteration 112, loss = 0.00822052\n",
      "Iteration 65, loss = 0.01157074\n",
      "Iteration 65, loss = 0.01106977\n",
      "Iteration 20, loss = 0.00739033\n",
      "Iteration 5, loss = 0.01407243\n",
      "Iteration 112, loss = 0.00652827\n",
      "Iteration 112, loss = 0.00901315\n",
      "Iteration 33, loss = 0.03942591\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 0.00338073\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 21, loss = 0.00505291\n",
      "[CV 3/3; 6/36] END alpha=0.0001, hidden_layer_sizes=(100,), learning_rate_init=0.1;, score=0.889 total time= 1.7min\n",
      "Iteration 113, loss = 0.00812077\n",
      "[CV 3/3; 5/36] END alpha=0.0001, hidden_layer_sizes=(100,), learning_rate_init=0.01;, score=0.908 total time= 2.2min\n",
      "[CV 2/3; 8/36] START alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate_init=0.01\n",
      "Iteration 12, loss = 0.01869425\n",
      "[CV 3/3; 8/36] START alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate_init=0.01\n",
      "Iteration 113, loss = 0.00643242\n",
      "Iteration 113, loss = 0.00888913\n",
      "Iteration 66, loss = 0.00924487\n",
      "Iteration 66, loss = 0.01158528\n",
      "Iteration 66, loss = 0.01084034\n",
      "Iteration 21, loss = 0.00668930\n",
      "Iteration 114, loss = 0.00798869\n",
      "Iteration 6, loss = 0.01217466\n",
      "Iteration 114, loss = 0.00634537\n",
      "Iteration 22, loss = 0.00476155\n",
      "Iteration 114, loss = 0.00874812\n",
      "Iteration 13, loss = 0.01643466\n",
      "Iteration 115, loss = 0.00785861\n",
      "Iteration 67, loss = 0.00908267\n",
      "Iteration 67, loss = 0.01115496\n",
      "Iteration 115, loss = 0.00625431\n",
      "Iteration 67, loss = 0.01054370\n",
      "Iteration 1, loss = 0.43301816\n",
      "Iteration 115, loss = 0.00861704\n",
      "Iteration 22, loss = 0.00601098\n",
      "Iteration 7, loss = 0.01979891\n",
      "Iteration 1, loss = 0.43054555\n",
      "Iteration 116, loss = 0.00781577\n",
      "Iteration 23, loss = 0.00434596\n",
      "Iteration 116, loss = 0.00618597\n",
      "Iteration 116, loss = 0.00859694\n",
      "Iteration 14, loss = 0.01405399\n",
      "Iteration 117, loss = 0.00765130\n",
      "Iteration 68, loss = 0.00885003\n",
      "Iteration 68, loss = 0.01103877\n",
      "Iteration 68, loss = 0.01038363\n",
      "Iteration 2, loss = 0.13651895\n",
      "Iteration 23, loss = 0.00562178\n",
      "Iteration 8, loss = 0.01741996\n",
      "Iteration 117, loss = 0.00610139\n",
      "Iteration 2, loss = 0.13261225\n",
      "Iteration 117, loss = 0.00833525\n",
      "Iteration 118, loss = 0.00763379\n",
      "Iteration 24, loss = 0.00415208\n",
      "Iteration 15, loss = 0.01256908\n",
      "Iteration 118, loss = 0.00605007\n",
      "Iteration 118, loss = 0.00831676\n",
      "Iteration 69, loss = 0.00862479\n",
      "Iteration 69, loss = 0.01069675\n",
      "Iteration 119, loss = 0.00756768\n",
      "Iteration 69, loss = 0.01014648\n",
      "Iteration 3, loss = 0.05685419\n",
      "Iteration 24, loss = 0.00531923\n",
      "Iteration 9, loss = 0.01266576\n",
      "Iteration 3, loss = 0.05837887\n",
      "Iteration 119, loss = 0.00595401\n",
      "Iteration 119, loss = 0.00819399\n",
      "Iteration 25, loss = 0.00390635\n",
      "Iteration 120, loss = 0.00738951\n",
      "Iteration 16, loss = 0.01123635\n",
      "Iteration 120, loss = 0.00591621\n",
      "Iteration 70, loss = 0.01056804\n",
      "Iteration 70, loss = 0.00849361\n",
      "Iteration 120, loss = 0.00818934\n",
      "Iteration 70, loss = 0.01002760\n",
      "Iteration 4, loss = 0.03578452\n",
      "Iteration 25, loss = 0.00502755\n",
      "Iteration 121, loss = 0.00734156\n",
      "Iteration 10, loss = 0.00665611\n",
      "Iteration 4, loss = 0.03248961\n",
      "Iteration 121, loss = 0.00583508\n",
      "Iteration 26, loss = 0.00377102\n",
      "Iteration 121, loss = 0.00814137\n",
      "Iteration 17, loss = 0.00942415\n",
      "Iteration 122, loss = 0.00722029\n",
      "Iteration 71, loss = 0.00826497\n",
      "Iteration 71, loss = 0.01023550\n",
      "Iteration 71, loss = 0.00981872\n",
      "Iteration 122, loss = 0.00572346\n",
      "Iteration 5, loss = 0.02185607\n",
      "Iteration 122, loss = 0.00803806\n",
      "Iteration 26, loss = 0.00464870\n",
      "Iteration 5, loss = 0.01929682\n",
      "Iteration 11, loss = 0.00542134\n",
      "Iteration 123, loss = 0.00717805\n",
      "Iteration 27, loss = 0.00365253\n",
      "Iteration 123, loss = 0.00567908\n",
      "Iteration 123, loss = 0.00783445\n",
      "Iteration 18, loss = 0.00857879\n",
      "Iteration 124, loss = 0.00709158\n",
      "Iteration 72, loss = 0.01011326\n",
      "Iteration 72, loss = 0.00809560\n",
      "Iteration 72, loss = 0.00953643\n",
      "Iteration 6, loss = 0.01337829\n",
      "Iteration 27, loss = 0.00434156\n",
      "Iteration 6, loss = 0.01172914\n",
      "Iteration 124, loss = 0.00561999\n",
      "Iteration 12, loss = 0.00617691\n",
      "Iteration 124, loss = 0.00785315\n",
      "Iteration 125, loss = 0.00701459\n",
      "Iteration 28, loss = 0.00350366\n",
      "Iteration 19, loss = 0.00774451\n",
      "Iteration 125, loss = 0.00555014\n",
      "Iteration 125, loss = 0.00770335\n",
      "Iteration 73, loss = 0.00988847\n",
      "Iteration 73, loss = 0.00795142\n",
      "Iteration 73, loss = 0.00941149\n",
      "Iteration 126, loss = 0.00691072\n",
      "Iteration 7, loss = 0.00808563\n",
      "Iteration 28, loss = 0.00412863\n",
      "Iteration 7, loss = 0.00728817\n",
      "Iteration 13, loss = 0.00648476\n",
      "Iteration 126, loss = 0.00549471\n",
      "Iteration 126, loss = 0.00764461\n",
      "Iteration 29, loss = 0.00336561\n",
      "Iteration 127, loss = 0.00686728\n",
      "Iteration 20, loss = 0.00691777\n",
      "Iteration 74, loss = 0.00976439\n",
      "Iteration 127, loss = 0.00544763\n",
      "Iteration 74, loss = 0.00778475\n",
      "Iteration 74, loss = 0.00918426\n",
      "Iteration 127, loss = 0.00750846\n",
      "Iteration 8, loss = 0.00529646\n",
      "Iteration 29, loss = 0.00396208\n",
      "Iteration 8, loss = 0.00471642\n",
      "Iteration 128, loss = 0.00677278\n",
      "Iteration 14, loss = 0.00439937\n",
      "Iteration 128, loss = 0.00534810\n",
      "Iteration 30, loss = 0.00336378\n",
      "Iteration 128, loss = 0.00739798\n",
      "Iteration 21, loss = 0.00631296\n",
      "Iteration 129, loss = 0.00668718\n",
      "Iteration 75, loss = 0.00948183\n",
      "Iteration 75, loss = 0.00761758\n",
      "Iteration 75, loss = 0.00910649\n",
      "Iteration 129, loss = 0.00530810\n",
      "Iteration 9, loss = 0.00502679\n",
      "Iteration 30, loss = 0.00384658\n",
      "Iteration 129, loss = 0.00731720\n",
      "Iteration 9, loss = 0.00383516\n",
      "Iteration 15, loss = 0.00372546\n",
      "Iteration 130, loss = 0.00657152\n",
      "Iteration 31, loss = 0.00310893\n",
      "Iteration 130, loss = 0.00525703\n",
      "Iteration 22, loss = 0.00586001\n",
      "Iteration 130, loss = 0.00715333\n",
      "Iteration 76, loss = 0.00929732\n",
      "Iteration 76, loss = 0.00747028\n",
      "Iteration 76, loss = 0.00888471\n",
      "Iteration 131, loss = 0.00652052\n",
      "Iteration 10, loss = 0.00778802\n",
      "Iteration 31, loss = 0.00375302\n",
      "Iteration 10, loss = 0.00349969\n",
      "Iteration 131, loss = 0.00521456\n",
      "Iteration 16, loss = 0.00580813\n",
      "Iteration 131, loss = 0.00708846\n",
      "Iteration 132, loss = 0.00648917\n",
      "Iteration 32, loss = 0.00306780\n",
      "Iteration 23, loss = 0.00553394\n",
      "Iteration 77, loss = 0.00737668\n",
      "Iteration 132, loss = 0.00513402\n",
      "Iteration 77, loss = 0.00917070\n",
      "Iteration 77, loss = 0.00868246\n",
      "Iteration 132, loss = 0.00700987\n",
      "Iteration 133, loss = 0.00642298\n",
      "Iteration 11, loss = 0.01932979\n",
      "Iteration 32, loss = 0.00365014\n",
      "Iteration 17, loss = 0.00770837\n",
      "Iteration 11, loss = 0.00338369\n",
      "Iteration 133, loss = 0.00508572\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 133, loss = 0.00697635\n",
      "Iteration 33, loss = 0.00297698\n",
      "[CV 1/3; 1/36] END alpha=0.0001, hidden_layer_sizes=(50,), learning_rate_init=0.001;, score=0.896 total time= 3.9min\n",
      "Iteration 134, loss = 0.00635592\n",
      "[CV 1/3; 9/36] START alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate_init=0.1\n",
      "Iteration 24, loss = 0.00506315\n",
      "Iteration 78, loss = 0.00722162\n",
      "Iteration 78, loss = 0.00903151\n",
      "Iteration 78, loss = 0.00852512\n",
      "Iteration 134, loss = 0.00703307\n",
      "Iteration 12, loss = 0.03236398\n",
      "Iteration 33, loss = 0.00354759\n",
      "Iteration 135, loss = 0.00626578\n",
      "Iteration 12, loss = 0.00343438\n",
      "Iteration 18, loss = 0.00928102\n",
      "Iteration 34, loss = 0.00294006\n",
      "Iteration 135, loss = 0.00680277\n",
      "Iteration 25, loss = 0.00480674\n",
      "Iteration 79, loss = 0.00705143\n",
      "Iteration 79, loss = 0.00894413\n",
      "Iteration 136, loss = 0.00622959\n",
      "Iteration 79, loss = 0.00845365\n",
      "Iteration 13, loss = 0.02557186\n",
      "Iteration 136, loss = 0.00675126\n",
      "Iteration 34, loss = 0.00352032\n",
      "Iteration 13, loss = 0.00332451\n",
      "Iteration 19, loss = 0.01066962\n",
      "Iteration 137, loss = 0.00618003\n",
      "Iteration 1, loss = 0.95065244\n",
      "Iteration 35, loss = 0.00288259\n",
      "Iteration 26, loss = 0.00452579\n",
      "Iteration 80, loss = 0.00692340\n",
      "Iteration 137, loss = 0.00676726\n",
      "Iteration 80, loss = 0.00863091\n",
      "Iteration 80, loss = 0.00823797\n",
      "Iteration 138, loss = 0.00608976\n",
      "Iteration 14, loss = 0.01489113\n",
      "Iteration 35, loss = 0.00334473\n",
      "Iteration 20, loss = 0.01039424\n",
      "Iteration 14, loss = 0.00349386\n",
      "Iteration 138, loss = 0.00666496\n",
      "Iteration 2, loss = 0.17385838\n",
      "Iteration 36, loss = 0.00282404\n",
      "Iteration 139, loss = 0.00601401\n",
      "Iteration 81, loss = 0.00679328\n",
      "Iteration 27, loss = 0.00435841\n",
      "Iteration 81, loss = 0.00857875\n",
      "Iteration 81, loss = 0.00814500\n",
      "Iteration 139, loss = 0.00659280\n",
      "Iteration 140, loss = 0.00596992\n",
      "Iteration 15, loss = 0.01511353\n",
      "Iteration 36, loss = 0.00332064\n",
      "Iteration 15, loss = 0.00334915\n",
      "Iteration 21, loss = 0.01289786\n",
      "Iteration 140, loss = 0.00643562\n",
      "Iteration 3, loss = 0.10602357\n",
      "Iteration 37, loss = 0.00284817\n",
      "Iteration 141, loss = 0.00590626\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 82, loss = 0.00672831\n",
      "Iteration 28, loss = 0.00413635\n",
      "Iteration 82, loss = 0.00833629\n",
      "Iteration 82, loss = 0.00797942\n",
      "[CV 3/3; 1/36] END alpha=0.0001, hidden_layer_sizes=(50,), learning_rate_init=0.001;, score=0.884 total time= 4.2min\n",
      "[CV 2/3; 9/36] START alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate_init=0.1\n",
      "Iteration 141, loss = 0.00634324\n",
      "Iteration 16, loss = 0.01188195\n",
      "Iteration 37, loss = 0.00322011\n",
      "Iteration 16, loss = 0.00329084\n",
      "Iteration 22, loss = 0.01622380\n",
      "Iteration 4, loss = 0.06640720\n",
      "Iteration 38, loss = 0.00278118\n",
      "Iteration 142, loss = 0.00630904\n",
      "Iteration 83, loss = 0.00662192\n",
      "Iteration 29, loss = 0.00397576\n",
      "Iteration 83, loss = 0.00784855\n",
      "Iteration 83, loss = 0.00828059\n",
      "Iteration 143, loss = 0.00626698\n",
      "Iteration 17, loss = 0.00503327\n",
      "Iteration 17, loss = 0.00319692\n",
      "Iteration 38, loss = 0.00315431\n",
      "Iteration 23, loss = 0.01151631\n",
      "Iteration 5, loss = 0.05536564\n",
      "Iteration 39, loss = 0.00273989\n",
      "Iteration 84, loss = 0.00643656\n",
      "Iteration 30, loss = 0.00384697\n",
      "Iteration 84, loss = 0.00767099\n",
      "Iteration 84, loss = 0.00822208\n",
      "Iteration 144, loss = 0.00620701\n",
      "Iteration 1, loss = 0.65867398\n",
      "Iteration 18, loss = 0.00370110\n",
      "Iteration 39, loss = 0.00315822\n",
      "Iteration 18, loss = 0.00315392\n",
      "Iteration 145, loss = 0.00629774\n",
      "Iteration 24, loss = 0.00488561\n",
      "Iteration 6, loss = 0.04249416\n",
      "Iteration 40, loss = 0.00265442\n",
      "Iteration 85, loss = 0.00636572\n",
      "Iteration 2, loss = 0.18705049\n",
      "Iteration 31, loss = 0.00373528\n",
      "Iteration 85, loss = 0.00798663\n",
      "Iteration 85, loss = 0.00761927\n",
      "Iteration 146, loss = 0.00606305\n",
      "Iteration 19, loss = 0.00348541\n",
      "Iteration 40, loss = 0.00308455\n",
      "Iteration 19, loss = 0.00301323\n",
      "Iteration 25, loss = 0.00338179\n",
      "Iteration 147, loss = 0.00604934\n",
      "Iteration 7, loss = 0.04018851\n",
      "Iteration 41, loss = 0.00269771\n",
      "Iteration 86, loss = 0.00623756\n",
      "Iteration 86, loss = 0.00789260\n",
      "Iteration 3, loss = 0.09720547\n",
      "Iteration 32, loss = 0.00369927\n",
      "Iteration 86, loss = 0.00751026\n",
      "Iteration 148, loss = 0.00595208\n",
      "Iteration 20, loss = 0.00339084\n",
      "Iteration 20, loss = 0.00308114\n",
      "Iteration 41, loss = 0.00308856\n",
      "Iteration 26, loss = 0.00334648\n",
      "Iteration 8, loss = 0.05483457\n",
      "Iteration 42, loss = 0.00270515\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 149, loss = 0.00590632\n",
      "Iteration 87, loss = 0.00612541\n",
      "[CV 1/3; 7/36] END alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate_init=0.001;, score=0.899 total time= 2.2min\n",
      "Iteration 87, loss = 0.00733221\n",
      "Iteration 87, loss = 0.00772631\n",
      "Iteration 33, loss = 0.00361222\n",
      "Iteration 4, loss = 0.05928655\n",
      "[CV 3/3; 9/36] START alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate_init=0.1\n",
      "Iteration 150, loss = 0.00578586\n",
      "Iteration 21, loss = 0.00336286\n",
      "Iteration 42, loss = 0.00302548\n",
      "Iteration 27, loss = 0.00350679\n",
      "Iteration 21, loss = 0.00315023\n",
      "Iteration 9, loss = 0.05887065\n",
      "Iteration 88, loss = 0.00603670\n",
      "Iteration 151, loss = 0.00579556\n",
      "Iteration 88, loss = 0.00720845\n",
      "Iteration 88, loss = 0.00761661\n",
      "Iteration 5, loss = 0.03932900\n",
      "Iteration 34, loss = 0.00343868\n",
      "Iteration 22, loss = 0.00330860\n",
      "Iteration 152, loss = 0.00573363\n",
      "Iteration 43, loss = 0.00301779\n",
      "Iteration 28, loss = 0.00333199\n",
      "Iteration 22, loss = 0.00316285\n",
      "Iteration 1, loss = 0.73230441\n",
      "Iteration 10, loss = 0.04852836\n",
      "Iteration 89, loss = 0.00596157\n",
      "Iteration 89, loss = 0.00710236\n",
      "Iteration 89, loss = 0.00746628\n",
      "Iteration 35, loss = 0.00340365\n",
      "Iteration 6, loss = 0.06164313\n",
      "Iteration 153, loss = 0.00564751\n",
      "Iteration 23, loss = 0.00328229\n",
      "Iteration 23, loss = 0.00302462\n",
      "Iteration 44, loss = 0.00298793\n",
      "Iteration 29, loss = 0.00328993\n",
      "Iteration 154, loss = 0.00558248\n",
      "Iteration 2, loss = 0.19149096\n",
      "Iteration 11, loss = 0.05871733\n",
      "Iteration 90, loss = 0.00584105\n",
      "Iteration 90, loss = 0.00738003\n",
      "Iteration 90, loss = 0.00698874\n",
      "Iteration 36, loss = 0.00332927\n",
      "Iteration 7, loss = 0.04532999\n",
      "Iteration 155, loss = 0.00554329\n",
      "Iteration 24, loss = 0.00336002\n",
      "Iteration 24, loss = 0.00301569\n",
      "Iteration 45, loss = 0.00304664\n",
      "Iteration 30, loss = 0.00327693\n",
      "Iteration 3, loss = 0.09615996\n",
      "Iteration 12, loss = 0.04823699\n",
      "Iteration 156, loss = 0.00546736\n",
      "Iteration 91, loss = 0.00724535\n",
      "Iteration 91, loss = 0.00686751\n",
      "Iteration 91, loss = 0.00575540\n",
      "Iteration 8, loss = 0.04111061\n",
      "Iteration 37, loss = 0.00329358\n",
      "Iteration 157, loss = 0.00546304\n",
      "Iteration 25, loss = 0.00335476\n",
      "Iteration 25, loss = 0.00313332\n",
      "Iteration 46, loss = 0.00291741\n",
      "Iteration 31, loss = 0.00322468\n",
      "Iteration 4, loss = 0.05455439\n",
      "Iteration 13, loss = 0.04700368\n",
      "Iteration 92, loss = 0.00714102\n",
      "Iteration 158, loss = 0.00538470\n",
      "Iteration 92, loss = 0.00571150\n",
      "Iteration 92, loss = 0.00678740\n",
      "Iteration 9, loss = 0.04829746\n",
      "Iteration 38, loss = 0.00328688\n",
      "Iteration 159, loss = 0.00537273\n",
      "Iteration 26, loss = 0.00327104\n",
      "Iteration 26, loss = 0.00301480\n",
      "Iteration 47, loss = 0.00287481\n",
      "Iteration 32, loss = 0.00327801\n",
      "Iteration 5, loss = 0.04181727\n",
      "Iteration 14, loss = 0.04441791\n",
      "Iteration 93, loss = 0.00709148\n",
      "Iteration 93, loss = 0.00561068\n",
      "Iteration 93, loss = 0.00669685\n",
      "Iteration 160, loss = 0.00527233\n",
      "Iteration 10, loss = 0.04277002\n",
      "Iteration 39, loss = 0.00320169\n",
      "Iteration 27, loss = 0.00316984\n",
      "Iteration 27, loss = 0.00297078\n",
      "Iteration 48, loss = 0.00285231\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 161, loss = 0.00529390\n",
      "Iteration 33, loss = 0.00322110\n",
      "[CV 2/3; 7/36] END alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate_init=0.001;, score=0.895 total time= 2.5min\n",
      "Iteration 6, loss = 0.03970120\n",
      "Iteration 15, loss = 0.04031113\n",
      "[CV 1/3; 10/36] START alpha=0.0001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.001\n",
      "Iteration 94, loss = 0.00701502\n",
      "Iteration 94, loss = 0.00554125\n",
      "Iteration 94, loss = 0.00658137\n",
      "Iteration 11, loss = 0.02725855\n",
      "Iteration 40, loss = 0.00320536\n",
      "Iteration 162, loss = 0.00522926\n",
      "Iteration 28, loss = 0.00309916\n",
      "Iteration 28, loss = 0.00326112\n",
      "Iteration 34, loss = 0.00325820\n",
      "Iteration 163, loss = 0.00524342\n",
      "Iteration 7, loss = 0.03473375\n",
      "Iteration 16, loss = 0.05713163\n",
      "Iteration 95, loss = 0.00547193\n",
      "Iteration 95, loss = 0.00683784\n",
      "Iteration 95, loss = 0.00650640\n",
      "Iteration 12, loss = 0.01558277\n",
      "Iteration 41, loss = 0.00335215\n",
      "Iteration 164, loss = 0.00512381\n",
      "Iteration 29, loss = 0.00299554\n",
      "Iteration 29, loss = 0.00319094\n",
      "Iteration 35, loss = 0.00321033\n",
      "Iteration 8, loss = 0.03342386\n",
      "Iteration 17, loss = 0.05414189\n",
      "Iteration 165, loss = 0.00508579\n",
      "Iteration 96, loss = 0.00543416\n",
      "Iteration 96, loss = 0.00683865\n",
      "Iteration 96, loss = 0.00641886\n",
      "Iteration 1, loss = 0.61936872\n",
      "Iteration 13, loss = 0.01499012\n",
      "Iteration 42, loss = 0.00311749\n",
      "Iteration 166, loss = 0.00502799\n",
      "Iteration 30, loss = 0.00322262\n",
      "Iteration 30, loss = 0.00293694\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/3; 8/36] END alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate_init=0.01;, score=0.906 total time= 1.6min\n",
      "Iteration 36, loss = 0.00323504\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 0.05021657\n",
      "Iteration 18, loss = 0.05858493\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/3; 10/36] START alpha=0.0001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.001\n",
      "[CV 1/3; 8/36] END alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate_init=0.01;, score=0.897 total time= 1.9min\n",
      "Iteration 97, loss = 0.00666584\n",
      "Iteration 97, loss = 0.00534307\n",
      "Iteration 97, loss = 0.00632283\n",
      "[CV 1/3; 9/36] END alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate_init=0.1;, score=0.886 total time=  59.0s\n",
      "[CV 3/3; 10/36] START alpha=0.0001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.001\n",
      "Iteration 167, loss = 0.00506006\n",
      "Iteration 14, loss = 0.01402374\n",
      "[CV 1/3; 11/36] START alpha=0.0001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.01\n",
      "Iteration 43, loss = 0.00309319\n",
      "Iteration 2, loss = 0.30240906\n",
      "Iteration 31, loss = 0.00317185\n",
      "Iteration 168, loss = 0.00501153\n",
      "Iteration 10, loss = 0.04464610\n",
      "Iteration 98, loss = 0.00658866\n",
      "Iteration 98, loss = 0.00523695\n",
      "Iteration 98, loss = 0.00624024\n",
      "Iteration 15, loss = 0.01570630\n",
      "Iteration 44, loss = 0.00322372\n",
      "Iteration 169, loss = 0.00493109\n",
      "Iteration 32, loss = 0.00315796\n",
      "Iteration 170, loss = 0.00494363\n",
      "Iteration 11, loss = 0.04221745\n",
      "Iteration 3, loss = 0.13151924\n",
      "Iteration 99, loss = 0.00517283\n",
      "Iteration 99, loss = 0.00650055\n",
      "Iteration 99, loss = 0.00617348\n",
      "Iteration 1, loss = 0.62351913\n",
      "Iteration 16, loss = 0.01478418\n",
      "Iteration 45, loss = 0.00345162\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.42564346\n",
      "Iteration 1, loss = 0.62392205\n",
      "[CV 3/3; 7/36] END alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate_init=0.001;, score=0.895 total time= 2.3min\n",
      "Iteration 171, loss = 0.00485081\n",
      "[CV 2/3; 11/36] START alpha=0.0001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.01\n",
      "Iteration 33, loss = 0.00314586\n",
      "Iteration 12, loss = 0.04107277\n",
      "Iteration 100, loss = 0.00511963\n",
      "Iteration 100, loss = 0.00643202\n",
      "Iteration 172, loss = 0.00479467\n",
      "Iteration 100, loss = 0.00609614\n",
      "Iteration 17, loss = 0.02315949\n",
      "Iteration 4, loss = 0.06233904\n",
      "Iteration 2, loss = 0.31810225\n",
      "Iteration 173, loss = 0.00483821\n",
      "Iteration 34, loss = 0.00319825\n",
      "Iteration 2, loss = 0.11567254\n",
      "Iteration 2, loss = 0.31894327\n",
      "Iteration 13, loss = 0.03390340\n",
      "Iteration 101, loss = 0.00633183\n",
      "Iteration 101, loss = 0.00505914\n",
      "Iteration 101, loss = 0.00597872\n",
      "Iteration 18, loss = 0.04178537\n",
      "Iteration 174, loss = 0.00476439\n",
      "Iteration 35, loss = 0.00314755\n",
      "Iteration 175, loss = 0.00471833Iteration 1, loss = 0.40710047\n",
      "\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 0.03429281\n",
      "Iteration 14, loss = 0.02545366\n",
      "Iteration 3, loss = 0.13580932\n",
      "[CV 2/3; 1/36] END alpha=0.0001, hidden_layer_sizes=(50,), learning_rate_init=0.001;, score=0.888 total time= 5.2min\n",
      "Iteration 102, loss = 0.00622467\n",
      "Iteration 102, loss = 0.00499545\n",
      "Iteration 102, loss = 0.00591106\n",
      "[CV 3/3; 11/36] START alpha=0.0001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.01\n",
      "Iteration 3, loss = 0.04884449\n",
      "Iteration 19, loss = 0.03558375\n",
      "Iteration 3, loss = 0.14026320\n",
      "Iteration 36, loss = 0.00319270\n",
      "Iteration 15, loss = 0.01990480\n",
      "Iteration 103, loss = 0.00615928\n",
      "Iteration 103, loss = 0.00496617\n",
      "Iteration 103, loss = 0.00586212\n",
      "Iteration 6, loss = 0.02229955\n",
      "Iteration 2, loss = 0.11893675\n",
      "Iteration 20, loss = 0.02782424\n",
      "Iteration 4, loss = 0.07082583\n",
      "Iteration 4, loss = 0.02311276\n",
      "Iteration 4, loss = 0.06499022\n",
      "Iteration 37, loss = 0.00315551\n",
      "Iteration 16, loss = 0.02567021\n",
      "Iteration 104, loss = 0.00490279\n",
      "Iteration 104, loss = 0.00617206\n",
      "Iteration 104, loss = 0.00579879\n",
      "Iteration 21, loss = 0.03160645\n",
      "Iteration 1, loss = 0.41414420\n",
      "Iteration 7, loss = 0.01339229\n",
      "Iteration 3, loss = 0.04913529\n",
      "Iteration 38, loss = 0.00309777\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 0.04098966\n",
      "[CV 2/3; 8/36] END alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate_init=0.01;, score=0.904 total time= 2.0min\n",
      "Iteration 105, loss = 0.00485412\n",
      "Iteration 105, loss = 0.00601526\n",
      "Iteration 17, loss = 0.03587230\n",
      "[CV 1/3; 12/36] START alpha=0.0001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.1\n",
      "Iteration 105, loss = 0.00572563\n",
      "Iteration 5, loss = 0.01849167\n",
      "Iteration 5, loss = 0.03554488\n",
      "Iteration 22, loss = 0.02556466\n",
      "Iteration 2, loss = 0.12845274\n",
      "Iteration 106, loss = 0.00478083\n",
      "Iteration 106, loss = 0.00590791\n",
      "Iteration 18, loss = 0.04252973\n",
      "Iteration 106, loss = 0.00564072\n",
      "Iteration 8, loss = 0.00825155\n",
      "Iteration 4, loss = 0.02668514\n",
      "Iteration 23, loss = 0.03506273\n",
      "Iteration 6, loss = 0.02957018\n",
      "Iteration 6, loss = 0.01099890\n",
      "Iteration 6, loss = 0.02157511\n",
      "Iteration 107, loss = 0.00557659\n",
      "Iteration 107, loss = 0.00473398\n",
      "Iteration 107, loss = 0.00587227\n",
      "Iteration 19, loss = 0.02717267\n",
      "Iteration 24, loss = 0.03415303\n",
      "Iteration 3, loss = 0.04612105\n",
      "Iteration 1, loss = 1.26283719\n",
      "Iteration 9, loss = 0.00596827\n",
      "Iteration 5, loss = 0.02450908\n",
      "Iteration 7, loss = 0.01968349\n",
      "Iteration 108, loss = 0.00468745\n",
      "Iteration 7, loss = 0.01079453\n",
      "Iteration 108, loss = 0.00556011\n",
      "Iteration 108, loss = 0.00585880\n",
      "Iteration 20, loss = 0.03841535\n",
      "Iteration 7, loss = 0.01440368\n",
      "Iteration 25, loss = 0.02373431\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/3; 9/36] END alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate_init=0.1;, score=0.890 total time= 1.3min\n",
      "[CV 2/3; 12/36] START alpha=0.0001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.1\n",
      "Iteration 4, loss = 0.02675961\n",
      "Iteration 2, loss = 0.23061715\n",
      "Iteration 109, loss = 0.00463397\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 0.00461779\n",
      "Iteration 6, loss = 0.01569993\n",
      "Iteration 109, loss = 0.00574392\n",
      "Iteration 109, loss = 0.00544926\n",
      "Iteration 21, loss = 0.03215007\n",
      "Iteration 8, loss = 0.01546567\n",
      "[CV 1/3; 4/36] END alpha=0.0001, hidden_layer_sizes=(100,), learning_rate_init=0.001;, score=0.897 total time= 5.5min\n",
      "[CV 3/3; 12/36] START alpha=0.0001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.1\n",
      "Iteration 8, loss = 0.00848232\n",
      "Iteration 8, loss = 0.00998360\n",
      "Iteration 110, loss = 0.00537687\n",
      "Iteration 110, loss = 0.00565123\n",
      "Iteration 22, loss = 0.06182145\n",
      "Iteration 3, loss = 0.11049576\n",
      "Iteration 5, loss = 0.01582034\n",
      "Iteration 7, loss = 0.00710231\n",
      "Iteration 11, loss = 0.00388416\n",
      "Iteration 9, loss = 0.01154228\n",
      "Iteration 1, loss = 1.10739883\n",
      "Iteration 111, loss = 0.00529898\n",
      "Iteration 9, loss = 0.00479142\n",
      "Iteration 111, loss = 0.00564656\n",
      "Iteration 9, loss = 0.00733378\n",
      "Iteration 23, loss = 0.04245390\n",
      "Iteration 1, loss = 1.06224902\n",
      "Iteration 4, loss = 0.07196571\n",
      "Iteration 6, loss = 0.02694874\n",
      "Iteration 112, loss = 0.00528896\n",
      "Iteration 12, loss = 0.00353891\n",
      "Iteration 8, loss = 0.00794400\n",
      "Iteration 112, loss = 0.00550746\n",
      "Iteration 24, loss = 0.03512996\n",
      "Iteration 10, loss = 0.00830139\n",
      "Iteration 2, loss = 0.18874675\n",
      "Iteration 10, loss = 0.02670751\n",
      "Iteration 10, loss = 0.00605746\n",
      "Iteration 113, loss = 0.00521802\n",
      "Iteration 113, loss = 0.00544319\n",
      "Iteration 2, loss = 0.22589580\n",
      "Iteration 5, loss = 0.05306179\n",
      "Iteration 25, loss = 0.04000360\n",
      "Iteration 7, loss = 0.01895193\n",
      "Iteration 9, loss = 0.00380247\n",
      "Iteration 13, loss = 0.00320498\n",
      "Iteration 11, loss = 0.00630005\n",
      "Iteration 3, loss = 0.09667689\n",
      "Iteration 114, loss = 0.00515901\n",
      "Iteration 11, loss = 0.03196525\n",
      "Iteration 11, loss = 0.00502685\n",
      "Iteration 114, loss = 0.00542910\n",
      "Iteration 26, loss = 0.03012610\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/3; 9/36] END alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate_init=0.1;, score=0.900 total time= 1.4min\n",
      "[CV 1/3; 13/36] START alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.001\n",
      "Iteration 3, loss = 0.13474603\n",
      "Iteration 6, loss = 0.05122203\n",
      "Iteration 8, loss = 0.01028424\n",
      "Iteration 115, loss = 0.00512268\n",
      "Iteration 14, loss = 0.00296597\n",
      "Iteration 10, loss = 0.00336114\n",
      "Iteration 115, loss = 0.00536385\n",
      "Iteration 12, loss = 0.00508984\n",
      "Iteration 4, loss = 0.08480150\n",
      "Iteration 12, loss = 0.00994255\n",
      "Iteration 12, loss = 0.00457002\n",
      "Iteration 1, loss = 0.65233127\n",
      "Iteration 116, loss = 0.00504968\n",
      "Iteration 116, loss = 0.00530433\n",
      "Iteration 7, loss = 0.06001613\n",
      "Iteration 4, loss = 0.09599074\n",
      "Iteration 9, loss = 0.00897315\n",
      "Iteration 2, loss = 0.52870378\n",
      "Iteration 15, loss = 0.00291461\n",
      "Iteration 11, loss = 0.00313690\n",
      "Iteration 13, loss = 0.00439800\n",
      "Iteration 3, loss = 0.41758638\n",
      "Iteration 5, loss = 0.06855852\n",
      "Iteration 117, loss = 0.00498551\n",
      "Iteration 13, loss = 0.00469978\n",
      "Iteration 13, loss = 0.00386788\n",
      "Iteration 117, loss = 0.00522300\n",
      "Iteration 4, loss = 0.33750552\n",
      "Iteration 8, loss = 0.05116955\n",
      "Iteration 5, loss = 0.09490335\n",
      "Iteration 10, loss = 0.01201017\n",
      "Iteration 118, loss = 0.00493868\n",
      "Iteration 16, loss = 0.00283091\n",
      "Iteration 12, loss = 0.00315212\n",
      "Iteration 5, loss = 0.27999664\n",
      "Iteration 118, loss = 0.00518235\n",
      "Iteration 14, loss = 0.00401128\n",
      "Iteration 6, loss = 0.07059956\n",
      "Iteration 14, loss = 0.00498239\n",
      "Iteration 14, loss = 0.00372316\n",
      "Iteration 6, loss = 0.23673545\n",
      "Iteration 119, loss = 0.00494828\n",
      "Iteration 119, loss = 0.00514959\n",
      "Iteration 9, loss = 0.04829932\n",
      "Iteration 6, loss = 0.09209789\n",
      "Iteration 11, loss = 0.00712393\n",
      "Iteration 7, loss = 0.20368681\n",
      "Iteration 13, loss = 0.00299493\n",
      "Iteration 17, loss = 0.00272709\n",
      "Iteration 15, loss = 0.00367858\n",
      "Iteration 7, loss = 0.06779303\n",
      "Iteration 8, loss = 0.17753679\n",
      "Iteration 120, loss = 0.00492123\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 0.00701194\n",
      "Iteration 15, loss = 0.00349675\n",
      "Iteration 120, loss = 0.00511689\n",
      "[CV 3/3; 4/36] END alpha=0.0001, hidden_layer_sizes=(100,), learning_rate_init=0.001;, score=0.889 total time= 6.1min\n",
      "[CV 2/3; 13/36] START alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.001\n",
      "Iteration 9, loss = 0.15657713\n",
      "Iteration 7, loss = 0.09215050\n",
      "Iteration 10, loss = 0.04203318\n",
      "Iteration 12, loss = 0.00473093\n",
      "Iteration 18, loss = 0.00281477\n",
      "Iteration 14, loss = 0.00302928\n",
      "Iteration 121, loss = 0.00506955\n",
      "Iteration 10, loss = 0.14050566\n",
      "Iteration 16, loss = 0.00340596\n",
      "Iteration 8, loss = 0.05564524\n",
      "Iteration 16, loss = 0.00510375\n",
      "Iteration 16, loss = 0.00363617\n",
      "Iteration 1, loss = 0.65232198\n",
      "Iteration 11, loss = 0.12600558\n",
      "Iteration 122, loss = 0.00501901\n",
      "Iteration 8, loss = 0.06961942\n",
      "Iteration 11, loss = 0.02791808\n",
      "Iteration 13, loss = 0.00692395\n",
      "Iteration 2, loss = 0.52926613\n",
      "Iteration 12, loss = 0.11428208\n",
      "Iteration 15, loss = 0.00299588\n",
      "Iteration 19, loss = 0.00284597\n",
      "Iteration 17, loss = 0.00340683\n",
      "Iteration 9, loss = 0.04084793\n",
      "Iteration 3, loss = 0.41846089\n",
      "Iteration 17, loss = 0.00383749\n",
      "Iteration 13, loss = 0.10406616\n",
      "Iteration 17, loss = 0.00340499\n",
      "Iteration 123, loss = 0.00496560\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/3; 4/36] END alpha=0.0001, hidden_layer_sizes=(100,), learning_rate_init=0.001;, score=0.897 total time= 6.3min\n",
      "[CV 3/3; 13/36] START alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.001\n",
      "Iteration 4, loss = 0.33900713\n",
      "Iteration 12, loss = 0.03094714\n",
      "Iteration 14, loss = 0.09625043\n",
      "Iteration 9, loss = 0.06555730\n",
      "Iteration 14, loss = 0.00710948\n",
      "Iteration 16, loss = 0.00298269\n",
      "Iteration 20, loss = 0.00278114\n",
      "Iteration 5, loss = 0.28219128\n",
      "Iteration 15, loss = 0.08862694\n",
      "Iteration 18, loss = 0.00325155\n",
      "Iteration 10, loss = 0.05031421\n",
      "Iteration 18, loss = 0.00523588\n",
      "Iteration 18, loss = 0.00310231\n",
      "Iteration 1, loss = 0.65269286\n",
      "Iteration 6, loss = 0.23879045\n",
      "Iteration 16, loss = 0.08224655\n",
      "Iteration 10, loss = 0.06162097\n",
      "Iteration 13, loss = 0.03802244\n",
      "Iteration 15, loss = 0.00491943\n",
      "Iteration 2, loss = 0.53034723\n",
      "Iteration 7, loss = 0.20608135\n",
      "Iteration 17, loss = 0.07711806\n",
      "Iteration 21, loss = 0.00269196\n",
      "Iteration 17, loss = 0.00294920\n",
      "Iteration 19, loss = 0.00313091\n",
      "Iteration 11, loss = 0.04010457\n",
      "Iteration 3, loss = 0.42043148\n",
      "Iteration 8, loss = 0.17979317\n",
      "Iteration 19, loss = 0.00476875\n",
      "Iteration 19, loss = 0.00302993\n",
      "Iteration 18, loss = 0.07251669\n",
      "Iteration 9, loss = 0.15877701\n",
      "Iteration 14, loss = 0.06199818\n",
      "Iteration 11, loss = 0.07328528\n",
      "Iteration 4, loss = 0.34064380\n",
      "Iteration 19, loss = 0.06801327\n",
      "Iteration 16, loss = 0.00416537\n",
      "Iteration 18, loss = 0.00302163\n",
      "Iteration 22, loss = 0.00259536\n",
      "Iteration 10, loss = 0.14177404\n",
      "Iteration 20, loss = 0.00296349\n",
      "Iteration 5, loss = 0.28387939\n",
      "Iteration 20, loss = 0.06394831\n",
      "Iteration 12, loss = 0.03902767\n",
      "Iteration 20, loss = 0.00602573\n",
      "Iteration 20, loss = 0.00311351\n",
      "Iteration 11, loss = 0.12760729\n",
      "Iteration 6, loss = 0.24065598\n",
      "Iteration 21, loss = 0.06073458\n",
      "Iteration 15, loss = 0.05318793\n",
      "Iteration 12, loss = 0.08963791\n",
      "Iteration 17, loss = 0.00427563\n",
      "Iteration 19, loss = 0.00305602\n",
      "Iteration 12, loss = 0.11629571\n",
      "Iteration 23, loss = 0.00256306\n",
      "Iteration 7, loss = 0.20754360\n",
      "Iteration 22, loss = 0.05745548\n",
      "Iteration 21, loss = 0.00287478\n",
      "Iteration 13, loss = 0.03454805\n",
      "Iteration 21, loss = 0.00827641\n",
      "Iteration 21, loss = 0.00306543\n",
      "Iteration 13, loss = 0.10602725\n",
      "Iteration 8, loss = 0.18193619\n",
      "Iteration 23, loss = 0.05492771\n",
      "Iteration 16, loss = 0.03317454\n",
      "Iteration 13, loss = 0.06760873\n",
      "Iteration 14, loss = 0.09804040\n",
      "Iteration 18, loss = 0.00402292\n",
      "Iteration 9, loss = 0.16045134\n",
      "Iteration 24, loss = 0.05238993\n",
      "Iteration 20, loss = 0.00305216\n",
      "Iteration 24, loss = 0.00290291\n",
      "Iteration 22, loss = 0.00285588\n",
      "Iteration 15, loss = 0.09096305\n",
      "Iteration 10, loss = 0.14333050\n",
      "Iteration 25, loss = 0.05008140\n",
      "Iteration 14, loss = 0.03337555\n",
      "Iteration 22, loss = 0.00808646\n",
      "Iteration 22, loss = 0.00316035\n",
      "Iteration 16, loss = 0.08547383\n",
      "Iteration 26, loss = 0.04809267\n",
      "Iteration 11, loss = 0.12874593\n",
      "Iteration 14, loss = 0.05733474\n",
      "Iteration 17, loss = 0.03108902\n",
      "Iteration 19, loss = 0.00335781\n",
      "Iteration 21, loss = 0.00289204\n",
      "Iteration 25, loss = 0.00264121\n",
      "Iteration 17, loss = 0.07946012\n",
      "Iteration 27, loss = 0.04612972\n",
      "Iteration 12, loss = 0.11675459\n",
      "Iteration 23, loss = 0.00288742\n",
      "Iteration 15, loss = 0.04595892\n",
      "Iteration 23, loss = 0.00563395\n",
      "Iteration 23, loss = 0.00288934\n",
      "Iteration 18, loss = 0.07444095\n",
      "Iteration 28, loss = 0.04436169\n",
      "Iteration 13, loss = 0.10660540\n",
      "Iteration 18, loss = 0.02939411\n",
      "Iteration 15, loss = 0.04495098\n",
      "Iteration 20, loss = 0.00334865\n",
      "Iteration 19, loss = 0.07014658\n",
      "Iteration 29, loss = 0.04288854\n",
      "Iteration 14, loss = 0.09827180\n",
      "Iteration 22, loss = 0.00295853\n",
      "Iteration 26, loss = 0.00296792\n",
      "Iteration 24, loss = 0.00290218\n",
      "Iteration 20, loss = 0.06641592\n",
      "Iteration 16, loss = 0.04347942\n",
      "Iteration 30, loss = 0.04131842\n",
      "Iteration 15, loss = 0.09070652\n",
      "Iteration 24, loss = 0.00657215\n",
      "Iteration 24, loss = 0.00288279\n",
      "Iteration 21, loss = 0.06315810\n",
      "Iteration 31, loss = 0.04008936\n",
      "Iteration 16, loss = 0.08426598\n",
      "Iteration 19, loss = 0.03065896\n",
      "Iteration 16, loss = 0.04606754\n",
      "Iteration 21, loss = 0.00333545\n",
      "Iteration 23, loss = 0.00295971\n",
      "Iteration 27, loss = 0.00277226\n",
      "Iteration 22, loss = 0.06025839\n",
      "Iteration 32, loss = 0.03874098\n",
      "Iteration 17, loss = 0.07844575\n",
      "Iteration 25, loss = 0.00273085\n",
      "Iteration 17, loss = 0.03514932\n",
      "Iteration 25, loss = 0.00537775\n",
      "Iteration 25, loss = 0.00318313\n",
      "Iteration 23, loss = 0.05755605\n",
      "Iteration 33, loss = 0.03757366\n",
      "Iteration 18, loss = 0.07362472\n",
      "Iteration 20, loss = 0.02264000\n",
      "Iteration 17, loss = 0.04603060\n",
      "Iteration 22, loss = 0.00338736\n",
      "Iteration 24, loss = 0.05521426\n",
      "Iteration 24, loss = 0.00299599\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 34, loss = 0.03624475\n",
      "Iteration 19, loss = 0.06918770\n",
      "Iteration 28, loss = 0.00293437\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/3; 11/36] END alpha=0.0001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.01;, score=0.900 total time= 1.9min\n",
      "[CV 1/3; 14/36] START alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.01\n",
      "[CV 1/3; 10/36] END alpha=0.0001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.001;, score=0.903 total time= 2.1min\n",
      "Iteration 26, loss = 0.00265840\n",
      "[CV 2/3; 14/36] START alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.01\n",
      "Iteration 18, loss = 0.02945065\n",
      "Iteration 25, loss = 0.05290210\n",
      "Iteration 26, loss = 0.00433731\n",
      "Iteration 35, loss = 0.03521517\n",
      "Iteration 26, loss = 0.00311193\n",
      "Iteration 20, loss = 0.06529997\n",
      "Iteration 21, loss = 0.02962428\n",
      "Iteration 26, loss = 0.05090763\n",
      "Iteration 36, loss = 0.03439869\n",
      "Iteration 18, loss = 0.04615188\n",
      "Iteration 23, loss = 0.00322485\n",
      "Iteration 21, loss = 0.06161396\n",
      "Iteration 1, loss = 0.44668315\n",
      "Iteration 1, loss = 0.44407496\n",
      "Iteration 27, loss = 0.04913326\n",
      "Iteration 37, loss = 0.03344596\n",
      "Iteration 27, loss = 0.00265441\n",
      "Iteration 22, loss = 0.05865087\n",
      "Iteration 19, loss = 0.02695101\n",
      "Iteration 27, loss = 0.00353450\n",
      "Iteration 2, loss = 0.16255960\n",
      "Iteration 27, loss = 0.00300229\n",
      "Iteration 2, loss = 0.16221287\n",
      "Iteration 28, loss = 0.04762868\n",
      "Iteration 38, loss = 0.03267167\n",
      "Iteration 23, loss = 0.05603885\n",
      "Iteration 22, loss = 0.11356672\n",
      "Iteration 19, loss = 0.06606498\n",
      "Iteration 24, loss = 0.00321931\n",
      "Iteration 3, loss = 0.08239130\n",
      "Iteration 3, loss = 0.08675400\n",
      "Iteration 29, loss = 0.04605941\n",
      "Iteration 39, loss = 0.03180098\n",
      "Iteration 24, loss = 0.05378764\n",
      "Iteration 28, loss = 0.00278051\n",
      "Iteration 4, loss = 0.05241351\n",
      "Iteration 20, loss = 0.03163792\n",
      "Iteration 4, loss = 0.05952806\n",
      "Iteration 28, loss = 0.00360399\n",
      "Iteration 28, loss = 0.00308450\n",
      "Iteration 30, loss = 0.04446502\n",
      "Iteration 40, loss = 0.03085387\n",
      "Iteration 25, loss = 0.05103805\n",
      "Iteration 23, loss = 0.14688377\n",
      "Iteration 5, loss = 0.03981861\n",
      "Iteration 20, loss = 0.08984071\n",
      "Iteration 5, loss = 0.04503283\n",
      "Iteration 25, loss = 0.00323987\n",
      "Iteration 31, loss = 0.04314550\n",
      "Iteration 41, loss = 0.03005964\n",
      "Iteration 26, loss = 0.04897142\n",
      "Iteration 6, loss = 0.03148095\n",
      "Iteration 29, loss = 0.00267386\n",
      "Iteration 32, loss = 0.04211665\n",
      "Iteration 6, loss = 0.03615315\n",
      "Iteration 21, loss = 0.03516052\n",
      "Iteration 42, loss = 0.02948417\n",
      "Iteration 27, loss = 0.04714683\n",
      "Iteration 29, loss = 0.00324653\n",
      "Iteration 29, loss = 0.00334096\n",
      "Iteration 7, loss = 0.02582749\n",
      "Iteration 33, loss = 0.04090093\n",
      "Iteration 24, loss = 0.11476494\n",
      "Iteration 7, loss = 0.03190046\n",
      "Iteration 43, loss = 0.02864821\n",
      "Iteration 28, loss = 0.04520615\n",
      "Iteration 21, loss = 0.08431608\n",
      "Iteration 26, loss = 0.00340569\n",
      "Iteration 8, loss = 0.02160383\n",
      "Iteration 34, loss = 0.03958323\n",
      "Iteration 44, loss = 0.02789800\n",
      "Iteration 8, loss = 0.02548862\n",
      "Iteration 29, loss = 0.04363300\n",
      "Iteration 30, loss = 0.00256644\n",
      "Iteration 22, loss = 0.04614655\n",
      "Iteration 30, loss = 0.00331589\n",
      "Iteration 30, loss = 0.00299351\n",
      "Iteration 35, loss = 0.03904813\n",
      "Iteration 9, loss = 0.01862218\n",
      "Iteration 45, loss = 0.02754732\n",
      "Iteration 30, loss = 0.04213738\n",
      "Iteration 9, loss = 0.02280493\n",
      "Iteration 25, loss = 0.08003221\n",
      "Iteration 22, loss = 0.09077718\n",
      "Iteration 27, loss = 0.00320577\n",
      "Iteration 36, loss = 0.03770562\n",
      "Iteration 46, loss = 0.02676114\n",
      "Iteration 10, loss = 0.01690947\n",
      "Iteration 31, loss = 0.04103434\n",
      "Iteration 10, loss = 0.01947347\n",
      "Iteration 31, loss = 0.00255434\n",
      "Iteration 23, loss = 0.03139839\n",
      "Iteration 37, loss = 0.03664537\n",
      "Iteration 47, loss = 0.02622297\n",
      "Iteration 31, loss = 0.00334623\n",
      "Iteration 31, loss = 0.00323430\n",
      "Iteration 11, loss = 0.01496957\n",
      "Iteration 32, loss = 0.03958512\n",
      "Iteration 11, loss = 0.01831137\n",
      "Iteration 26, loss = 0.05608871\n",
      "Iteration 38, loss = 0.03583744\n",
      "Iteration 23, loss = 0.09101753\n",
      "Iteration 48, loss = 0.02551884\n",
      "Iteration 28, loss = 0.00322918\n",
      "Iteration 33, loss = 0.03832676\n",
      "Iteration 12, loss = 0.01332896\n",
      "Iteration 12, loss = 0.01599389\n",
      "Iteration 39, loss = 0.03518576\n",
      "Iteration 49, loss = 0.02501542\n",
      "Iteration 34, loss = 0.03717840\n",
      "Iteration 32, loss = 0.00261975\n",
      "Iteration 24, loss = 0.03754832\n",
      "Iteration 13, loss = 0.01193377\n",
      "Iteration 13, loss = 0.01420268\n",
      "Iteration 32, loss = 0.00286350\n",
      "Iteration 32, loss = 0.00327329\n",
      "Iteration 40, loss = 0.03446940\n",
      "Iteration 50, loss = 0.02444601\n",
      "Iteration 35, loss = 0.03622934\n",
      "Iteration 27, loss = 0.03935411\n",
      "Iteration 14, loss = 0.01108307\n",
      "Iteration 24, loss = 0.07805457\n",
      "Iteration 14, loss = 0.01292842\n",
      "Iteration 29, loss = 0.00329546\n",
      "Iteration 41, loss = 0.03355494\n",
      "Iteration 51, loss = 0.02395474\n",
      "Iteration 36, loss = 0.03534471\n",
      "Iteration 15, loss = 0.01023602\n",
      "Iteration 33, loss = 0.00251562\n",
      "Iteration 15, loss = 0.01299656\n",
      "Iteration 25, loss = 0.04102152\n",
      "Iteration 42, loss = 0.03290148\n",
      "Iteration 33, loss = 0.00329029\n",
      "Iteration 33, loss = 0.00291107\n",
      "Iteration 52, loss = 0.02341083\n",
      "Iteration 37, loss = 0.03445695\n",
      "Iteration 16, loss = 0.00951304\n",
      "Iteration 28, loss = 0.03654917\n",
      "Iteration 16, loss = 0.01116194\n",
      "Iteration 25, loss = 0.07883242\n",
      "Iteration 43, loss = 0.03219100\n",
      "Iteration 30, loss = 0.00323489\n",
      "Iteration 53, loss = 0.02304486\n",
      "Iteration 38, loss = 0.03358045\n",
      "Iteration 17, loss = 0.00910162\n",
      "Iteration 17, loss = 0.01119356\n",
      "Iteration 34, loss = 0.00278938\n",
      "Iteration 44, loss = 0.03132281\n",
      "Iteration 26, loss = 0.03704988\n",
      "Iteration 54, loss = 0.02247477\n",
      "Iteration 39, loss = 0.03271394\n",
      "Iteration 34, loss = 0.00325740\n",
      "Iteration 34, loss = 0.00275690\n",
      "Iteration 18, loss = 0.00874464\n",
      "Iteration 18, loss = 0.01012704\n",
      "Iteration 45, loss = 0.03082261\n",
      "Iteration 29, loss = 0.03970475\n",
      "Iteration 55, loss = 0.02199706\n",
      "Iteration 40, loss = 0.03186673\n",
      "Iteration 26, loss = 0.05549457\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 0.00326777\n",
      "[CV 3/3; 12/36] END alpha=0.0001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.1;, score=0.880 total time= 2.0min\n",
      "Iteration 19, loss = 0.00846720\n",
      "Iteration 19, loss = 0.00981381\n",
      "[CV 3/3; 14/36] START alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.01\n",
      "Iteration 46, loss = 0.03018843\n",
      "Iteration 41, loss = 0.03126933\n",
      "Iteration 56, loss = 0.02168190\n",
      "Iteration 35, loss = 0.00259956\n",
      "Iteration 27, loss = 0.03044644\n",
      "Iteration 35, loss = 0.00311585\n",
      "Iteration 35, loss = 0.00324437\n",
      "Iteration 20, loss = 0.00820138\n",
      "Iteration 20, loss = 0.00972083\n",
      "Iteration 47, loss = 0.02950634\n",
      "Iteration 42, loss = 0.03041467\n",
      "Iteration 57, loss = 0.02122724\n",
      "Iteration 30, loss = 0.05010420\n",
      "Iteration 32, loss = 0.00315390\n",
      "Iteration 48, loss = 0.02893101\n",
      "Iteration 1, loss = 0.45333115\n",
      "Iteration 21, loss = 0.00797010\n",
      "Iteration 21, loss = 0.00905837\n",
      "Iteration 43, loss = 0.02990919\n",
      "Iteration 58, loss = 0.02080196\n",
      "Iteration 36, loss = 0.00258142\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 28, loss = 0.03324770\n",
      "Iteration 49, loss = 0.02837016\n",
      "[CV 2/3; 10/36] END alpha=0.0001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.001;, score=0.899 total time= 2.7min\n",
      "Iteration 22, loss = 0.00771771\n",
      "Iteration 59, loss = 0.02037857\n",
      "Iteration 2, loss = 0.17301364\n",
      "Iteration 22, loss = 0.00860358\n",
      "Iteration 44, loss = 0.02920451\n",
      "Iteration 36, loss = 0.00324328\n",
      "Iteration 36, loss = 0.00282238\n",
      "[CV 1/3; 15/36] START alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.1\n",
      "Iteration 31, loss = 0.07036530\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 50, loss = 0.02783062\n",
      "Iteration 60, loss = 0.02000758\n",
      "Iteration 45, loss = 0.02852223\n",
      "Iteration 33, loss = 0.00317572\n",
      "Iteration 23, loss = 0.00755700\n",
      "Iteration 23, loss = 0.00825536\n",
      "[CV 1/3; 12/36] END alpha=0.0001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.1;, score=0.880 total time= 2.4min\n",
      "Iteration 3, loss = 0.08894394\n",
      "[CV 2/3; 15/36] START alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.1\n",
      "Iteration 51, loss = 0.02742106\n",
      "Iteration 61, loss = 0.01960097\n",
      "Iteration 46, loss = 0.02801563\n",
      "Iteration 24, loss = 0.00813446\n",
      "Iteration 24, loss = 0.00737481\n",
      "Iteration 29, loss = 0.06474039\n",
      "Iteration 4, loss = 0.05874204\n",
      "Iteration 1, loss = 0.44136324\n",
      "Iteration 37, loss = 0.00328536\n",
      "Iteration 37, loss = 0.00313396\n",
      "Iteration 52, loss = 0.02703274\n",
      "Iteration 62, loss = 0.01932764\n",
      "Iteration 47, loss = 0.02741667\n",
      "Iteration 25, loss = 0.00779887\n",
      "Iteration 25, loss = 0.00729764\n",
      "Iteration 5, loss = 0.04218176\n",
      "Iteration 2, loss = 0.16889732\n",
      "Iteration 1, loss = 0.43107385\n",
      "Iteration 34, loss = 0.00317110\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 53, loss = 0.02660569\n",
      "[CV 3/3; 11/36] END alpha=0.0001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.01;, score=0.905 total time= 2.6min\n",
      "Iteration 63, loss = 0.01897749\n",
      "Iteration 48, loss = 0.02691761\n",
      "[CV 3/3; 15/36] START alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.1\n",
      "Iteration 26, loss = 0.00759362\n",
      "Iteration 26, loss = 0.00716825\n",
      "Iteration 6, loss = 0.03293427\n",
      "Iteration 3, loss = 0.09743256\n",
      "Iteration 2, loss = 0.16282515\n",
      "Iteration 30, loss = 0.07028298\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 38, loss = 0.00322570\n",
      "Iteration 38, loss = 0.00275592\n",
      "[CV 2/3; 12/36] END alpha=0.0001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.1;, score=0.883 total time= 2.3min\n",
      "Iteration 54, loss = 0.02584321\n",
      "[CV 1/3; 16/36] START alpha=0.001, hidden_layer_sizes=(100,), learning_rate_init=0.001\n",
      "Iteration 64, loss = 0.01857561\n",
      "Iteration 49, loss = 0.02636650\n",
      "Iteration 27, loss = 0.00743488\n",
      "Iteration 27, loss = 0.00699869\n",
      "Iteration 7, loss = 0.02849706\n",
      "Iteration 4, loss = 0.06900274\n",
      "Iteration 3, loss = 0.09897976\n",
      "Iteration 55, loss = 0.02552695\n",
      "Iteration 65, loss = 0.01832975\n",
      "Iteration 50, loss = 0.02589249\n",
      "Iteration 1, loss = 0.42431939\n",
      "Iteration 28, loss = 0.00728755\n",
      "Iteration 28, loss = 0.00689336\n",
      "Iteration 8, loss = 0.02371560\n",
      "Iteration 5, loss = 0.06122112\n",
      "Iteration 4, loss = 0.07309019\n",
      "Iteration 56, loss = 0.02503505\n",
      "Iteration 39, loss = 0.00282338\n",
      "Iteration 66, loss = 0.01799963\n",
      "Iteration 39, loss = 0.00322990\n",
      "Iteration 51, loss = 0.02555219\n",
      "Iteration 2, loss = 0.16368805\n",
      "Iteration 29, loss = 0.00684874\n",
      "Iteration 29, loss = 0.00735589\n",
      "Iteration 9, loss = 0.02071952\n",
      "Iteration 6, loss = 0.07516830\n",
      "Iteration 5, loss = 0.05958578\n",
      "Iteration 1, loss = 0.64338414\n",
      "Iteration 57, loss = 0.02472453\n",
      "Iteration 67, loss = 0.01772523\n",
      "Iteration 52, loss = 0.02502158\n",
      "Iteration 3, loss = 0.09738783\n",
      "Iteration 10, loss = 0.01777548\n",
      "Iteration 30, loss = 0.00714655\n",
      "Iteration 30, loss = 0.00672167\n",
      "Iteration 7, loss = 0.12150443\n",
      "Iteration 6, loss = 0.06094681\n",
      "Iteration 58, loss = 0.02427396\n",
      "Iteration 68, loss = 0.01743154\n",
      "Iteration 53, loss = 0.02463162\n",
      "Iteration 2, loss = 0.48075344\n",
      "Iteration 40, loss = 0.00322050\n",
      "Iteration 40, loss = 0.00282170\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 0.08198235\n",
      "Iteration 11, loss = 0.01643119\n",
      "Iteration 31, loss = 0.00705642\n",
      "Iteration 31, loss = 0.00656380\n",
      "Iteration 8, loss = 0.10779843\n",
      "[CV 1/3; 11/36] END alpha=0.0001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.01;, score=0.900 total time= 3.0min\n",
      "Iteration 7, loss = 0.07030328\n",
      "Iteration 59, loss = 0.02386947\n",
      "[CV 2/3; 16/36] START alpha=0.001, hidden_layer_sizes=(100,), learning_rate_init=0.001\n",
      "Iteration 54, loss = 0.02411138\n",
      "Iteration 69, loss = 0.01713577\n",
      "Iteration 5, loss = 0.07186145\n",
      "Iteration 32, loss = 0.00693654\n",
      "Iteration 12, loss = 0.01448084\n",
      "Iteration 32, loss = 0.00655015\n",
      "Iteration 9, loss = 0.09458887\n",
      "Iteration 8, loss = 0.13929399\n",
      "Iteration 60, loss = 0.02373332\n",
      "Iteration 3, loss = 0.35400614\n",
      "Iteration 70, loss = 0.01678258\n",
      "Iteration 55, loss = 0.02361202\n",
      "Iteration 6, loss = 0.09075396\n",
      "Iteration 13, loss = 0.01334727\n",
      "Iteration 33, loss = 0.00681484\n",
      "Iteration 41, loss = 0.00274383\n",
      "Iteration 10, loss = 0.09294759\n",
      "Iteration 33, loss = 0.00645830\n",
      "Iteration 9, loss = 0.13505037\n",
      "Iteration 61, loss = 0.02315613\n",
      "Iteration 71, loss = 0.01655149\n",
      "Iteration 56, loss = 0.02325631\n",
      "Iteration 1, loss = 0.64333780\n",
      "Iteration 4, loss = 0.27222755\n",
      "Iteration 7, loss = 0.09952928\n",
      "Iteration 34, loss = 0.00667072\n",
      "Iteration 14, loss = 0.01238932\n",
      "Iteration 11, loss = 0.08853440\n",
      "Iteration 34, loss = 0.00635773\n",
      "Iteration 62, loss = 0.02296491\n",
      "Iteration 10, loss = 0.11286074\n",
      "Iteration 72, loss = 0.01624458\n",
      "Iteration 57, loss = 0.02285748\n",
      "Iteration 8, loss = 0.11236337\n",
      "Iteration 35, loss = 0.00656710\n",
      "Iteration 15, loss = 0.01110239\n",
      "Iteration 63, loss = 0.02235579\n",
      "Iteration 12, loss = 0.08338485\n",
      "Iteration 35, loss = 0.00626729\n",
      "Iteration 11, loss = 0.10297486\n",
      "Iteration 42, loss = 0.00282982\n",
      "Iteration 73, loss = 0.01615420\n",
      "Iteration 58, loss = 0.02257660\n",
      "Iteration 2, loss = 0.47944336\n",
      "Iteration 5, loss = 0.21573461\n",
      "Iteration 9, loss = 0.10536608\n",
      "Iteration 64, loss = 0.02229983\n",
      "Iteration 36, loss = 0.00651122\n",
      "Iteration 16, loss = 0.01055766\n",
      "Iteration 13, loss = 0.08066814\n",
      "Iteration 36, loss = 0.00620090\n",
      "Iteration 12, loss = 0.09491711\n",
      "Iteration 74, loss = 0.01576108\n",
      "Iteration 59, loss = 0.02210391\n",
      "Iteration 3, loss = 0.35413817\n",
      "Iteration 6, loss = 0.17680047\n",
      "Iteration 65, loss = 0.02188118\n",
      "Iteration 14, loss = 0.07675366\n",
      "Iteration 10, loss = 0.09838431\n",
      "Iteration 37, loss = 0.00644204\n",
      "Iteration 17, loss = 0.00993394\n",
      "Iteration 37, loss = 0.00611640\n",
      "Iteration 75, loss = 0.01558021\n",
      "Iteration 60, loss = 0.02184256\n",
      "Iteration 13, loss = 0.08021477\n",
      "Iteration 43, loss = 0.00277498\n",
      "Iteration 66, loss = 0.02142587\n",
      "Iteration 15, loss = 0.08554364\n",
      "Iteration 11, loss = 0.11247763\n",
      "Iteration 38, loss = 0.00636471\n",
      "Iteration 18, loss = 0.00946620\n",
      "Iteration 76, loss = 0.01536060\n",
      "Iteration 38, loss = 0.00607076\n",
      "Iteration 61, loss = 0.02148590\n",
      "Iteration 14, loss = 0.07974212\n",
      "Iteration 7, loss = 0.14821703\n",
      "Iteration 4, loss = 0.27211411\n",
      "Iteration 67, loss = 0.02107484\n",
      "Iteration 16, loss = 0.08552197\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 0.13252041\n",
      "Iteration 39, loss = 0.00631028\n",
      "Iteration 77, loss = 0.01503872\n",
      "Iteration 62, loss = 0.02131344\n",
      "Iteration 19, loss = 0.00901983\n",
      "[CV 1/3; 15/36] END alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.1;, score=0.892 total time=  30.9s\n",
      "Iteration 39, loss = 0.00606613\n",
      "Iteration 15, loss = 0.07937723\n",
      "[CV 3/3; 16/36] START alpha=0.001, hidden_layer_sizes=(100,), learning_rate_init=0.001\n",
      "Iteration 44, loss = 0.00286650\n",
      "Iteration 68, loss = 0.02093678\n",
      "Iteration 8, loss = 0.12638303\n",
      "Iteration 5, loss = 0.21831947\n",
      "Iteration 78, loss = 0.01491332\n",
      "Iteration 63, loss = 0.02076013\n",
      "Iteration 13, loss = 0.11438302\n",
      "Iteration 40, loss = 0.00625826\n",
      "Iteration 20, loss = 0.00875436\n",
      "Iteration 40, loss = 0.00612492\n",
      "Iteration 16, loss = 0.08964800\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/3; 15/36] END alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.1;, score=0.885 total time=  31.3s\n",
      "[CV 1/3; 17/36] START alpha=0.001, hidden_layer_sizes=(100,), learning_rate_init=0.01\n",
      "Iteration 69, loss = 0.02049924\n",
      "Iteration 79, loss = 0.01455816\n",
      "Iteration 64, loss = 0.02054561\n",
      "Iteration 14, loss = 0.10322877\n",
      "Iteration 41, loss = 0.00631034\n",
      "Iteration 21, loss = 0.00854200\n",
      "Iteration 41, loss = 0.00591404\n",
      "Iteration 9, loss = 0.11029920\n",
      "Iteration 6, loss = 0.17759450\n",
      "Iteration 70, loss = 0.02021623\n",
      "Iteration 45, loss = 0.00348560\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64288447\n",
      "Iteration 80, loss = 0.01445448\n",
      "Iteration 65, loss = 0.02024735\n",
      "Iteration 15, loss = 0.09467980\n",
      "Iteration 42, loss = 0.00612909\n",
      "Iteration 22, loss = 0.00827007\n",
      "[CV 3/3; 10/36] END alpha=0.0001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.001;, score=0.894 total time= 3.3min\n",
      "Iteration 42, loss = 0.00586547\n",
      "[CV 2/3; 17/36] START alpha=0.001, hidden_layer_sizes=(100,), learning_rate_init=0.01\n",
      "Iteration 71, loss = 0.01987841\n",
      "Iteration 10, loss = 0.09715394\n",
      "Iteration 81, loss = 0.01426222\n",
      "Iteration 66, loss = 0.01986040\n",
      "Iteration 7, loss = 0.14944321\n",
      "Iteration 16, loss = 0.08981821\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 0.00605402\n",
      "Iteration 1, loss = 0.42170262\n",
      "Iteration 23, loss = 0.00813239\n",
      "Iteration 43, loss = 0.00581782\n",
      "[CV 3/3; 15/36] END alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.1;, score=0.891 total time=  31.3s\n",
      "[CV 3/3; 17/36] START alpha=0.001, hidden_layer_sizes=(100,), learning_rate_init=0.01\n",
      "Iteration 2, loss = 0.48016923\n",
      "Iteration 72, loss = 0.01974000\n",
      "Iteration 82, loss = 0.01401320\n",
      "Iteration 67, loss = 0.01955524\n",
      "Iteration 44, loss = 0.00594505\n",
      "Iteration 24, loss = 0.00796538\n",
      "Iteration 44, loss = 0.00583111\n",
      "Iteration 11, loss = 0.08671107\n",
      "Iteration 8, loss = 0.12817105\n",
      "Iteration 73, loss = 0.01936592\n",
      "Iteration 2, loss = 0.14800089\n",
      "Iteration 1, loss = 0.41776364\n",
      "Iteration 68, loss = 0.01933821\n",
      "Iteration 83, loss = 0.01378892\n",
      "Iteration 45, loss = 0.00588756\n",
      "Iteration 3, loss = 0.35656893\n",
      "Iteration 25, loss = 0.00775166\n",
      "Iteration 45, loss = 0.00575532\n",
      "Iteration 74, loss = 0.01922975\n",
      "Iteration 69, loss = 0.01905210\n",
      "Iteration 84, loss = 0.01368905\n",
      "Iteration 12, loss = 0.07795215\n",
      "Iteration 9, loss = 0.11197969\n",
      "Iteration 1, loss = 0.42231503\n",
      "Iteration 46, loss = 0.00587037\n",
      "Iteration 3, loss = 0.07672918\n",
      "Iteration 26, loss = 0.00760729\n",
      "Iteration 46, loss = 0.00571545\n",
      "Iteration 2, loss = 0.13899592\n",
      "Iteration 75, loss = 0.01885002\n",
      "Iteration 70, loss = 0.01879134\n",
      "Iteration 85, loss = 0.01346299\n",
      "Iteration 4, loss = 0.27595397\n",
      "Iteration 47, loss = 0.00581685\n",
      "Iteration 27, loss = 0.00747414\n",
      "Iteration 47, loss = 0.00562639\n",
      "Iteration 76, loss = 0.01869478\n",
      "Iteration 13, loss = 0.07076858\n",
      "Iteration 10, loss = 0.09919215\n",
      "Iteration 71, loss = 0.01848933\n",
      "Iteration 86, loss = 0.01332032\n",
      "Iteration 4, loss = 0.04897908\n",
      "Iteration 2, loss = 0.15832725\n",
      "Iteration 3, loss = 0.07836128\n",
      "Iteration 48, loss = 0.00578035\n",
      "Iteration 28, loss = 0.00735655\n",
      "Iteration 48, loss = 0.00572695\n",
      "Iteration 77, loss = 0.01836926\n",
      "Iteration 5, loss = 0.22235558\n",
      "Iteration 72, loss = 0.01833102\n",
      "Iteration 87, loss = 0.01311795\n",
      "Iteration 14, loss = 0.06515532\n",
      "Iteration 11, loss = 0.08825924\n",
      "Iteration 29, loss = 0.00737187\n",
      "Iteration 49, loss = 0.00573733\n",
      "Iteration 5, loss = 0.03326398\n",
      "Iteration 49, loss = 0.00559027\n",
      "Iteration 78, loss = 0.01833097\n",
      "Iteration 3, loss = 0.08013463\n",
      "Iteration 4, loss = 0.05624558\n",
      "Iteration 73, loss = 0.01801930\n",
      "Iteration 88, loss = 0.01291266\n",
      "Iteration 6, loss = 0.18193935\n",
      "Iteration 50, loss = 0.00565497\n",
      "Iteration 30, loss = 0.00710580\n",
      "Iteration 79, loss = 0.01790685\n",
      "Iteration 50, loss = 0.00560563\n",
      "Iteration 74, loss = 0.01777206\n",
      "Iteration 15, loss = 0.06047557\n",
      "Iteration 89, loss = 0.01276910\n",
      "Iteration 12, loss = 0.08031776\n",
      "Iteration 6, loss = 0.02736513\n",
      "Iteration 5, loss = 0.04254574\n",
      "Iteration 4, loss = 0.04947584\n",
      "Iteration 80, loss = 0.01757818\n",
      "Iteration 51, loss = 0.00563536\n",
      "Iteration 31, loss = 0.00708679\n",
      "Iteration 51, loss = 0.00552468\n",
      "Iteration 75, loss = 0.01750555\n",
      "Iteration 90, loss = 0.01261747\n",
      "Iteration 7, loss = 0.15302918\n",
      "Iteration 16, loss = 0.05570963\n",
      "Iteration 13, loss = 0.07355570\n",
      "Iteration 81, loss = 0.01756371\n",
      "Iteration 52, loss = 0.00562669\n",
      "Iteration 32, loss = 0.00695567\n",
      "Iteration 52, loss = 0.00546884\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 7, loss = 0.02125812\n",
      "Iteration 76, loss = 0.01731031\n",
      "[CV 1/3; 14/36] END alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.01;, score=0.908 total time= 1.7min\n",
      "Iteration 91, loss = 0.01247007\n",
      "Iteration 6, loss = 0.03362657\n",
      "[CV 1/3; 18/36] START alpha=0.001, hidden_layer_sizes=(100,), learning_rate_init=0.1\n",
      "Iteration 5, loss = 0.03417037\n",
      "Iteration 82, loss = 0.01726146\n",
      "Iteration 53, loss = 0.00559069\n",
      "Iteration 77, loss = 0.01705168\n",
      "Iteration 33, loss = 0.00677609\n",
      "Iteration 8, loss = 0.13068377\n",
      "Iteration 92, loss = 0.01231526\n",
      "Iteration 17, loss = 0.05206548\n",
      "Iteration 14, loss = 0.06762881\n",
      "Iteration 8, loss = 0.01717188\n",
      "Iteration 7, loss = 0.02878725\n",
      "Iteration 83, loss = 0.01711232\n",
      "Iteration 78, loss = 0.01685389\n",
      "Iteration 54, loss = 0.00561449\n",
      "Iteration 6, loss = 0.02841992\n",
      "Iteration 34, loss = 0.00670011\n",
      "Iteration 93, loss = 0.01215929\n",
      "Iteration 9, loss = 0.11438427\n",
      "Iteration 84, loss = 0.01674884\n",
      "Iteration 18, loss = 0.04878486\n",
      "Iteration 79, loss = 0.01684983\n",
      "Iteration 15, loss = 0.06324768\n",
      "Iteration 1, loss = 0.51652828\n",
      "Iteration 55, loss = 0.00550491\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 35, loss = 0.00666761\n",
      "Iteration 94, loss = 0.01199373\n",
      "Iteration 9, loss = 0.01529346\n",
      "[CV 2/3; 14/36] END alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.01;, score=0.898 total time= 1.8min\n",
      "Iteration 8, loss = 0.02421353\n",
      "[CV 2/3; 18/36] START alpha=0.001, hidden_layer_sizes=(100,), learning_rate_init=0.1\n",
      "Iteration 85, loss = 0.01653770\n",
      "Iteration 7, loss = 0.02328172\n",
      "Iteration 80, loss = 0.01642161\n",
      "Iteration 95, loss = 0.01183157\n",
      "Iteration 36, loss = 0.00663881\n",
      "Iteration 10, loss = 0.10036614\n",
      "Iteration 19, loss = 0.04596107\n",
      "Iteration 16, loss = 0.05835251\n",
      "Iteration 86, loss = 0.01636746\n",
      "Iteration 2, loss = 0.22483737\n",
      "Iteration 10, loss = 0.01318037\n",
      "Iteration 81, loss = 0.01630974\n",
      "Iteration 9, loss = 0.02199996\n",
      "Iteration 96, loss = 0.01173879\n",
      "Iteration 37, loss = 0.00670659\n",
      "Iteration 8, loss = 0.01889952\n",
      "Iteration 87, loss = 0.01615435\n",
      "Iteration 82, loss = 0.01603179\n",
      "Iteration 20, loss = 0.04389544\n",
      "Iteration 97, loss = 0.01153097\n",
      "Iteration 11, loss = 0.08988061\n",
      "Iteration 17, loss = 0.05494571\n",
      "Iteration 1, loss = 0.46098014\n",
      "Iteration 38, loss = 0.00658192\n",
      "Iteration 11, loss = 0.01187441\n",
      "Iteration 3, loss = 0.13172415\n",
      "Iteration 10, loss = 0.02011294\n",
      "Iteration 88, loss = 0.01599885\n",
      "Iteration 83, loss = 0.01577523\n",
      "Iteration 98, loss = 0.01144099\n",
      "Iteration 9, loss = 0.01589014\n",
      "Iteration 39, loss = 0.00646939\n",
      "Iteration 21, loss = 0.04123386\n",
      "Iteration 89, loss = 0.01586556\n",
      "Iteration 12, loss = 0.08075711\n",
      "Iteration 18, loss = 0.05202807\n",
      "Iteration 84, loss = 0.01560268\n",
      "Iteration 12, loss = 0.01088981\n",
      "Iteration 2, loss = 0.19444395\n",
      "Iteration 99, loss = 0.01134190\n",
      "Iteration 11, loss = 0.01673960\n",
      "Iteration 4, loss = 0.09945319\n",
      "Iteration 40, loss = 0.00654068\n",
      "Iteration 90, loss = 0.01566624\n",
      "Iteration 10, loss = 0.01402535\n",
      "Iteration 85, loss = 0.01537309\n",
      "Iteration 100, loss = 0.01119730\n",
      "Iteration 22, loss = 0.03927891\n",
      "Iteration 19, loss = 0.04902195\n",
      "Iteration 41, loss = 0.00685618\n",
      "Iteration 13, loss = 0.01014801\n",
      "Iteration 13, loss = 0.07391621\n",
      "Iteration 3, loss = 0.12590005\n",
      "Iteration 91, loss = 0.01542903\n",
      "Iteration 12, loss = 0.01485921\n",
      "Iteration 5, loss = 0.08237634\n",
      "Iteration 86, loss = 0.01519422\n",
      "Iteration 101, loss = 0.01114902\n",
      "Iteration 42, loss = 0.00657145\n",
      "Iteration 11, loss = 0.01235752\n",
      "Iteration 92, loss = 0.01521626\n",
      "Iteration 87, loss = 0.01507864\n",
      "Iteration 23, loss = 0.03775949\n",
      "Iteration 20, loss = 0.04698399\n",
      "Iteration 102, loss = 0.01099973\n",
      "Iteration 14, loss = 0.00964058\n",
      "Iteration 14, loss = 0.06740697\n",
      "Iteration 13, loss = 0.01379476\n",
      "Iteration 4, loss = 0.08732001\n",
      "Iteration 43, loss = 0.00630745\n",
      "Iteration 6, loss = 0.07338701\n",
      "Iteration 93, loss = 0.01512226\n",
      "Iteration 88, loss = 0.01486565\n",
      "Iteration 103, loss = 0.01085261\n",
      "Iteration 12, loss = 0.01159770\n",
      "Iteration 24, loss = 0.03616992\n",
      "Iteration 21, loss = 0.04444224\n",
      "Iteration 44, loss = 0.00607201\n",
      "Iteration 94, loss = 0.01498218\n",
      "Iteration 15, loss = 0.00932207\n",
      "Iteration 89, loss = 0.01478887\n",
      "Iteration 14, loss = 0.01290122\n",
      "Iteration 15, loss = 0.06208301\n",
      "Iteration 5, loss = 0.07278403\n",
      "Iteration 104, loss = 0.01072203\n",
      "Iteration 7, loss = 0.08103715\n",
      "Iteration 95, loss = 0.01474869\n",
      "Iteration 45, loss = 0.00602952\n",
      "Iteration 90, loss = 0.01452105\n",
      "Iteration 13, loss = 0.01073160\n",
      "Iteration 105, loss = 0.01066551\n",
      "Iteration 25, loss = 0.03452615\n",
      "Iteration 22, loss = 0.04312475\n",
      "Iteration 16, loss = 0.00897755\n",
      "Iteration 15, loss = 0.01140628\n",
      "Iteration 96, loss = 0.01453675\n",
      "Iteration 16, loss = 0.05764533\n",
      "Iteration 6, loss = 0.07265247\n",
      "Iteration 46, loss = 0.00599148\n",
      "Iteration 91, loss = 0.01439734\n",
      "Iteration 8, loss = 0.08062853\n",
      "Iteration 106, loss = 0.01056203\n",
      "Iteration 97, loss = 0.01437832\n",
      "Iteration 14, loss = 0.01012120\n",
      "Iteration 26, loss = 0.03324951\n",
      "Iteration 47, loss = 0.00600759\n",
      "Iteration 23, loss = 0.04072787\n",
      "Iteration 92, loss = 0.01417845\n",
      "Iteration 107, loss = 0.01041387\n",
      "Iteration 17, loss = 0.00866616\n",
      "Iteration 16, loss = 0.01055013\n",
      "Iteration 17, loss = 0.05390379\n",
      "Iteration 7, loss = 0.09491543\n",
      "Iteration 98, loss = 0.01428879\n",
      "Iteration 9, loss = 0.11006655\n",
      "Iteration 48, loss = 0.00582479\n",
      "Iteration 93, loss = 0.01402967\n",
      "Iteration 108, loss = 0.01031056\n",
      "Iteration 27, loss = 0.03197243\n",
      "Iteration 15, loss = 0.00955074\n",
      "Iteration 24, loss = 0.03927300\n",
      "Iteration 99, loss = 0.01419621\n",
      "Iteration 18, loss = 0.00837349\n",
      "Iteration 17, loss = 0.00996826\n",
      "Iteration 94, loss = 0.01384882\n",
      "Iteration 49, loss = 0.00595499\n",
      "Iteration 109, loss = 0.01024907\n",
      "Iteration 18, loss = 0.05094742\n",
      "Iteration 8, loss = 0.10747632\n",
      "Iteration 10, loss = 0.15047229\n",
      "Iteration 100, loss = 0.01399991\n",
      "Iteration 95, loss = 0.01379461\n",
      "Iteration 28, loss = 0.03082573\n",
      "Iteration 110, loss = 0.01017576\n",
      "Iteration 50, loss = 0.00585198\n",
      "Iteration 25, loss = 0.03756999\n",
      "Iteration 16, loss = 0.00930685\n",
      "Iteration 19, loss = 0.00813537\n",
      "Iteration 18, loss = 0.00954672\n",
      "Iteration 101, loss = 0.01378428\n",
      "Iteration 19, loss = 0.04786150\n",
      "Iteration 9, loss = 0.11238242\n",
      "Iteration 96, loss = 0.01356475\n",
      "Iteration 111, loss = 0.01007810\n",
      "Iteration 51, loss = 0.00594784\n",
      "Iteration 11, loss = 0.16514685\n",
      "Iteration 29, loss = 0.02971503\n",
      "Iteration 102, loss = 0.01372113\n",
      "Iteration 26, loss = 0.03617423\n",
      "Iteration 17, loss = 0.00892298\n",
      "Iteration 97, loss = 0.01342048\n",
      "Iteration 20, loss = 0.00795108\n",
      "Iteration 112, loss = 0.00994217\n",
      "Iteration 19, loss = 0.00914650\n",
      "Iteration 52, loss = 0.00573460\n",
      "Iteration 20, loss = 0.04519685\n",
      "Iteration 10, loss = 0.12264378\n",
      "Iteration 103, loss = 0.01351224\n",
      "Iteration 12, loss = 0.15009033\n",
      "Iteration 98, loss = 0.01331118\n",
      "Iteration 113, loss = 0.00984475\n",
      "Iteration 30, loss = 0.02868623\n",
      "Iteration 53, loss = 0.00571687\n",
      "Iteration 27, loss = 0.03499096\n",
      "Iteration 21, loss = 0.00776440\n",
      "Iteration 18, loss = 0.00860675\n",
      "Iteration 104, loss = 0.01339819\n",
      "Iteration 20, loss = 0.00881349\n",
      "Iteration 99, loss = 0.01309716\n",
      "Iteration 114, loss = 0.00975713\n",
      "Iteration 54, loss = 0.00576270\n",
      "Iteration 11, loss = 0.12094208\n",
      "Iteration 21, loss = 0.04277337\n",
      "Iteration 13, loss = 0.13697623\n",
      "Iteration 105, loss = 0.01326888\n",
      "Iteration 31, loss = 0.02801433\n",
      "Iteration 100, loss = 0.01296296\n",
      "Iteration 115, loss = 0.00966644\n",
      "Iteration 28, loss = 0.03382597\n",
      "Iteration 22, loss = 0.00756580\n",
      "Iteration 19, loss = 0.00833351\n",
      "Iteration 21, loss = 0.00852807\n",
      "Iteration 55, loss = 0.00571399\n",
      "Iteration 106, loss = 0.01312075\n",
      "Iteration 101, loss = 0.01298827\n",
      "Iteration 116, loss = 0.00959275\n",
      "Iteration 22, loss = 0.04092130\n",
      "Iteration 12, loss = 0.12687051\n",
      "Iteration 14, loss = 0.12323757\n",
      "Iteration 56, loss = 0.00579291\n",
      "Iteration 32, loss = 0.02696594\n",
      "Iteration 29, loss = 0.03294229\n",
      "Iteration 107, loss = 0.01298503\n",
      "Iteration 23, loss = 0.00738841\n",
      "Iteration 102, loss = 0.01274180\n",
      "Iteration 22, loss = 0.00832238\n",
      "Iteration 20, loss = 0.00814620\n",
      "Iteration 117, loss = 0.00951073\n",
      "Iteration 57, loss = 0.00605213\n",
      "Iteration 108, loss = 0.01288499\n",
      "Iteration 23, loss = 0.03895328\n",
      "Iteration 13, loss = 0.13216052\n",
      "Iteration 103, loss = 0.01256018\n",
      "Iteration 118, loss = 0.00945900\n",
      "Iteration 15, loss = 0.10472451\n",
      "Iteration 33, loss = 0.02618349\n",
      "Iteration 30, loss = 0.03174685\n",
      "Iteration 24, loss = 0.00726081\n",
      "Iteration 58, loss = 0.00925408\n",
      "Iteration 23, loss = 0.00806263\n",
      "Iteration 109, loss = 0.01274186\n",
      "Iteration 21, loss = 0.00797821\n",
      "Iteration 104, loss = 0.01242084\n",
      "Iteration 119, loss = 0.00936672\n",
      "Iteration 14, loss = 0.13549015\n",
      "Iteration 24, loss = 0.03733146\n",
      "Iteration 110, loss = 0.01262130\n",
      "Iteration 59, loss = 0.21578836\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 34, loss = 0.02544169\n",
      "Iteration 16, loss = 0.09184393\n",
      "[CV 3/3; 14/36] END alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.01;, score=0.874 total time= 1.9min\n",
      "Iteration 31, loss = 0.03141724\n",
      "Iteration 105, loss = 0.01229459\n",
      "Iteration 120, loss = 0.00931880\n",
      "[CV 3/3; 18/36] START alpha=0.001, hidden_layer_sizes=(100,), learning_rate_init=0.1\n",
      "Iteration 25, loss = 0.00710273\n",
      "Iteration 24, loss = 0.00781417\n",
      "Iteration 22, loss = 0.00777074\n",
      "Iteration 111, loss = 0.01260166\n",
      "Iteration 106, loss = 0.01223783\n",
      "Iteration 121, loss = 0.00924137\n",
      "Iteration 25, loss = 0.03584070\n",
      "Iteration 15, loss = 0.12500425\n",
      "Iteration 35, loss = 0.02471877\n",
      "Iteration 17, loss = 0.08317801\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 0.03001332\n",
      "Iteration 112, loss = 0.01245694\n",
      "Iteration 26, loss = 0.00694873\n",
      "[CV 1/3; 18/36] END alpha=0.001, hidden_layer_sizes=(100,), learning_rate_init=0.1;, score=0.891 total time=  54.6s\n",
      "Iteration 25, loss = 0.00777498\n",
      "[CV 1/3; 19/36] START alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate_init=0.001\n",
      "Iteration 107, loss = 0.01204901\n",
      "Iteration 122, loss = 0.00912073\n",
      "Iteration 23, loss = 0.00766170\n",
      "Iteration 113, loss = 0.01233585\n",
      "Iteration 1, loss = 0.47604912\n",
      "Iteration 36, loss = 0.02424008\n",
      "Iteration 26, loss = 0.03461862\n",
      "Iteration 16, loss = 0.12461548\n",
      "Iteration 123, loss = 0.00907835\n",
      "Iteration 108, loss = 0.01190185\n",
      "Iteration 33, loss = 0.02929086\n",
      "Iteration 27, loss = 0.00684359\n",
      "Iteration 26, loss = 0.00805821\n",
      "Iteration 114, loss = 0.01219764\n",
      "Iteration 24, loss = 0.00756275\n",
      "Iteration 109, loss = 0.01198037\n",
      "Iteration 124, loss = 0.00901392\n",
      "Iteration 37, loss = 0.02332788\n",
      "Iteration 2, loss = 0.20435378\n",
      "Iteration 1, loss = 0.63769398\n",
      "Iteration 17, loss = 0.12489972\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 27, loss = 0.03328047\n",
      "Iteration 115, loss = 0.01206803\n",
      "Iteration 34, loss = 0.02846646\n",
      "[CV 2/3; 18/36] END alpha=0.001, hidden_layer_sizes=(100,), learning_rate_init=0.1;, score=0.895 total time=  54.5s\n",
      "Iteration 110, loss = 0.01183594\n",
      "Iteration 125, loss = 0.00894415\n",
      "Iteration 28, loss = 0.00671891\n",
      "[CV 2/3; 19/36] START alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate_init=0.001\n",
      "Iteration 27, loss = 0.00753158\n",
      "Iteration 25, loss = 0.00739944\n",
      "Iteration 116, loss = 0.01206189\n",
      "Iteration 111, loss = 0.01163699\n",
      "Iteration 126, loss = 0.00888523\n",
      "Iteration 38, loss = 0.02282683\n",
      "Iteration 2, loss = 0.40850839\n",
      "Iteration 3, loss = 0.11556364\n",
      "Iteration 28, loss = 0.03212937\n",
      "Iteration 35, loss = 0.02761097\n",
      "Iteration 29, loss = 0.00661513\n",
      "Iteration 117, loss = 0.01178403\n",
      "Iteration 28, loss = 0.00732808\n",
      "Iteration 112, loss = 0.01156645\n",
      "Iteration 127, loss = 0.00883520\n",
      "Iteration 26, loss = 0.00869120\n",
      "Iteration 1, loss = 0.64049297\n",
      "Iteration 118, loss = 0.01176768\n",
      "Iteration 39, loss = 0.02221543\n",
      "Iteration 3, loss = 0.22651259\n",
      "Iteration 113, loss = 0.01146055\n",
      "Iteration 4, loss = 0.07838297\n",
      "Iteration 128, loss = 0.00873101\n",
      "Iteration 36, loss = 0.02697123\n",
      "Iteration 29, loss = 0.03129458\n",
      "Iteration 30, loss = 0.00650683\n",
      "Iteration 29, loss = 0.00730213\n",
      "Iteration 119, loss = 0.01165201\n",
      "Iteration 114, loss = 0.01132867\n",
      "Iteration 129, loss = 0.00868726\n",
      "Iteration 27, loss = 0.00785540\n",
      "Iteration 40, loss = 0.02192848\n",
      "Iteration 2, loss = 0.40864669\n",
      "Iteration 4, loss = 0.13470015\n",
      "Iteration 120, loss = 0.01166249\n",
      "Iteration 37, loss = 0.02632535\n",
      "Iteration 5, loss = 0.06530662\n",
      "Iteration 115, loss = 0.01120416\n",
      "Iteration 30, loss = 0.03012530\n",
      "Iteration 31, loss = 0.00638645\n",
      "Iteration 130, loss = 0.00863269\n",
      "Iteration 30, loss = 0.00799744\n",
      "Iteration 121, loss = 0.01158372\n",
      "Iteration 116, loss = 0.01115941\n",
      "Iteration 28, loss = 0.00850066\n",
      "Iteration 3, loss = 0.22740788\n",
      "Iteration 131, loss = 0.00859309\n",
      "Iteration 41, loss = 0.02109210\n",
      "Iteration 5, loss = 0.08948509\n",
      "Iteration 38, loss = 0.02592684\n",
      "Iteration 6, loss = 0.07113643\n",
      "Iteration 32, loss = 0.00627088\n",
      "Iteration 31, loss = 0.02946278\n",
      "Iteration 122, loss = 0.01154436\n",
      "Iteration 31, loss = 0.06143752\n",
      "Iteration 117, loss = 0.01099259\n",
      "Iteration 132, loss = 0.00849880\n",
      "Iteration 29, loss = 0.11665319\n",
      "Iteration 42, loss = 0.02042491\n",
      "Iteration 4, loss = 0.13466290\n",
      "Iteration 123, loss = 0.01130017\n",
      "Iteration 6, loss = 0.06235681\n",
      "Iteration 118, loss = 0.01097341\n",
      "Iteration 39, loss = 0.02504548\n",
      "Iteration 133, loss = 0.00844664\n",
      "Iteration 33, loss = 0.00619313\n",
      "Iteration 7, loss = 0.08990791\n",
      "Iteration 32, loss = 0.02847246\n",
      "Iteration 32, loss = 0.21425402\n",
      "Iteration 124, loss = 0.01134832\n",
      "Iteration 119, loss = 0.01094746\n",
      "Iteration 134, loss = 0.00843234\n",
      "Iteration 43, loss = 0.02000912\n",
      "Iteration 30, loss = 0.18877592\n",
      "Iteration 5, loss = 0.08977287\n",
      "Iteration 7, loss = 0.04699899\n",
      "Iteration 40, loss = 0.02444397\n",
      "Iteration 34, loss = 0.00612582\n",
      "Iteration 125, loss = 0.01118054\n",
      "Iteration 120, loss = 0.01073931\n",
      "Iteration 8, loss = 0.13842849\n",
      "Iteration 33, loss = 0.07864723\n",
      "Iteration 33, loss = 0.02745560\n",
      "Iteration 135, loss = 0.00842874\n",
      "Iteration 126, loss = 0.01113781\n",
      "Iteration 121, loss = 0.01069013\n",
      "Iteration 44, loss = 0.01964389\n",
      "Iteration 6, loss = 0.06434040\n",
      "Iteration 31, loss = 0.05180073\n",
      "Iteration 136, loss = 0.00833385\n",
      "Iteration 8, loss = 0.03711071\n",
      "Iteration 41, loss = 0.02379545\n",
      "Iteration 35, loss = 0.00602922\n",
      "Iteration 34, loss = 0.03398709\n",
      "Iteration 9, loss = 0.14343343\n",
      "Iteration 127, loss = 0.01100216\n",
      "Iteration 34, loss = 0.02681132\n",
      "Iteration 122, loss = 0.01056931\n",
      "Iteration 137, loss = 0.00824567\n",
      "Iteration 45, loss = 0.01912732\n",
      "Iteration 7, loss = 0.05063666\n",
      "Iteration 9, loss = 0.03078606\n",
      "Iteration 32, loss = 0.02743741\n",
      "Iteration 128, loss = 0.01087128\n",
      "Iteration 123, loss = 0.01052225\n",
      "Iteration 42, loss = 0.02328789\n",
      "Iteration 36, loss = 0.00592798\n",
      "Iteration 138, loss = 0.00817292\n",
      "Iteration 35, loss = 0.02513997\n",
      "Iteration 10, loss = 0.13467723\n",
      "Iteration 35, loss = 0.02630198\n",
      "Iteration 129, loss = 0.01078323\n",
      "Iteration 124, loss = 0.01044513\n",
      "Iteration 139, loss = 0.00816294\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 46, loss = 0.01862458\n",
      "[CV 1/3; 13/36] END alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.001;, score=0.893 total time= 4.2min\n",
      "Iteration 8, loss = 0.04052660\n",
      "Iteration 10, loss = 0.02524715\n",
      "[CV 3/3; 19/36] START alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate_init=0.001\n",
      "Iteration 33, loss = 0.02290688\n",
      "Iteration 43, loss = 0.02267220\n",
      "Iteration 37, loss = 0.00585071\n",
      "Iteration 130, loss = 0.01061713\n",
      "Iteration 125, loss = 0.01035822\n",
      "Iteration 36, loss = 0.02254393\n",
      "Iteration 11, loss = 0.11538444\n",
      "Iteration 36, loss = 0.02554207\n",
      "Iteration 131, loss = 0.01054952\n",
      "Iteration 47, loss = 0.01838135\n",
      "Iteration 126, loss = 0.01025246\n",
      "Iteration 9, loss = 0.03591929\n",
      "Iteration 11, loss = 0.02173079\n",
      "Iteration 44, loss = 0.02248648\n",
      "Iteration 34, loss = 0.02126616\n",
      "Iteration 38, loss = 0.00578971\n",
      "Iteration 37, loss = 0.02104430\n",
      "Iteration 132, loss = 0.01047889\n",
      "Iteration 127, loss = 0.01019408\n",
      "Iteration 12, loss = 0.10391690\n",
      "Iteration 37, loss = 0.02485303\n",
      "Iteration 1, loss = 0.64070155\n",
      "Iteration 48, loss = 0.01781145\n",
      "Iteration 10, loss = 0.03019470\n",
      "Iteration 12, loss = 0.01893133\n",
      "Iteration 133, loss = 0.01042386\n",
      "Iteration 45, loss = 0.02192308\n",
      "Iteration 128, loss = 0.01009812\n",
      "Iteration 39, loss = 0.00574233\n",
      "Iteration 35, loss = 0.02027539\n",
      "Iteration 38, loss = 0.01999414\n",
      "Iteration 13, loss = 0.09196844\n",
      "Iteration 38, loss = 0.02431461\n",
      "Iteration 134, loss = 0.01050457\n",
      "Iteration 129, loss = 0.01001441\n",
      "Iteration 2, loss = 0.41517866\n",
      "Iteration 49, loss = 0.01742646\n",
      "Iteration 11, loss = 0.02570915\n",
      "Iteration 13, loss = 0.01621867\n",
      "Iteration 46, loss = 0.02151817\n",
      "Iteration 40, loss = 0.00571571\n",
      "Iteration 36, loss = 0.01934805\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 135, loss = 0.01026783\n",
      "Iteration 130, loss = 0.00989238\n",
      "Iteration 39, loss = 0.01911497\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/3; 17/36] END alpha=0.001, hidden_layer_sizes=(100,), learning_rate_init=0.01;, score=0.909 total time= 1.9min\n",
      "[CV 2/3; 17/36] END alpha=0.001, hidden_layer_sizes=(100,), learning_rate_init=0.01;, score=0.898 total time= 1.9min\n",
      "[CV 1/3; 20/36] START alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate_init=0.01\n",
      "[CV 2/3; 20/36] START alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate_init=0.01\n",
      "Iteration 14, loss = 0.08290674\n",
      "Iteration 39, loss = 0.02367062\n",
      "Iteration 50, loss = 0.01708373\n",
      "Iteration 3, loss = 0.23489396\n",
      "Iteration 136, loss = 0.01021713\n",
      "Iteration 131, loss = 0.00984437\n",
      "Iteration 14, loss = 0.01418143\n",
      "Iteration 12, loss = 0.02266771\n",
      "Iteration 47, loss = 0.02111265\n",
      "Iteration 41, loss = 0.00564787\n",
      "Iteration 137, loss = 0.01024341\n",
      "Iteration 132, loss = 0.00982448\n",
      "Iteration 15, loss = 0.07604458\n",
      "Iteration 40, loss = 0.02304544\n",
      "Iteration 51, loss = 0.01674020\n",
      "Iteration 4, loss = 0.14144518\n",
      "Iteration 13, loss = 0.02029112\n",
      "Iteration 15, loss = 0.01220589\n",
      "Iteration 1, loss = 0.43398331\n",
      "Iteration 48, loss = 0.02054509\n",
      "Iteration 1, loss = 0.41721759\n",
      "Iteration 133, loss = 0.00974032\n",
      "Iteration 138, loss = 0.01013658\n",
      "Iteration 42, loss = 0.00557611\n",
      "Iteration 16, loss = 0.08345608\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 134, loss = 0.00967975\n",
      "Iteration 139, loss = 0.01008672\n",
      "Iteration 41, loss = 0.02256642\n",
      "Iteration 52, loss = 0.01640139\n",
      "[CV 3/3; 18/36] END alpha=0.001, hidden_layer_sizes=(100,), learning_rate_init=0.1;, score=0.893 total time=  51.9s\n",
      "[CV 3/3; 20/36] START alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate_init=0.01\n",
      "Iteration 14, loss = 0.01801956\n",
      "Iteration 16, loss = 0.01036020\n",
      "Iteration 5, loss = 0.09260005\n",
      "Iteration 49, loss = 0.02029572\n",
      "Iteration 2, loss = 0.14018144\n",
      "Iteration 2, loss = 0.13184441\n",
      "Iteration 43, loss = 0.00549457\n",
      "Iteration 135, loss = 0.00958164\n",
      "Iteration 140, loss = 0.00987487\n",
      "Iteration 53, loss = 0.01606153\n",
      "Iteration 42, loss = 0.02208990\n",
      "Iteration 136, loss = 0.00954924\n",
      "Iteration 141, loss = 0.00979887\n",
      "Iteration 17, loss = 0.00928999\n",
      "Iteration 15, loss = 0.01583377\n",
      "Iteration 50, loss = 0.01965323\n",
      "Iteration 3, loss = 0.06097477\n",
      "Iteration 6, loss = 0.06530289\n",
      "Iteration 3, loss = 0.06946592\n",
      "Iteration 44, loss = 0.00545480\n",
      "Iteration 137, loss = 0.00949335\n",
      "Iteration 142, loss = 0.00975550\n",
      "Iteration 1, loss = 0.43070692\n",
      "Iteration 54, loss = 0.01574927\n",
      "Iteration 43, loss = 0.02158588\n",
      "Iteration 18, loss = 0.00815986\n",
      "Iteration 16, loss = 0.01396876\n",
      "Iteration 51, loss = 0.01930895\n",
      "Iteration 4, loss = 0.04029407\n",
      "Iteration 4, loss = 0.03196939\n",
      "Iteration 7, loss = 0.05065330\n",
      "Iteration 138, loss = 0.00940653\n",
      "Iteration 45, loss = 0.00537890\n",
      "Iteration 143, loss = 0.00972059\n",
      "Iteration 2, loss = 0.13463796\n",
      "Iteration 55, loss = 0.01527171\n",
      "Iteration 139, loss = 0.00931538\n",
      "Iteration 144, loss = 0.00965709\n",
      "Iteration 44, loss = 0.02128854\n",
      "Iteration 19, loss = 0.00767638\n",
      "Iteration 17, loss = 0.01212144\n",
      "Iteration 52, loss = 0.01900654\n",
      "Iteration 5, loss = 0.02703965\n",
      "Iteration 46, loss = 0.00534303\n",
      "Iteration 5, loss = 0.02033789\n",
      "Iteration 8, loss = 0.03951681\n",
      "Iteration 140, loss = 0.00927285\n",
      "Iteration 145, loss = 0.00982544\n",
      "Iteration 3, loss = 0.05966818\n",
      "Iteration 56, loss = 0.01510360\n",
      "Iteration 45, loss = 0.02081951\n",
      "Iteration 141, loss = 0.00920653\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 146, loss = 0.00953581\n",
      "Iteration 20, loss = 0.00683738\n",
      "Iteration 18, loss = 0.01117913\n",
      "Iteration 53, loss = 0.01852532\n",
      "Iteration 6, loss = 0.02115099\n",
      "Iteration 47, loss = 0.00534313\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 0.01592275\n",
      "[CV 3/3; 13/36] END alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.001;, score=0.890 total time= 4.3min\n",
      "Iteration 9, loss = 0.03247028\n",
      "[CV 1/3; 21/36] START alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate_init=0.1\n",
      "[CV 1/3; 17/36] END alpha=0.001, hidden_layer_sizes=(100,), learning_rate_init=0.01;, score=0.909 total time= 2.3min\n",
      "[CV 2/3; 21/36] START alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate_init=0.1\n",
      "Iteration 4, loss = 0.03253880\n",
      "Iteration 147, loss = 0.00949716\n",
      "Iteration 57, loss = 0.01476029\n",
      "Iteration 46, loss = 0.02024753\n",
      "Iteration 21, loss = 0.00627969\n",
      "Iteration 54, loss = 0.01826786\n",
      "Iteration 19, loss = 0.01011886\n",
      "Iteration 7, loss = 0.01576239\n",
      "Iteration 7, loss = 0.01086154\n",
      "Iteration 148, loss = 0.00938974\n",
      "Iteration 10, loss = 0.02686709\n",
      "Iteration 5, loss = 0.02072239\n",
      "Iteration 58, loss = 0.01451205\n",
      "Iteration 149, loss = 0.00933544\n",
      "Iteration 1, loss = 0.68421210\n",
      "Iteration 1, loss = 1.00498324\n",
      "Iteration 47, loss = 0.01994919\n",
      "Iteration 22, loss = 0.00594471\n",
      "Iteration 55, loss = 0.01775372\n",
      "Iteration 20, loss = 0.00910324\n",
      "Iteration 8, loss = 0.01009175\n",
      "Iteration 8, loss = 0.01162346\n",
      "Iteration 11, loss = 0.02312665\n",
      "Iteration 150, loss = 0.00920567\n",
      "Iteration 6, loss = 0.01437079\n",
      "Iteration 59, loss = 0.01426375\n",
      "Iteration 151, loss = 0.00921514\n",
      "Iteration 2, loss = 0.22512002\n",
      "Iteration 56, loss = 0.01764473\n",
      "Iteration 2, loss = 0.23492233\n",
      "Iteration 23, loss = 0.00548484\n",
      "Iteration 48, loss = 0.01950800\n",
      "Iteration 21, loss = 0.00831914\n",
      "Iteration 9, loss = 0.02133157\n",
      "Iteration 9, loss = 0.01319055\n",
      "Iteration 12, loss = 0.02037243\n",
      "Iteration 7, loss = 0.01185326\n",
      "Iteration 152, loss = 0.00916144\n",
      "Iteration 60, loss = 0.01390263\n",
      "Iteration 57, loss = 0.01727874\n",
      "Iteration 3, loss = 0.13954744\n",
      "Iteration 24, loss = 0.00523940\n",
      "Iteration 3, loss = 0.15499881\n",
      "Iteration 49, loss = 0.01914904\n",
      "Iteration 22, loss = 0.00759714\n",
      "Iteration 10, loss = 0.04964840\n",
      "Iteration 10, loss = 0.01225004\n",
      "Iteration 153, loss = 0.00906437\n",
      "Iteration 13, loss = 0.01810478\n",
      "Iteration 8, loss = 0.01383084\n",
      "Iteration 61, loss = 0.01369524\n",
      "Iteration 154, loss = 0.00900363\n",
      "Iteration 58, loss = 0.01692403\n",
      "Iteration 4, loss = 0.09816722\n",
      "Iteration 25, loss = 0.00496271\n",
      "Iteration 23, loss = 0.00716402\n",
      "Iteration 11, loss = 0.04131886\n",
      "Iteration 50, loss = 0.01879730\n",
      "Iteration 4, loss = 0.11036898\n",
      "Iteration 11, loss = 0.01831043\n",
      "Iteration 155, loss = 0.00895679\n",
      "Iteration 14, loss = 0.01571053\n",
      "Iteration 9, loss = 0.02942491\n",
      "Iteration 62, loss = 0.01343189\n",
      "Iteration 59, loss = 0.01673232\n",
      "Iteration 5, loss = 0.08218987\n",
      "Iteration 26, loss = 0.00481583\n",
      "Iteration 156, loss = 0.00887408\n",
      "Iteration 24, loss = 0.00687215\n",
      "Iteration 12, loss = 0.03070065\n",
      "Iteration 51, loss = 0.01845482\n",
      "Iteration 12, loss = 0.02089802\n",
      "Iteration 5, loss = 0.10390963\n",
      "Iteration 10, loss = 0.03796637\n",
      "Iteration 63, loss = 0.01324204\n",
      "Iteration 15, loss = 0.01417374\n",
      "Iteration 157, loss = 0.00885747\n",
      "Iteration 60, loss = 0.01658951\n",
      "Iteration 6, loss = 0.09380950\n",
      "Iteration 27, loss = 0.00468962\n",
      "Iteration 13, loss = 0.01667582\n",
      "Iteration 25, loss = 0.00647685\n",
      "Iteration 13, loss = 0.02868255\n",
      "Iteration 52, loss = 0.01805543\n",
      "Iteration 6, loss = 0.10350300\n",
      "Iteration 158, loss = 0.00878568\n",
      "Iteration 64, loss = 0.01298309\n",
      "Iteration 11, loss = 0.02928116\n",
      "Iteration 16, loss = 0.01282033\n",
      "Iteration 159, loss = 0.00877130\n",
      "Iteration 61, loss = 0.01635693\n",
      "Iteration 7, loss = 0.07346856\n",
      "Iteration 28, loss = 0.00453576\n",
      "Iteration 14, loss = 0.01082207\n",
      "Iteration 26, loss = 0.00610145\n",
      "Iteration 14, loss = 0.02448192\n",
      "Iteration 53, loss = 0.01777267\n",
      "Iteration 7, loss = 0.09915252\n",
      "Iteration 160, loss = 0.00864946\n",
      "Iteration 65, loss = 0.01280863\n",
      "Iteration 12, loss = 0.02002620\n",
      "Iteration 17, loss = 0.01099340\n",
      "Iteration 62, loss = 0.01576821\n",
      "Iteration 8, loss = 0.07325117\n",
      "Iteration 29, loss = 0.00439415\n",
      "Iteration 161, loss = 0.00868855\n",
      "Iteration 15, loss = 0.00935016\n",
      "Iteration 27, loss = 0.00574019\n",
      "Iteration 15, loss = 0.01776492\n",
      "Iteration 54, loss = 0.01736522\n",
      "Iteration 8, loss = 0.09593095\n",
      "Iteration 66, loss = 0.01257904\n",
      "Iteration 13, loss = 0.01306169\n",
      "Iteration 162, loss = 0.00860601\n",
      "Iteration 18, loss = 0.01014879\n",
      "Iteration 63, loss = 0.01556014\n",
      "Iteration 9, loss = 0.08256838\n",
      "Iteration 30, loss = 0.00439460\n",
      "Iteration 16, loss = 0.00900322\n",
      "Iteration 28, loss = 0.00548930\n",
      "Iteration 16, loss = 0.01211867\n",
      "Iteration 163, loss = 0.00862868\n",
      "Iteration 55, loss = 0.01698855\n",
      "Iteration 9, loss = 0.09514868\n",
      "Iteration 67, loss = 0.01243028\n",
      "Iteration 14, loss = 0.01034118\n",
      "Iteration 64, loss = 0.01528574\n",
      "Iteration 19, loss = 0.00928931\n",
      "Iteration 164, loss = 0.00851702\n",
      "Iteration 31, loss = 0.00413254\n",
      "Iteration 10, loss = 0.08169420\n",
      "Iteration 17, loss = 0.00874269\n",
      "Iteration 29, loss = 0.00530354\n",
      "Iteration 17, loss = 0.00947350\n",
      "Iteration 56, loss = 0.01672904\n",
      "Iteration 68, loss = 0.01219472\n",
      "Iteration 10, loss = 0.10334520\n",
      "Iteration 165, loss = 0.00845473\n",
      "Iteration 15, loss = 0.00945784\n",
      "Iteration 65, loss = 0.01497005\n",
      "Iteration 20, loss = 0.00843456\n",
      "Iteration 11, loss = 0.07894045\n",
      "Iteration 32, loss = 0.00409901\n",
      "Iteration 166, loss = 0.00838438\n",
      "Iteration 18, loss = 0.00851687\n",
      "Iteration 30, loss = 0.00516731\n",
      "Iteration 18, loss = 0.00893143\n",
      "Iteration 69, loss = 0.01196511\n",
      "Iteration 57, loss = 0.01644551\n",
      "Iteration 11, loss = 0.10543761\n",
      "Iteration 16, loss = 0.00898683\n",
      "Iteration 167, loss = 0.00842302\n",
      "Iteration 66, loss = 0.01497531\n",
      "Iteration 21, loss = 0.00779469\n",
      "Iteration 12, loss = 0.07847917\n",
      "Iteration 33, loss = 0.00401624\n",
      "Iteration 31, loss = 0.00505303\n",
      "Iteration 19, loss = 0.00850896\n",
      "Iteration 19, loss = 0.00875157\n",
      "Iteration 168, loss = 0.00843135\n",
      "Iteration 70, loss = 0.01185070\n",
      "Iteration 58, loss = 0.01617197\n",
      "Iteration 12, loss = 0.10270988\n",
      "Iteration 17, loss = 0.00880753\n",
      "Iteration 67, loss = 0.01453706\n",
      "Iteration 169, loss = 0.00830658\n",
      "Iteration 22, loss = 0.00731757\n",
      "Iteration 13, loss = 0.08006271\n",
      "Iteration 34, loss = 0.00397940\n",
      "Iteration 20, loss = 0.00815374\n",
      "Iteration 32, loss = 0.00494034\n",
      "Iteration 20, loss = 0.00853244\n",
      "Iteration 71, loss = 0.01162232\n",
      "Iteration 59, loss = 0.01590220\n",
      "Iteration 170, loss = 0.00832559\n",
      "Iteration 18, loss = 0.00850290\n",
      "Iteration 13, loss = 0.09815918\n",
      "Iteration 68, loss = 0.01446979\n",
      "Iteration 14, loss = 0.08440893\n",
      "Iteration 35, loss = 0.00393039\n",
      "Iteration 23, loss = 0.00695477\n",
      "Iteration 171, loss = 0.00820786\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 33, loss = 0.00482027\n",
      "Iteration 21, loss = 0.00808272\n",
      "Iteration 21, loss = 0.00836242\n",
      "Iteration 72, loss = 0.01146862\n",
      "[CV 2/3; 13/36] END alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.001;, score=0.889 total time= 5.2min\n",
      "[CV 3/3; 21/36] START alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate_init=0.1\n",
      "Iteration 60, loss = 0.01565929\n",
      "Iteration 19, loss = 0.00821272\n",
      "Iteration 14, loss = 0.10339724\n",
      "Iteration 69, loss = 0.01411592\n",
      "Iteration 15, loss = 0.08393672\n",
      "Iteration 36, loss = 0.00386035\n",
      "Iteration 24, loss = 0.00643294\n",
      "Iteration 34, loss = 0.00475619\n",
      "Iteration 22, loss = 0.00797431\n",
      "Iteration 22, loss = 0.00819752\n",
      "Iteration 73, loss = 0.01132170\n",
      "Iteration 20, loss = 0.00814777\n",
      "Iteration 61, loss = 0.01536060\n",
      "Iteration 15, loss = 0.09604126\n",
      "Iteration 70, loss = 0.01398979\n",
      "Iteration 16, loss = 0.09981374\n",
      "Iteration 37, loss = 0.00388564\n",
      "Iteration 25, loss = 0.00613829\n",
      "Iteration 1, loss = 0.73640764\n",
      "Iteration 35, loss = 0.00457621\n",
      "Iteration 23, loss = 0.00777575\n",
      "Iteration 23, loss = 0.00802470\n",
      "Iteration 74, loss = 0.01115421\n",
      "Iteration 21, loss = 0.00803965\n",
      "Iteration 62, loss = 0.01504097\n",
      "Iteration 71, loss = 0.01365801\n",
      "Iteration 16, loss = 0.09494170\n",
      "Iteration 17, loss = 0.09680716\n",
      "Iteration 38, loss = 0.00383499\n",
      "Iteration 2, loss = 0.24563117\n",
      "Iteration 26, loss = 0.00585338\n",
      "Iteration 24, loss = 0.00769184\n",
      "Iteration 36, loss = 0.00453249\n",
      "Iteration 75, loss = 0.01098781\n",
      "Iteration 24, loss = 0.00799156\n",
      "Iteration 22, loss = 0.00798866\n",
      "Iteration 63, loss = 0.01497767\n",
      "Iteration 72, loss = 0.01353410\n",
      "Iteration 17, loss = 0.09914893\n",
      "Iteration 18, loss = 0.08404264\n",
      "Iteration 39, loss = 0.00378837\n",
      "Iteration 25, loss = 0.00742690\n",
      "Iteration 37, loss = 0.00442771\n",
      "Iteration 27, loss = 0.00565529\n",
      "Iteration 3, loss = 0.14884342\n",
      "Iteration 76, loss = 0.01084439\n",
      "Iteration 25, loss = 0.00786539\n",
      "Iteration 23, loss = 0.00774169\n",
      "Iteration 64, loss = 0.01456044\n",
      "Iteration 73, loss = 0.01332985\n",
      "Iteration 18, loss = 0.10208871\n",
      "Iteration 19, loss = 0.07863411\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 0.00370688\n",
      "[CV 2/3; 21/36] END alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate_init=0.1;, score=0.897 total time=  59.8s\n",
      "Iteration 26, loss = 0.00742293Iteration 38, loss = 0.00434884\n",
      "\n",
      "[CV 1/3; 22/36] START alpha=0.001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.001\n",
      "Iteration 77, loss = 0.01076600\n",
      "Iteration 26, loss = 0.00764415\n",
      "Iteration 28, loss = 0.00539446\n",
      "Iteration 4, loss = 0.10063959\n",
      "Iteration 24, loss = 0.00762320\n",
      "Iteration 74, loss = 0.01321938\n",
      "Iteration 65, loss = 0.01442129\n",
      "Iteration 41, loss = 0.00374183\n",
      "Iteration 19, loss = 0.11273718\n",
      "Iteration 78, loss = 0.01061265\n",
      "Iteration 27, loss = 0.00747048\n",
      "Iteration 39, loss = 0.00432885\n",
      "Iteration 27, loss = 0.00743625\n",
      "Iteration 5, loss = 0.08735868\n",
      "Iteration 29, loss = 0.00521924\n",
      "Iteration 25, loss = 0.00761555\n",
      "Iteration 75, loss = 0.01293288\n",
      "Iteration 66, loss = 0.01419484\n",
      "Iteration 42, loss = 0.00373535\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 20, loss = 0.10536194\n",
      "[CV 1/3; 19/36] END alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate_init=0.001;, score=0.899 total time= 2.1min\n",
      "Iteration 79, loss = 0.01043791\n",
      "Iteration 28, loss = 0.00725826\n",
      "Iteration 40, loss = 0.00425584\n",
      "[CV 2/3; 22/36] START alpha=0.001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.001\n",
      "Iteration 28, loss = 0.00742266\n",
      "Iteration 1, loss = 0.61988064\n",
      "Iteration 30, loss = 0.00506916\n",
      "Iteration 6, loss = 0.08248593\n",
      "Iteration 26, loss = 0.00739863\n",
      "Iteration 76, loss = 0.01274841\n",
      "Iteration 67, loss = 0.01391604\n",
      "Iteration 21, loss = 0.10320195\n",
      "Iteration 80, loss = 0.01030676\n",
      "Iteration 41, loss = 0.00426875\n",
      "Iteration 29, loss = 0.00708572\n",
      "Iteration 29, loss = 0.00724194\n",
      "Iteration 7, loss = 0.11238915\n",
      "Iteration 31, loss = 0.00496216\n",
      "Iteration 27, loss = 0.00730328\n",
      "Iteration 77, loss = 0.01262507\n",
      "Iteration 2, loss = 0.30209656\n",
      "Iteration 68, loss = 0.01375699\n",
      "Iteration 81, loss = 0.01018377\n",
      "Iteration 22, loss = 0.10054376\n",
      "Iteration 42, loss = 0.00418829\n",
      "Iteration 30, loss = 0.00703610\n",
      "Iteration 30, loss = 0.00719407\n",
      "Iteration 1, loss = 0.62385156\n",
      "Iteration 28, loss = 0.00731617\n",
      "Iteration 32, loss = 0.00491080\n",
      "Iteration 8, loss = 0.10380840\n",
      "Iteration 78, loss = 0.01249365\n",
      "Iteration 69, loss = 0.01351627\n",
      "Iteration 82, loss = 0.01015138\n",
      "Iteration 3, loss = 0.13221019\n",
      "Iteration 43, loss = 0.00419891\n",
      "Iteration 31, loss = 0.00686642\n",
      "Iteration 23, loss = 0.09453224\n",
      "Iteration 31, loss = 0.00705079\n",
      "Iteration 29, loss = 0.00714400\n",
      "Iteration 79, loss = 0.01241601\n",
      "Iteration 33, loss = 0.00480760\n",
      "Iteration 9, loss = 0.09383237\n",
      "Iteration 2, loss = 0.31735848\n",
      "Iteration 70, loss = 0.01341270\n",
      "Iteration 83, loss = 0.01004381\n",
      "Iteration 32, loss = 0.00685257\n",
      "Iteration 44, loss = 0.00413650\n",
      "Iteration 24, loss = 0.10439742\n",
      "Iteration 32, loss = 0.00693090\n",
      "Iteration 30, loss = 0.00696266\n",
      "Iteration 80, loss = 0.01208613\n",
      "Iteration 4, loss = 0.06394232\n",
      "Iteration 10, loss = 0.09867155\n",
      "Iteration 34, loss = 0.00461448\n",
      "Iteration 71, loss = 0.01321283\n",
      "Iteration 84, loss = 0.00982311\n",
      "Iteration 33, loss = 0.00670204\n",
      "Iteration 45, loss = 0.00423230\n",
      "Iteration 3, loss = 0.13660286\n",
      "Iteration 25, loss = 0.10606989\n",
      "Iteration 33, loss = 0.00682649\n",
      "Iteration 31, loss = 0.00686372\n",
      "Iteration 81, loss = 0.01204501\n",
      "Iteration 35, loss = 0.00455071\n",
      "Iteration 11, loss = 0.09443227\n",
      "Iteration 5, loss = 0.03623251\n",
      "Iteration 85, loss = 0.00975640\n",
      "Iteration 72, loss = 0.01291778\n",
      "Iteration 46, loss = 0.00406210\n",
      "Iteration 34, loss = 0.00667218\n",
      "Iteration 34, loss = 0.00679160\n",
      "Iteration 32, loss = 0.00690639\n",
      "Iteration 26, loss = 0.10616871\n",
      "Iteration 82, loss = 0.01179220\n",
      "Iteration 4, loss = 0.07215677\n",
      "Iteration 12, loss = 0.08746566\n",
      "Iteration 36, loss = 0.00447485\n",
      "Iteration 86, loss = 0.00962924\n",
      "Iteration 73, loss = 0.01279459\n",
      "Iteration 47, loss = 0.00401115\n",
      "Iteration 35, loss = 0.00654457\n",
      "Iteration 35, loss = 0.00666077\n",
      "Iteration 33, loss = 0.00680898\n",
      "Iteration 6, loss = 0.02407534\n",
      "Iteration 27, loss = 0.10580879\n",
      "Iteration 83, loss = 0.01175507\n",
      "Iteration 13, loss = 0.09896442\n",
      "Iteration 37, loss = 0.00442164\n",
      "Iteration 87, loss = 0.00951279\n",
      "Iteration 74, loss = 0.01258534\n",
      "Iteration 5, loss = 0.04248658\n",
      "Iteration 48, loss = 0.00396901\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 36, loss = 0.00654175\n",
      "[CV 2/3; 19/36] END alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate_init=0.001;, score=0.894 total time= 2.5min\n",
      "Iteration 36, loss = 0.00662749\n",
      "Iteration 34, loss = 0.00657764\n",
      "[CV 3/3; 22/36] START alpha=0.001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.001\n",
      "Iteration 84, loss = 0.01170877\n",
      "Iteration 28, loss = 0.11610219\n",
      "Iteration 7, loss = 0.01495435\n",
      "Iteration 38, loss = 0.00439821\n",
      "Iteration 14, loss = 0.09734058\n",
      "Iteration 88, loss = 0.00942058\n",
      "Iteration 75, loss = 0.01250595\n",
      "Iteration 37, loss = 0.00651280\n",
      "Iteration 37, loss = 0.00649448\n",
      "Iteration 35, loss = 0.00667634\n",
      "Iteration 85, loss = 0.01146143\n",
      "Iteration 6, loss = 0.03157263\n",
      "Iteration 29, loss = 0.11469587\n",
      "Iteration 15, loss = 0.09423831\n",
      "Iteration 89, loss = 0.00934940\n",
      "Iteration 39, loss = 0.00430042\n",
      "Iteration 76, loss = 0.01228158\n",
      "Iteration 38, loss = 0.00625908\n",
      "Iteration 8, loss = 0.00963330\n",
      "Iteration 38, loss = 0.00636879\n",
      "Iteration 36, loss = 0.00647064\n",
      "Iteration 86, loss = 0.01138540\n",
      "Iteration 30, loss = 0.10456251\n",
      "Iteration 1, loss = 0.62417115\n",
      "Iteration 90, loss = 0.00921396\n",
      "Iteration 7, loss = 0.02132802\n",
      "Iteration 16, loss = 0.09910747\n",
      "Iteration 40, loss = 0.00430928\n",
      "Iteration 77, loss = 0.01207619\n",
      "Iteration 39, loss = 0.00625413\n",
      "Iteration 37, loss = 0.00641386\n",
      "Iteration 87, loss = 0.01120784\n",
      "Iteration 39, loss = 0.00633933\n",
      "Iteration 31, loss = 0.11571340\n",
      "Iteration 9, loss = 0.00720611\n",
      "Iteration 91, loss = 0.00912409\n",
      "Iteration 41, loss = 0.00446474\n",
      "Iteration 17, loss = 0.09238176\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 2, loss = 0.31833515\n",
      "[CV 3/3; 21/36] END alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate_init=0.1;, score=0.887 total time=  56.6s\n",
      "Iteration 78, loss = 0.01191545\n",
      "[CV 1/3; 23/36] START alpha=0.001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.01\n",
      "Iteration 40, loss = 0.00607235\n",
      "Iteration 8, loss = 0.01705673\n",
      "Iteration 88, loss = 0.01108962\n",
      "Iteration 38, loss = 0.00645067\n",
      "Iteration 40, loss = 0.00629450\n",
      "Iteration 32, loss = 0.10835835\n",
      "Iteration 92, loss = 0.00910015\n",
      "Iteration 42, loss = 0.00422413\n",
      "Iteration 10, loss = 0.00581992\n",
      "Iteration 41, loss = 0.00610959\n",
      "Iteration 79, loss = 0.01185251\n",
      "Iteration 89, loss = 0.01094702\n",
      "Iteration 39, loss = 0.00625434\n",
      "Iteration 41, loss = 0.00630275\n",
      "Iteration 3, loss = 0.14075328\n",
      "Iteration 9, loss = 0.01298137\n",
      "Iteration 33, loss = 0.09478098\n",
      "Iteration 93, loss = 0.00899131\n",
      "Iteration 43, loss = 0.00418616\n",
      "Iteration 42, loss = 0.00605700\n",
      "Iteration 90, loss = 0.01085943\n",
      "Iteration 80, loss = 0.01164295\n",
      "Iteration 40, loss = 0.00630256\n",
      "Iteration 42, loss = 0.00869854\n",
      "Iteration 11, loss = 0.00503563\n",
      "Iteration 1, loss = 0.42676476\n",
      "Iteration 94, loss = 0.00890815\n",
      "Iteration 34, loss = 0.09251933\n",
      "Iteration 4, loss = 0.06626912\n",
      "Iteration 10, loss = 0.00968301\n",
      "Iteration 43, loss = 0.00600176\n",
      "Iteration 44, loss = 0.00427355\n",
      "Iteration 91, loss = 0.01071347\n",
      "Iteration 41, loss = 0.00630090\n",
      "Iteration 81, loss = 0.01155087\n",
      "Iteration 43, loss = 0.11806682\n",
      "Iteration 95, loss = 0.00883799\n",
      "Iteration 35, loss = 0.09246343\n",
      "Iteration 12, loss = 0.00463488\n",
      "Iteration 2, loss = 0.11937698\n",
      "Iteration 44, loss = 0.00586625\n",
      "Iteration 45, loss = 0.00450704\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 92, loss = 0.01060975\n",
      "Iteration 42, loss = 0.00604526\n",
      "Iteration 5, loss = 0.03682731\n",
      "[CV 3/3; 19/36] END alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate_init=0.001;, score=0.894 total time= 2.4min\n",
      "Iteration 44, loss = 0.08471235\n",
      "Iteration 82, loss = 0.01138093\n",
      "[CV 2/3; 23/36] START alpha=0.001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.01\n",
      "Iteration 11, loss = 0.00753261\n",
      "Iteration 96, loss = 0.00881918\n",
      "Iteration 36, loss = 0.09319993\n",
      "Iteration 45, loss = 0.00584791\n",
      "Iteration 93, loss = 0.01056588\n",
      "Iteration 43, loss = 0.00612968\n",
      "Iteration 13, loss = 0.00426818\n",
      "Iteration 3, loss = 0.05239909\n",
      "Iteration 45, loss = 0.02776363\n",
      "Iteration 83, loss = 0.01125275\n",
      "Iteration 97, loss = 0.00871423\n",
      "Iteration 6, loss = 0.02279388\n",
      "Iteration 37, loss = 0.11275953\n",
      "Iteration 12, loss = 0.00623046\n",
      "Iteration 46, loss = 0.00579188\n",
      "Iteration 94, loss = 0.01048645\n",
      "Iteration 44, loss = 0.00618595\n",
      "Iteration 46, loss = 0.01780087\n",
      "Iteration 84, loss = 0.01106389\n",
      "Iteration 1, loss = 0.40830703\n",
      "Iteration 98, loss = 0.00859210\n",
      "Iteration 4, loss = 0.02824710\n",
      "Iteration 14, loss = 0.00402913\n",
      "Iteration 38, loss = 0.10843649\n",
      "Iteration 47, loss = 0.00567050\n",
      "Iteration 95, loss = 0.01031078\n",
      "Iteration 7, loss = 0.01554533\n",
      "Iteration 45, loss = 0.00642204\n",
      "Iteration 47, loss = 0.01660118\n",
      "Iteration 13, loss = 0.00547403\n",
      "Iteration 85, loss = 0.01101566\n",
      "Iteration 99, loss = 0.00852215\n",
      "Iteration 2, loss = 0.12329127\n",
      "Iteration 39, loss = 0.10155149\n",
      "Iteration 48, loss = 0.00559418\n",
      "Iteration 96, loss = 0.01031867\n",
      "Iteration 5, loss = 0.01926618Iteration 15, loss = 0.00394590\n",
      "\n",
      "Iteration 46, loss = 0.00610834\n",
      "Iteration 48, loss = 0.01514797\n",
      "Iteration 86, loss = 0.01090483\n",
      "Iteration 8, loss = 0.01111643\n",
      "Iteration 100, loss = 0.00847004\n",
      "Iteration 14, loss = 0.00504161\n",
      "Iteration 97, loss = 0.01014484\n",
      "Iteration 49, loss = 0.00549792\n",
      "Iteration 40, loss = 0.09139021\n",
      "Iteration 47, loss = 0.00615068\n",
      "Iteration 49, loss = 0.01423837\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 0.05147651\n",
      "[CV 2/3; 20/36] END alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate_init=0.01;, score=0.904 total time= 2.5min\n",
      "Iteration 87, loss = 0.01073474\n",
      "Iteration 16, loss = 0.00386189\n",
      "Iteration 6, loss = 0.02154932\n",
      "Iteration 101, loss = 0.00839846\n",
      "[CV 3/3; 23/36] START alpha=0.001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.01\n",
      "Iteration 9, loss = 0.00848731\n",
      "Iteration 98, loss = 0.01007100\n",
      "Iteration 50, loss = 0.00549680\n",
      "Iteration 41, loss = 0.09435553\n",
      "Iteration 48, loss = 0.05008103\n",
      "Iteration 15, loss = 0.00471847\n",
      "Iteration 102, loss = 0.00833535\n",
      "Iteration 88, loss = 0.01059944\n",
      "Iteration 4, loss = 0.02903467\n",
      "Iteration 7, loss = 0.02494052\n",
      "Iteration 17, loss = 0.00375601\n",
      "Iteration 99, loss = 0.00997110\n",
      "Iteration 51, loss = 0.00548314\n",
      "Iteration 49, loss = 0.17247062\n",
      "Iteration 42, loss = 0.09437970\n",
      "Iteration 10, loss = 0.00712823\n",
      "Iteration 103, loss = 0.00831455\n",
      "Iteration 89, loss = 0.01049098\n",
      "Iteration 16, loss = 0.00443413\n",
      "Iteration 1, loss = 0.41619537\n",
      "Iteration 100, loss = 0.00991880\n",
      "Iteration 52, loss = 0.00542664\n",
      "Iteration 5, loss = 0.02059260\n",
      "Iteration 50, loss = 0.05635058\n",
      "Iteration 43, loss = 0.09564748\n",
      "Iteration 18, loss = 0.00382337\n",
      "Iteration 8, loss = 0.01972755\n",
      "Iteration 104, loss = 0.00823683\n",
      "Iteration 90, loss = 0.01037370\n",
      "Iteration 11, loss = 0.00602133\n",
      "Iteration 101, loss = 0.00979625\n",
      "Iteration 53, loss = 0.00553261\n",
      "Iteration 17, loss = 0.00443887\n",
      "Iteration 51, loss = 0.02321178\n",
      "Iteration 2, loss = 0.13210459\n",
      "Iteration 44, loss = 0.09019612\n",
      "Iteration 105, loss = 0.00818179\n",
      "Iteration 91, loss = 0.01024077\n",
      "Iteration 6, loss = 0.01635627\n",
      "Iteration 9, loss = 0.01455707\n",
      "Iteration 19, loss = 0.00385733\n",
      "Iteration 102, loss = 0.00968558\n",
      "Iteration 54, loss = 0.00529918\n",
      "Iteration 52, loss = 0.01703314\n",
      "Iteration 12, loss = 0.00563402\n",
      "Iteration 45, loss = 0.10839484\n",
      "Iteration 106, loss = 0.00810109\n",
      "Iteration 18, loss = 0.00428578\n",
      "Iteration 3, loss = 0.04938629\n",
      "Iteration 92, loss = 0.01016263\n",
      "Iteration 103, loss = 0.00960997\n",
      "Iteration 55, loss = 0.00522616\n",
      "Iteration 7, loss = 0.01232334\n",
      "Iteration 53, loss = 0.01600373\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 20, loss = 0.00374940\n",
      "Iteration 10, loss = 0.01481977\n",
      "[CV 3/3; 20/36] END alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate_init=0.01;, score=0.910 total time= 2.7min\n",
      "Iteration 107, loss = 0.00804169\n",
      "Iteration 46, loss = 0.09678728\n",
      "[CV 1/3; 24/36] START alpha=0.001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.1\n",
      "Iteration 13, loss = 0.00491169\n",
      "Iteration 93, loss = 0.01007300\n",
      "Iteration 104, loss = 0.00963451\n",
      "Iteration 19, loss = 0.00416818\n",
      "Iteration 56, loss = 0.00523513\n",
      "Iteration 4, loss = 0.02980281\n",
      "Iteration 108, loss = 0.00800859\n",
      "Iteration 47, loss = 0.09373598\n",
      "Iteration 8, loss = 0.01007033\n",
      "Iteration 11, loss = 0.01108051\n",
      "Iteration 21, loss = 0.00366480\n",
      "Iteration 94, loss = 0.00995237\n",
      "Iteration 105, loss = 0.00947395\n",
      "Iteration 57, loss = 0.00514316\n",
      "Iteration 14, loss = 0.00478509\n",
      "Iteration 109, loss = 0.00792519\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 20, loss = 0.00396315\n",
      "Iteration 5, loss = 0.02054258\n",
      "Iteration 48, loss = 0.08557745\n",
      "[CV 1/3; 16/36] END alpha=0.001, hidden_layer_sizes=(100,), learning_rate_init=0.001;, score=0.898 total time= 5.3min\n",
      "[CV 2/3; 24/36] START alpha=0.001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.1\n",
      "Iteration 1, loss = 1.27384867\n",
      "Iteration 95, loss = 0.00988209\n",
      "Iteration 106, loss = 0.00936119\n",
      "Iteration 9, loss = 0.01028229\n",
      "Iteration 58, loss = 0.00513157\n",
      "Iteration 22, loss = 0.00357262\n",
      "Iteration 12, loss = 0.01037645\n",
      "Iteration 15, loss = 0.00453226\n",
      "Iteration 49, loss = 0.09438255\n",
      "Iteration 96, loss = 0.00977181\n",
      "Iteration 21, loss = 0.00387797\n",
      "Iteration 107, loss = 0.00932376\n",
      "Iteration 6, loss = 0.03454427\n",
      "Iteration 59, loss = 0.00507030\n",
      "Iteration 2, loss = 0.32488811\n",
      "Iteration 10, loss = 0.01857813\n",
      "Iteration 13, loss = 0.02064043\n",
      "Iteration 23, loss = 0.00353915\n",
      "Iteration 50, loss = 0.09120531\n",
      "Iteration 108, loss = 0.00933165\n",
      "Iteration 1, loss = 1.12456129\n",
      "Iteration 97, loss = 0.00968758\n",
      "Iteration 16, loss = 0.00460217\n",
      "Iteration 60, loss = 0.00505330\n",
      "Iteration 22, loss = 0.00385691\n",
      "Iteration 7, loss = 0.03931557\n",
      "Iteration 3, loss = 0.18554365\n",
      "Iteration 51, loss = 0.09410739\n",
      "Iteration 109, loss = 0.00918364\n",
      "Iteration 11, loss = 0.02263194\n",
      "Iteration 98, loss = 0.00959971\n",
      "Iteration 14, loss = 0.02298306\n",
      "Iteration 61, loss = 0.00492934\n",
      "Iteration 24, loss = 0.00388335\n",
      "Iteration 2, loss = 0.25788527\n",
      "Iteration 17, loss = 0.00441370\n",
      "Iteration 52, loss = 0.10544757\n",
      "Iteration 23, loss = 0.00389932\n",
      "Iteration 110, loss = 0.00906703\n",
      "Iteration 8, loss = 0.02828221\n",
      "Iteration 99, loss = 0.00952105\n",
      "Iteration 4, loss = 0.15134688\n",
      "Iteration 62, loss = 0.00494098\n",
      "Iteration 12, loss = 0.02197443\n",
      "Iteration 15, loss = 0.02187468\n",
      "Iteration 25, loss = 0.00365217\n",
      "Iteration 3, loss = 0.17116571\n",
      "Iteration 111, loss = 0.00913989\n",
      "Iteration 18, loss = 0.00410685\n",
      "Iteration 53, loss = 0.10822491\n",
      "Iteration 100, loss = 0.00944110\n",
      "Iteration 63, loss = 0.00490832\n",
      "Iteration 24, loss = 0.00390772\n",
      "Iteration 9, loss = 0.01821418\n",
      "Iteration 5, loss = 0.13942752\n",
      "Iteration 13, loss = 0.02387982\n",
      "Iteration 112, loss = 0.00893171\n",
      "Iteration 54, loss = 0.11253307\n",
      "Iteration 16, loss = 0.01586989\n",
      "Iteration 26, loss = 0.00394120\n",
      "Iteration 64, loss = 0.00481297\n",
      "Iteration 101, loss = 0.00932146\n",
      "Iteration 4, loss = 0.13113168\n",
      "Iteration 19, loss = 0.00404426\n",
      "Iteration 25, loss = 0.00373166\n",
      "Iteration 113, loss = 0.00885552\n",
      "Iteration 10, loss = 0.01360474\n",
      "Iteration 6, loss = 0.12281023\n",
      "Iteration 55, loss = 0.11290387\n",
      "Iteration 65, loss = 0.00477360\n",
      "Iteration 102, loss = 0.00925199\n",
      "Iteration 14, loss = 0.02115208\n",
      "Iteration 17, loss = 0.01225068\n",
      "Iteration 27, loss = 0.00374054\n",
      "Iteration 5, loss = 0.12097001\n",
      "Iteration 20, loss = 0.00413169\n",
      "Iteration 114, loss = 0.00884504\n",
      "Iteration 66, loss = 0.00500499\n",
      "Iteration 56, loss = 0.13084932\n",
      "Iteration 26, loss = 0.00365706\n",
      "Iteration 103, loss = 0.00921023\n",
      "Iteration 11, loss = 0.01133130\n",
      "Iteration 7, loss = 0.10230890\n",
      "Iteration 15, loss = 0.02808350\n",
      "Iteration 115, loss = 0.00875948\n",
      "Iteration 18, loss = 0.01148323\n",
      "Iteration 28, loss = 0.00385979\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 67, loss = 0.00494745\n",
      "Iteration 6, loss = 0.12004189\n",
      "[CV 1/3; 22/36] END alpha=0.001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.001;, score=0.901 total time= 2.1min\n",
      "Iteration 57, loss = 0.12519942\n",
      "Iteration 104, loss = 0.00913919\n",
      "[CV 3/3; 24/36] START alpha=0.001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.1\n",
      "Iteration 21, loss = 0.00408281\n",
      "Iteration 27, loss = 0.00365663\n",
      "Iteration 12, loss = 0.00995187\n",
      "Iteration 116, loss = 0.00870042\n",
      "Iteration 8, loss = 0.10682740\n",
      "Iteration 16, loss = 0.03262865\n",
      "Iteration 68, loss = 0.00549067\n",
      "Iteration 58, loss = 0.12144691\n",
      "Iteration 105, loss = 0.00905723\n",
      "Iteration 19, loss = 0.01265174\n",
      "Iteration 7, loss = 0.12218464\n",
      "Iteration 22, loss = 0.00419650\n",
      "Iteration 117, loss = 0.00861588\n",
      "Iteration 28, loss = 0.00376688\n",
      "Iteration 69, loss = 0.29587315\n",
      "Iteration 13, loss = 0.00906279\n",
      "Iteration 9, loss = 0.10983686\n",
      "Iteration 59, loss = 0.11360388\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.01294422\n",
      "Iteration 106, loss = 0.00896049\n",
      "[CV 1/3; 21/36] END alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate_init=0.1;, score=0.871 total time= 3.2min\n",
      "Iteration 17, loss = 0.02367006\n",
      "[CV 1/3; 25/36] START alpha=0.01, hidden_layer_sizes=(50,), learning_rate_init=0.001\n",
      "Iteration 118, loss = 0.00857102\n",
      "Iteration 20, loss = 0.01440792\n",
      "Iteration 8, loss = 0.12460110\n",
      "Iteration 70, loss = 0.11581991\n",
      "Iteration 23, loss = 0.00388296\n",
      "Iteration 107, loss = 0.00888532\n",
      "Iteration 29, loss = 0.00366638\n",
      "Iteration 14, loss = 0.00893387\n",
      "Iteration 119, loss = 0.00855417\n",
      "Iteration 10, loss = 0.09905279\n",
      "Iteration 2, loss = 0.25274297\n",
      "Iteration 1, loss = 0.65315617\n",
      "Iteration 18, loss = 0.01919919\n",
      "Iteration 71, loss = 0.04294294\n",
      "Iteration 21, loss = 0.01652983\n",
      "Iteration 108, loss = 0.00887406\n",
      "Iteration 9, loss = 0.12677558\n",
      "Iteration 2, loss = 0.52843406\n",
      "Iteration 24, loss = 0.00386947\n",
      "Iteration 120, loss = 0.00850538\n",
      "Iteration 30, loss = 0.00356638\n",
      "Iteration 72, loss = 0.02104004\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 0.00857402\n",
      "Iteration 3, loss = 0.41803459\n",
      "Iteration 11, loss = 0.09556496\n",
      "Iteration 3, loss = 0.16315278\n",
      "[CV 1/3; 20/36] END alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate_init=0.01;, score=0.899 total time= 3.7min\n",
      "[CV 2/3; 25/36] START alpha=0.01, hidden_layer_sizes=(50,), learning_rate_init=0.001\n",
      "Iteration 19, loss = 0.01364205\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 109, loss = 0.00874578\n",
      "Iteration 121, loss = 0.00847460\n",
      "[CV 2/3; 23/36] END alpha=0.001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.01;, score=0.891 total time= 1.4min\n",
      "Iteration 22, loss = 0.01689625\n",
      "Iteration 4, loss = 0.33943489\n",
      "[CV 3/3; 25/36] START alpha=0.01, hidden_layer_sizes=(50,), learning_rate_init=0.001\n",
      "Iteration 10, loss = 0.10929454\n",
      "Iteration 25, loss = 0.00415607\n",
      "Iteration 5, loss = 0.28344888\n",
      "Iteration 31, loss = 0.00354508\n",
      "Iteration 16, loss = 0.00864501\n",
      "Iteration 110, loss = 0.00867296\n",
      "Iteration 4, loss = 0.11841105\n",
      "Iteration 122, loss = 0.00845003\n",
      "Iteration 12, loss = 0.09825715\n",
      "Iteration 1, loss = 0.65307384\n",
      "Iteration 1, loss = 0.65348503\n",
      "Iteration 6, loss = 0.24156662\n",
      "Iteration 23, loss = 0.01333866\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 2, loss = 0.52883803\n",
      "[CV 1/3; 23/36] END alpha=0.001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.01;, score=0.894 total time= 1.7min\n",
      "Iteration 11, loss = 0.09437231\n",
      "[CV 1/3; 26/36] START alpha=0.01, hidden_layer_sizes=(50,), learning_rate_init=0.01\n",
      "Iteration 26, loss = 0.00408625\n",
      "Iteration 7, loss = 0.20972384\n",
      "Iteration 111, loss = 0.00857986\n",
      "Iteration 2, loss = 0.53008306\n",
      "Iteration 123, loss = 0.00835796\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/3; 16/36] END alpha=0.001, hidden_layer_sizes=(100,), learning_rate_init=0.001;, score=0.892 total time= 6.1min\n",
      "[CV 2/3; 26/36] START alpha=0.01, hidden_layer_sizes=(50,), learning_rate_init=0.01\n",
      "Iteration 32, loss = 0.00361188\n",
      "Iteration 3, loss = 0.41897877\n",
      "Iteration 17, loss = 0.00824544\n",
      "Iteration 5, loss = 0.11290030\n",
      "Iteration 13, loss = 0.10148208\n",
      "Iteration 8, loss = 0.18464280\n",
      "Iteration 3, loss = 0.42084454\n",
      "Iteration 4, loss = 0.34093006\n",
      "Iteration 112, loss = 0.00856715\n",
      "Iteration 1, loss = 0.44985468\n",
      "Iteration 9, loss = 0.16461963\n",
      "Iteration 12, loss = 0.09024281\n",
      "Iteration 4, loss = 0.34251719\n",
      "Iteration 27, loss = 0.00399967\n",
      "Iteration 1, loss = 0.44669335\n",
      "Iteration 5, loss = 0.28564557\n",
      "Iteration 2, loss = 0.17831123\n",
      "Iteration 33, loss = 0.00350033\n",
      "Iteration 18, loss = 0.00797208\n",
      "Iteration 10, loss = 0.14942608\n",
      "Iteration 6, loss = 0.11085332\n",
      "Iteration 14, loss = 0.11098565\n",
      "Iteration 5, loss = 0.28724239\n",
      "Iteration 2, loss = 0.17744469\n",
      "Iteration 113, loss = 0.00848157\n",
      "Iteration 6, loss = 0.24358405\n",
      "Iteration 3, loss = 0.10566014\n",
      "Iteration 11, loss = 0.13568961\n",
      "Iteration 6, loss = 0.24535112\n",
      "Iteration 3, loss = 0.10950896\n",
      "Iteration 13, loss = 0.08666685\n",
      "Iteration 7, loss = 0.21209923\n",
      "Iteration 28, loss = 0.00409444\n",
      "Iteration 12, loss = 0.12466096\n",
      "Iteration 4, loss = 0.07765334\n",
      "Iteration 114, loss = 0.00842529\n",
      "Iteration 34, loss = 0.00378918\n",
      "Iteration 7, loss = 0.21341925\n",
      "Iteration 19, loss = 0.00782531\n",
      "Iteration 4, loss = 0.08584402\n",
      "Iteration 8, loss = 0.18689124\n",
      "Iteration 7, loss = 0.10621652\n",
      "Iteration 15, loss = 0.10485633\n",
      "Iteration 13, loss = 0.11508253\n",
      "Iteration 5, loss = 0.06535155\n",
      "Iteration 8, loss = 0.18889408\n",
      "Iteration 5, loss = 0.07106269\n",
      "Iteration 9, loss = 0.16681656\n",
      "Iteration 14, loss = 0.09660510\n",
      "Iteration 115, loss = 0.00838992\n",
      "Iteration 14, loss = 0.10786445\n",
      "Iteration 29, loss = 0.00432717\n",
      "Iteration 6, loss = 0.05809169\n",
      "Iteration 6, loss = 0.06284660\n",
      "Iteration 9, loss = 0.16837088\n",
      "Iteration 10, loss = 0.15066381\n",
      "Iteration 35, loss = 0.00358796\n",
      "Iteration 20, loss = 0.00770316\n",
      "Iteration 8, loss = 0.09239160\n",
      "Iteration 15, loss = 0.10076422\n",
      "Iteration 16, loss = 0.09781180\n",
      "Iteration 7, loss = 0.05201962\n",
      "Iteration 7, loss = 0.06107937\n",
      "Iteration 10, loss = 0.15211706\n",
      "Iteration 11, loss = 0.13726994\n",
      "Iteration 116, loss = 0.00830385\n",
      "Iteration 16, loss = 0.09483131\n",
      "Iteration 15, loss = 0.09355171\n",
      "Iteration 8, loss = 0.04765051\n",
      "Iteration 8, loss = 0.05563703\n",
      "Iteration 30, loss = 0.00403678\n",
      "Iteration 11, loss = 0.13832595\n",
      "Iteration 12, loss = 0.12667153\n",
      "Iteration 17, loss = 0.09018050\n",
      "Iteration 36, loss = 0.00355196\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 21, loss = 0.00759000\n",
      "Iteration 9, loss = 0.04420319\n",
      "Iteration 9, loss = 0.08447107\n",
      "Iteration 17, loss = 0.10179671\n",
      "Iteration 117, loss = 0.00823195\n",
      "[CV 2/3; 22/36] END alpha=0.001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.001;, score=0.898 total time= 2.7min\n",
      "Iteration 9, loss = 0.05490673\n",
      "[CV 3/3; 26/36] START alpha=0.01, hidden_layer_sizes=(50,), learning_rate_init=0.01\n",
      "Iteration 12, loss = 0.12707254\n",
      "Iteration 13, loss = 0.11702628\n",
      "Iteration 18, loss = 0.08601347\n",
      "Iteration 10, loss = 0.04251320\n",
      "Iteration 10, loss = 0.05734123\n",
      "Iteration 14, loss = 0.10962577\n",
      "Iteration 13, loss = 0.11756627\n",
      "Iteration 16, loss = 0.09273302\n",
      "Iteration 31, loss = 0.00424332\n",
      "Iteration 19, loss = 0.08187206\n",
      "Iteration 118, loss = 0.00817698\n",
      "Iteration 11, loss = 0.04096215\n",
      "Iteration 11, loss = 0.05646106\n",
      "Iteration 22, loss = 0.00754802\n",
      "Iteration 15, loss = 0.10306676\n",
      "Iteration 1, loss = 0.45637587\n",
      "Iteration 14, loss = 0.10986277\n",
      "Iteration 10, loss = 0.10861745\n",
      "Iteration 18, loss = 0.08884932\n",
      "Iteration 20, loss = 0.07810794\n",
      "Iteration 12, loss = 0.05555493\n",
      "Iteration 12, loss = 0.04227281\n",
      "Iteration 16, loss = 0.09813022\n",
      "Iteration 2, loss = 0.18872728\n",
      "Iteration 15, loss = 0.10282217\n",
      "Iteration 119, loss = 0.00820015\n",
      "Iteration 17, loss = 0.10181334\n",
      "Iteration 21, loss = 0.07519792\n",
      "Iteration 32, loss = 0.00386673\n",
      "Iteration 13, loss = 0.05381394\n",
      "Iteration 13, loss = 0.05261146\n",
      "Iteration 17, loss = 0.09251226\n",
      "Iteration 3, loss = 0.11147823\n",
      "Iteration 16, loss = 0.09690455\n",
      "Iteration 23, loss = 0.00725712\n",
      "Iteration 22, loss = 0.07217319\n",
      "Iteration 19, loss = 0.08776451\n",
      "Iteration 11, loss = 0.11246220\n",
      "Iteration 120, loss = 0.00814075\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 0.05375038\n",
      "Iteration 14, loss = 0.08360895\n",
      "Iteration 18, loss = 0.08785100\n",
      "[CV 3/3; 16/36] END alpha=0.001, hidden_layer_sizes=(100,), learning_rate_init=0.001;, score=0.896 total time= 6.3min\n",
      "Iteration 4, loss = 0.08436104\n",
      "Iteration 17, loss = 0.09151456\n",
      "[CV 1/3; 27/36] START alpha=0.01, hidden_layer_sizes=(50,), learning_rate_init=0.1\n",
      "Iteration 23, loss = 0.06990825\n",
      "Iteration 18, loss = 0.10727684\n",
      "Iteration 15, loss = 0.05262241\n",
      "Iteration 33, loss = 0.00390040\n",
      "Iteration 15, loss = 0.10515345\n",
      "Iteration 19, loss = 0.08390903\n",
      "Iteration 5, loss = 0.06906210\n",
      "Iteration 18, loss = 0.08710593\n",
      "Iteration 24, loss = 0.06761206\n",
      "Iteration 24, loss = 0.00714116\n",
      "Iteration 20, loss = 0.09965074\n",
      "Iteration 12, loss = 0.09640873\n",
      "Iteration 16, loss = 0.05934197\n",
      "Iteration 20, loss = 0.08050361\n",
      "Iteration 16, loss = 0.09199315\n",
      "Iteration 1, loss = 0.54041789\n",
      "Iteration 6, loss = 0.05918717\n",
      "Iteration 19, loss = 0.08305236\n",
      "Iteration 25, loss = 0.06553432\n",
      "Iteration 17, loss = 0.06994342\n",
      "Iteration 21, loss = 0.07752989\n",
      "Iteration 19, loss = 0.11028267\n",
      "Iteration 17, loss = 0.07052786\n",
      "Iteration 2, loss = 0.31948862\n",
      "Iteration 34, loss = 0.00369042\n",
      "Iteration 7, loss = 0.05440047\n",
      "Iteration 20, loss = 0.07952836\n",
      "Iteration 26, loss = 0.06370814\n",
      "Iteration 18, loss = 0.07049949\n",
      "Iteration 25, loss = 0.00709558\n",
      "Iteration 22, loss = 0.07491664\n",
      "Iteration 21, loss = 0.10589438\n",
      "Iteration 13, loss = 0.08531891\n",
      "Iteration 18, loss = 0.06049787\n",
      "Iteration 3, loss = 0.23886668\n",
      "Iteration 21, loss = 0.07610612\n",
      "Iteration 8, loss = 0.05011569\n",
      "Iteration 27, loss = 0.06195705\n",
      "Iteration 19, loss = 0.06917668\n",
      "Iteration 23, loss = 0.07244069\n",
      "Iteration 4, loss = 0.26836692\n",
      "Iteration 19, loss = 0.05166529\n",
      "Iteration 20, loss = 0.09130678\n",
      "Iteration 28, loss = 0.06034232\n",
      "Iteration 22, loss = 0.07343797\n",
      "Iteration 9, loss = 0.04769834\n",
      "Iteration 35, loss = 0.00414343\n",
      "Iteration 20, loss = 0.06624212\n",
      "Iteration 24, loss = 0.07033948\n",
      "Iteration 5, loss = 0.28889870\n",
      "Iteration 26, loss = 0.00716853\n",
      "Iteration 20, loss = 0.04609899\n",
      "Iteration 29, loss = 0.05909708\n",
      "Iteration 22, loss = 0.13985937\n",
      "Iteration 14, loss = 0.10920280\n",
      "Iteration 23, loss = 0.07110050\n",
      "Iteration 10, loss = 0.04990526\n",
      "Iteration 21, loss = 0.05624260\n",
      "Iteration 25, loss = 0.06820932\n",
      "Iteration 6, loss = 0.26189108\n",
      "Iteration 21, loss = 0.04232164\n",
      "Iteration 30, loss = 0.05759955\n",
      "Iteration 24, loss = 0.06909868\n",
      "Iteration 11, loss = 0.05644271\n",
      "Iteration 21, loss = 0.07815281\n",
      "Iteration 22, loss = 0.04944882\n",
      "Iteration 36, loss = 0.00379209\n",
      "Iteration 26, loss = 0.06641436\n",
      "Iteration 7, loss = 0.23669730\n",
      "Iteration 31, loss = 0.05651215\n",
      "Iteration 22, loss = 0.03805177\n",
      "Iteration 25, loss = 0.06661401\n",
      "Iteration 27, loss = 0.00682841\n",
      "Iteration 12, loss = 0.06142099\n",
      "Iteration 23, loss = 0.04293460\n",
      "Iteration 15, loss = 0.11490245\n",
      "Iteration 23, loss = 0.10537648\n",
      "Iteration 27, loss = 0.06483156\n",
      "Iteration 8, loss = 0.26487516\n",
      "Iteration 32, loss = 0.05526804\n",
      "Iteration 23, loss = 0.03481502\n",
      "Iteration 26, loss = 0.06466780\n",
      "Iteration 24, loss = 0.03889651\n",
      "Iteration 13, loss = 0.06877240\n",
      "Iteration 28, loss = 0.06352401\n",
      "Iteration 22, loss = 0.08489482\n",
      "Iteration 9, loss = 0.27116431\n",
      "Iteration 37, loss = 0.00413817\n",
      "Iteration 33, loss = 0.05422927\n",
      "Iteration 24, loss = 0.03201971\n",
      "Iteration 27, loss = 0.06302376\n",
      "Iteration 14, loss = 0.06211485\n",
      "Iteration 25, loss = 0.03484226\n",
      "Iteration 29, loss = 0.06213384\n",
      "Iteration 28, loss = 0.00673553\n",
      "Iteration 10, loss = 0.26475501\n",
      "Iteration 16, loss = 0.09832447\n",
      "Iteration 24, loss = 0.08462374\n",
      "Iteration 34, loss = 0.05299304\n",
      "Iteration 25, loss = 0.02977674\n",
      "Iteration 28, loss = 0.06122954\n",
      "Iteration 26, loss = 0.03199438\n",
      "Iteration 15, loss = 0.05795154\n",
      "Iteration 30, loss = 0.06068455\n",
      "Iteration 11, loss = 0.27643827\n",
      "Iteration 35, loss = 0.05205231\n",
      "Iteration 23, loss = 0.07391512\n",
      "Iteration 26, loss = 0.02790258\n",
      "Iteration 38, loss = 0.00369688\n",
      "Iteration 27, loss = 0.02971792\n",
      "Iteration 29, loss = 0.05981625\n",
      "Iteration 16, loss = 0.05317777\n",
      "Iteration 31, loss = 0.05944354\n",
      "Iteration 12, loss = 0.26208439\n",
      "Iteration 29, loss = 0.00675630\n",
      "Iteration 36, loss = 0.05133542\n",
      "Iteration 17, loss = 0.09611229\n",
      "Iteration 25, loss = 0.07429738\n",
      "Iteration 27, loss = 0.02635979\n",
      "Iteration 30, loss = 0.05847380\n",
      "Iteration 32, loss = 0.05859042\n",
      "Iteration 28, loss = 0.02800660\n",
      "Iteration 17, loss = 0.04919424\n",
      "Iteration 13, loss = 0.22996117\n",
      "Iteration 37, loss = 0.05045669\n",
      "Iteration 28, loss = 0.02515701\n",
      "Iteration 24, loss = 0.07491420\n",
      "Iteration 33, loss = 0.05750593\n",
      "Iteration 29, loss = 0.02677747\n",
      "Iteration 31, loss = 0.05748429\n",
      "Iteration 39, loss = 0.00375831\n",
      "Iteration 18, loss = 0.04365844\n",
      "Iteration 14, loss = 0.20994541\n",
      "Iteration 38, loss = 0.04983672\n",
      "Iteration 30, loss = 0.00654891\n",
      "Iteration 29, loss = 0.02425179\n",
      "Iteration 34, loss = 0.05623048\n",
      "Iteration 18, loss = 0.11233716\n",
      "Iteration 26, loss = 0.06703322\n",
      "Iteration 30, loss = 0.02560050\n",
      "Iteration 32, loss = 0.05612929\n",
      "Iteration 19, loss = 0.03832957\n",
      "Iteration 15, loss = 0.20092110\n",
      "Iteration 39, loss = 0.04902381\n",
      "Iteration 35, loss = 0.05583013\n",
      "Iteration 31, loss = 0.02458099\n",
      "Iteration 30, loss = 0.02346079\n",
      "Iteration 33, loss = 0.05498869\n",
      "Iteration 16, loss = 0.22802271\n",
      "Iteration 25, loss = 0.07771082\n",
      "Iteration 20, loss = 0.03490055\n",
      "Iteration 40, loss = 0.00373581\n",
      "Iteration 40, loss = 0.04811649\n",
      "Iteration 36, loss = 0.05463127\n",
      "Iteration 32, loss = 0.02372581\n",
      "Iteration 31, loss = 0.02274055\n",
      "Iteration 31, loss = 0.00650583\n",
      "Iteration 17, loss = 0.29415549\n",
      "Iteration 34, loss = 0.05393167\n",
      "Iteration 21, loss = 0.03159327\n",
      "Iteration 19, loss = 0.08782460\n",
      "Iteration 27, loss = 0.06022644\n",
      "Iteration 41, loss = 0.04736354\n",
      "Iteration 37, loss = 0.05358592\n",
      "Iteration 33, loss = 0.02295540\n",
      "Iteration 32, loss = 0.02188371\n",
      "Iteration 18, loss = 0.29326109\n",
      "Iteration 35, loss = 0.05307179\n",
      "Iteration 22, loss = 0.02908803\n",
      "Iteration 42, loss = 0.04695822\n",
      "Iteration 26, loss = 0.09541783\n",
      "Iteration 41, loss = 0.00364780\n",
      "Iteration 38, loss = 0.05286037\n",
      "Iteration 34, loss = 0.02251858\n",
      "Iteration 33, loss = 0.02131205\n",
      "Iteration 19, loss = 0.26887719\n",
      "Iteration 36, loss = 0.05229540\n",
      "Iteration 43, loss = 0.04608671\n",
      "Iteration 32, loss = 0.00633361\n",
      "Iteration 23, loss = 0.02742849\n",
      "Iteration 20, loss = 0.07848338\n",
      "Iteration 28, loss = 0.05521708\n",
      "Iteration 39, loss = 0.05236108\n",
      "Iteration 35, loss = 0.02213787\n",
      "Iteration 20, loss = 0.21932274\n",
      "Iteration 34, loss = 0.02062211\n",
      "Iteration 37, loss = 0.05146435\n",
      "Iteration 44, loss = 0.04538026\n",
      "Iteration 24, loss = 0.02607172\n",
      "Iteration 40, loss = 0.05170810\n",
      "Iteration 36, loss = 0.02157967\n",
      "Iteration 27, loss = 0.10934379\n",
      "Iteration 21, loss = 0.18687558\n",
      "Iteration 42, loss = 0.00378279\n",
      "Iteration 35, loss = 0.02012340\n",
      "Iteration 38, loss = 0.05070524\n",
      "Iteration 45, loss = 0.04512242\n",
      "Iteration 25, loss = 0.02588434\n",
      "Iteration 33, loss = 0.00626293\n",
      "Iteration 41, loss = 0.05084005\n",
      "Iteration 37, loss = 0.02105602\n",
      "Iteration 29, loss = 0.13051840\n",
      "Iteration 21, loss = 0.08755194\n",
      "Iteration 22, loss = 0.16763208\n",
      "Iteration 36, loss = 0.01992928\n",
      "Iteration 39, loss = 0.04990814\n",
      "Iteration 46, loss = 0.04441436\n",
      "Iteration 26, loss = 0.02557265\n",
      "Iteration 42, loss = 0.05015328\n",
      "Iteration 38, loss = 0.02077372\n",
      "Iteration 23, loss = 0.18905977\n",
      "Iteration 28, loss = 0.10023964\n",
      "Iteration 37, loss = 0.01973270\n",
      "Iteration 40, loss = 0.04913216\n",
      "Iteration 43, loss = 0.00364637\n",
      "Iteration 47, loss = 0.04388454\n",
      "Iteration 27, loss = 0.02676067\n",
      "Iteration 43, loss = 0.04958762\n",
      "Iteration 39, loss = 0.02068985\n",
      "Iteration 24, loss = 0.25857014\n",
      "Iteration 34, loss = 0.00615416\n",
      "Iteration 22, loss = 0.07670376\n",
      "Iteration 38, loss = 0.01958371\n",
      "Iteration 30, loss = 0.14997407\n",
      "Iteration 48, loss = 0.04319339\n",
      "Iteration 41, loss = 0.04854472\n",
      "Iteration 28, loss = 0.02900348\n",
      "Iteration 44, loss = 0.04879758\n",
      "Iteration 40, loss = 0.02067808\n",
      "Iteration 25, loss = 0.34442338\n",
      "Iteration 39, loss = 0.01956446\n",
      "Iteration 49, loss = 0.04274484\n",
      "Iteration 42, loss = 0.04767556\n",
      "Iteration 29, loss = 0.08970256\n",
      "Iteration 29, loss = 0.04200764\n",
      "Iteration 44, loss = 0.00370732\n",
      "Iteration 45, loss = 0.04838131\n",
      "Iteration 41, loss = 0.02350385\n",
      "Iteration 26, loss = 0.33892778\n",
      "Iteration 50, loss = 0.04217980\n",
      "Iteration 40, loss = 0.02091845\n",
      "Iteration 35, loss = 0.00609508\n",
      "Iteration 43, loss = 0.04720369\n",
      "Iteration 23, loss = 0.07022289\n",
      "Iteration 31, loss = 0.13085471\n",
      "Iteration 46, loss = 0.04767901\n",
      "Iteration 30, loss = 0.10108097\n",
      "Iteration 42, loss = 0.06932700\n",
      "Iteration 27, loss = 0.28216852\n",
      "Iteration 51, loss = 0.04167073\n",
      "Iteration 41, loss = 0.05213874\n",
      "Iteration 44, loss = 0.04652494\n",
      "Iteration 47, loss = 0.04706309\n",
      "Iteration 31, loss = 0.13813049\n",
      "Iteration 43, loss = 0.19050775\n",
      "Iteration 30, loss = 0.08596346\n",
      "Iteration 28, loss = 0.20966009\n",
      "Iteration 45, loss = 0.00435156\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 52, loss = 0.04114862\n",
      "[CV 3/3; 22/36] END alpha=0.001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.001;, score=0.891 total time= 3.3min\n",
      "Iteration 42, loss = 0.19298791\n",
      "[CV 2/3; 27/36] START alpha=0.01, hidden_layer_sizes=(50,), learning_rate_init=0.1\n",
      "Iteration 45, loss = 0.04589399\n",
      "Iteration 36, loss = 0.00601278\n",
      "Iteration 48, loss = 0.04652796\n",
      "Iteration 44, loss = 0.12596296\n",
      "Iteration 32, loss = 0.12395546\n",
      "Iteration 32, loss = 0.09842034\n",
      "Iteration 29, loss = 0.15846956\n",
      "Iteration 24, loss = 0.07290948\n",
      "Iteration 53, loss = 0.04071434\n",
      "Iteration 43, loss = 0.14091806\n",
      "Iteration 46, loss = 0.04538977\n",
      "Iteration 49, loss = 0.04601698\n",
      "Iteration 45, loss = 0.08374014\n",
      "Iteration 30, loss = 0.13756205\n",
      "Iteration 33, loss = 0.07266181\n",
      "Iteration 31, loss = 0.09125179\n",
      "Iteration 54, loss = 0.04021736\n",
      "Iteration 44, loss = 0.08340172\n",
      "Iteration 1, loss = 0.53484309\n",
      "Iteration 47, loss = 0.04481650\n",
      "Iteration 50, loss = 0.04548690\n",
      "Iteration 46, loss = 0.06847202\n",
      "Iteration 31, loss = 0.14406948\n",
      "Iteration 34, loss = 0.06132690\n",
      "Iteration 37, loss = 0.00621270\n",
      "Iteration 33, loss = 0.12984387\n",
      "Iteration 25, loss = 0.11103331\n",
      "Iteration 55, loss = 0.03975898\n",
      "Iteration 45, loss = 0.06603498\n",
      "Iteration 2, loss = 0.30392904\n",
      "Iteration 48, loss = 0.04434810\n",
      "Iteration 51, loss = 0.04514701\n",
      "Iteration 47, loss = 0.05947779\n",
      "Iteration 32, loss = 0.17612739\n",
      "Iteration 35, loss = 0.05424475\n",
      "Iteration 56, loss = 0.03954145\n",
      "Iteration 32, loss = 0.08491010\n",
      "Iteration 46, loss = 0.05778972\n",
      "Iteration 49, loss = 0.04380933\n",
      "Iteration 52, loss = 0.04474320\n",
      "Iteration 3, loss = 0.24383519\n",
      "Iteration 48, loss = 0.05262905\n",
      "Iteration 33, loss = 0.25353624\n",
      "Iteration 36, loss = 0.04828667\n",
      "Iteration 57, loss = 0.03906555\n",
      "Iteration 38, loss = 0.15775453\n",
      "Iteration 34, loss = 0.11545764\n",
      "Iteration 26, loss = 0.11066343\n",
      "Iteration 53, loss = 0.04439574\n",
      "Iteration 47, loss = 0.05158863\n",
      "Iteration 50, loss = 0.04335354\n",
      "Iteration 4, loss = 0.25449311\n",
      "Iteration 49, loss = 0.04763733\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 34, loss = 0.32261032\n",
      "[CV 2/3; 26/36] END alpha=0.01, hidden_layer_sizes=(50,), learning_rate_init=0.01;, score=0.894 total time= 1.5min\n",
      "[CV 3/3; 27/36] START alpha=0.01, hidden_layer_sizes=(50,), learning_rate_init=0.1\n",
      "Iteration 37, loss = 0.04378496\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 58, loss = 0.03864011\n",
      "[CV 3/3; 26/36] END alpha=0.01, hidden_layer_sizes=(50,), learning_rate_init=0.01;, score=0.905 total time= 1.2min\n",
      "[CV 1/3; 28/36] START alpha=0.01, hidden_layer_sizes=(100,), learning_rate_init=0.001\n",
      "Iteration 54, loss = 0.04367151\n",
      "Iteration 48, loss = 0.04686252\n",
      "Iteration 51, loss = 0.04308501\n",
      "Iteration 5, loss = 0.28796220\n",
      "Iteration 35, loss = 0.29343175\n",
      "Iteration 33, loss = 0.09059316\n",
      "Iteration 59, loss = 0.03818937\n",
      "Iteration 55, loss = 0.04333857\n",
      "Iteration 39, loss = 0.06136923\n",
      "Iteration 35, loss = 0.11122129\n",
      "Iteration 36, loss = 0.24881932\n",
      "Iteration 49, loss = 0.04292678\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 52, loss = 0.04249662\n",
      "Iteration 6, loss = 0.27847565\n",
      "Iteration 1, loss = 0.52607687\n",
      "Iteration 27, loss = 0.14394844\n",
      "[CV 1/3; 26/36] END alpha=0.01, hidden_layer_sizes=(50,), learning_rate_init=0.01;, score=0.900 total time= 1.6min\n",
      "[CV 2/3; 28/36] START alpha=0.01, hidden_layer_sizes=(100,), learning_rate_init=0.001\n",
      "Iteration 60, loss = 0.03775751\n",
      "Iteration 56, loss = 0.04281608\n",
      "Iteration 37, loss = 0.20877653\n",
      "Iteration 53, loss = 0.04215778\n",
      "Iteration 2, loss = 0.30658551\n",
      "Iteration 7, loss = 0.24569118\n",
      "Iteration 1, loss = 0.64545542\n",
      "Iteration 61, loss = 0.03739306\n",
      "Iteration 34, loss = 0.09871560\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/3; 24/36] END alpha=0.001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.1;, score=0.888 total time= 2.5min\n",
      "Iteration 57, loss = 0.04254976\n",
      "[CV 3/3; 28/36] START alpha=0.01, hidden_layer_sizes=(100,), learning_rate_init=0.001\n",
      "Iteration 38, loss = 0.19921293\n",
      "Iteration 3, loss = 0.24195662\n",
      "Iteration 54, loss = 0.04164172\n",
      "Iteration 8, loss = 0.22914049\n",
      "Iteration 40, loss = 0.03175043\n",
      "Iteration 36, loss = 0.11986376\n",
      "Iteration 28, loss = 0.11375600\n",
      "Iteration 62, loss = 0.03710293\n",
      "Iteration 58, loss = 0.04209799\n",
      "Iteration 39, loss = 0.19012049\n",
      "Iteration 4, loss = 0.26167748\n",
      "Iteration 2, loss = 0.48167456\n",
      "Iteration 55, loss = 0.04112184\n",
      "Iteration 1, loss = 0.64529914\n",
      "Iteration 9, loss = 0.21233758\n",
      "Iteration 63, loss = 0.03669160\n",
      "Iteration 59, loss = 0.04167660\n",
      "Iteration 40, loss = 0.20484732\n",
      "Iteration 5, loss = 0.28099173\n",
      "Iteration 56, loss = 0.04074025\n",
      "Iteration 10, loss = 0.25080735\n",
      "Iteration 64, loss = 0.03635731\n",
      "Iteration 1, loss = 0.64488804\n",
      "Iteration 41, loss = 0.01998791\n",
      "Iteration 37, loss = 0.12921694\n",
      "Iteration 29, loss = 0.09385530\n",
      "Iteration 60, loss = 0.04158027\n",
      "Iteration 3, loss = 0.35631254\n",
      "Iteration 41, loss = 0.25267274\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 2, loss = 0.48013827\n",
      "Iteration 6, loss = 0.25579586\n",
      "[CV 1/3; 27/36] END alpha=0.01, hidden_layer_sizes=(50,), learning_rate_init=0.1;, score=0.861 total time= 1.3min\n",
      "[CV 1/3; 29/36] START alpha=0.01, hidden_layer_sizes=(100,), learning_rate_init=0.01\n",
      "Iteration 57, loss = 0.04036197\n",
      "Iteration 11, loss = 0.28701854\n",
      "Iteration 65, loss = 0.03605308\n",
      "Iteration 61, loss = 0.04102110\n",
      "Iteration 7, loss = 0.25612395\n",
      "Iteration 2, loss = 0.48100186\n",
      "Iteration 58, loss = 0.04006747\n",
      "Iteration 12, loss = 0.26055130\n",
      "Iteration 66, loss = 0.03567477\n",
      "Iteration 4, loss = 0.27632035\n",
      "Iteration 62, loss = 0.04092139\n",
      "Iteration 3, loss = 0.35643940\n",
      "Iteration 42, loss = 0.01718018\n",
      "Iteration 38, loss = 0.13188691\n",
      "Iteration 8, loss = 0.26819945\n",
      "Iteration 30, loss = 0.09032341\n",
      "Iteration 59, loss = 0.03961688\n",
      "Iteration 13, loss = 0.21581041\n",
      "Iteration 67, loss = 0.03543537\n",
      "Iteration 3, loss = 0.35887871\n",
      "Iteration 63, loss = 0.04017427\n",
      "Iteration 1, loss = 0.42748478\n",
      "Iteration 9, loss = 0.25901102\n",
      "Iteration 5, loss = 0.22157279\n",
      "Iteration 60, loss = 0.03927253\n",
      "Iteration 68, loss = 0.03517304\n",
      "Iteration 14, loss = 0.18068412\n",
      "Iteration 4, loss = 0.27628234\n",
      "Iteration 64, loss = 0.04020489\n",
      "Iteration 10, loss = 0.24025460\n",
      "Iteration 43, loss = 0.01636687\n",
      "Iteration 39, loss = 0.13446487\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 0.28007134\n",
      "Iteration 61, loss = 0.03883077\n",
      "Iteration 69, loss = 0.03476592\n",
      "Iteration 31, loss = 0.08957251\n",
      "Iteration 15, loss = 0.19065429\n",
      "Iteration 2, loss = 0.17224578\n",
      "[CV 1/3; 24/36] END alpha=0.001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.1;, score=0.886 total time= 2.8min\n",
      "Iteration 65, loss = 0.03982096\n",
      "[CV 2/3; 29/36] START alpha=0.01, hidden_layer_sizes=(100,), learning_rate_init=0.01\n",
      "Iteration 6, loss = 0.18416921\n",
      "Iteration 11, loss = 0.23735936\n",
      "Iteration 70, loss = 0.03442545\n",
      "Iteration 62, loss = 0.03877662\n",
      "Iteration 5, loss = 0.22418508\n",
      "Iteration 16, loss = 0.24865859\n",
      "Iteration 66, loss = 0.03924313\n",
      "Iteration 5, loss = 0.22821121\n",
      "Iteration 12, loss = 0.29987647\n",
      "Iteration 3, loss = 0.10871821\n",
      "Iteration 71, loss = 0.03414780\n",
      "Iteration 44, loss = 0.01560870\n",
      "Iteration 63, loss = 0.03821000\n",
      "Iteration 17, loss = 0.28641471\n",
      "Iteration 7, loss = 0.15699843\n",
      "Iteration 67, loss = 0.03883363\n",
      "Iteration 32, loss = 0.10618533\n",
      "Iteration 13, loss = 0.30459288\n",
      "Iteration 1, loss = 0.42387932\n",
      "Iteration 6, loss = 0.18499749\n",
      "Iteration 72, loss = 0.03373257\n",
      "Iteration 6, loss = 0.18935467\n",
      "Iteration 64, loss = 0.03798184\n",
      "Iteration 18, loss = 0.26146277\n",
      "Iteration 68, loss = 0.03870920\n",
      "Iteration 4, loss = 0.08272922\n",
      "Iteration 14, loss = 0.27529221\n",
      "Iteration 8, loss = 0.13630066\n",
      "Iteration 73, loss = 0.03370004\n",
      "Iteration 65, loss = 0.03766317\n",
      "Iteration 19, loss = 0.23197044\n",
      "Iteration 69, loss = 0.03832695\n",
      "Iteration 45, loss = 0.01524516\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 2, loss = 0.16371405\n",
      "Iteration 15, loss = 0.24473939\n",
      "Iteration 7, loss = 0.16180463\n",
      "Iteration 33, loss = 0.09449909\n",
      "[CV 3/3; 23/36] END alpha=0.001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.01;, score=0.902 total time= 3.3min\n",
      "Iteration 7, loss = 0.15821005\n",
      "[CV 3/3; 29/36] START alpha=0.01, hidden_layer_sizes=(100,), learning_rate_init=0.01\n",
      "Iteration 74, loss = 0.03321677\n",
      "Iteration 5, loss = 0.06837680\n",
      "Iteration 66, loss = 0.03718326\n",
      "Iteration 70, loss = 0.03799711\n",
      "Iteration 20, loss = 0.22459168\n",
      "Iteration 16, loss = 0.26051964\n",
      "Iteration 9, loss = 0.12125238\n",
      "Iteration 75, loss = 0.03311599\n",
      "Iteration 3, loss = 0.11010133\n",
      "Iteration 8, loss = 0.14065042\n",
      "Iteration 71, loss = 0.03764633\n",
      "Iteration 67, loss = 0.03690471\n",
      "Iteration 21, loss = 0.22743596\n",
      "Iteration 17, loss = 0.28952122\n",
      "Iteration 8, loss = 0.13810352\n",
      "Iteration 6, loss = 0.06208568\n",
      "Iteration 34, loss = 0.11228534\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 76, loss = 0.03277175\n",
      "[CV 3/3; 24/36] END alpha=0.001, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.1;, score=0.883 total time= 2.4min\n",
      "Iteration 72, loss = 0.03744664\n",
      "Iteration 68, loss = 0.03660399\n",
      "Iteration 10, loss = 0.10896098\n",
      "Iteration 1, loss = 0.42847975\n",
      "Iteration 22, loss = 0.24342467\n",
      "[CV 1/3; 30/36] START alpha=0.01, hidden_layer_sizes=(100,), learning_rate_init=0.1\n",
      "Iteration 18, loss = 0.27246449\n",
      "Iteration 4, loss = 0.08804187\n",
      "Iteration 9, loss = 0.12537888\n",
      "Iteration 77, loss = 0.03237643\n",
      "Iteration 73, loss = 0.03718448\n",
      "Iteration 69, loss = 0.03639385\n",
      "Iteration 9, loss = 0.12291290\n",
      "Iteration 23, loss = 0.24039129\n",
      "Iteration 19, loss = 0.25892844\n",
      "Iteration 7, loss = 0.05159162\n",
      "Iteration 78, loss = 0.03225713\n",
      "Iteration 11, loss = 0.09929512\n",
      "Iteration 2, loss = 0.18308678\n",
      "Iteration 74, loss = 0.03693421\n",
      "Iteration 5, loss = 0.07853730\n",
      "Iteration 70, loss = 0.03626174\n",
      "Iteration 10, loss = 0.11228281\n",
      "Iteration 20, loss = 0.24068985\n",
      "Iteration 24, loss = 0.22017397\n",
      "Iteration 79, loss = 0.03188779\n",
      "Iteration 1, loss = 0.69165474\n",
      "Iteration 10, loss = 0.11100873\n",
      "Iteration 75, loss = 0.03641170\n",
      "Iteration 8, loss = 0.04753402\n",
      "Iteration 71, loss = 0.03579008\n",
      "Iteration 21, loss = 0.22821796\n",
      "Iteration 25, loss = 0.20556230\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 0.11416317\n",
      "[CV 2/3; 27/36] END alpha=0.01, hidden_layer_sizes=(50,), learning_rate_init=0.1;, score=0.886 total time=  48.5s\n",
      "Iteration 12, loss = 0.09118966\n",
      "Iteration 6, loss = 0.07611991\n",
      "[CV 2/3; 30/36] START alpha=0.01, hidden_layer_sizes=(100,), learning_rate_init=0.1\n",
      "Iteration 80, loss = 0.03161117\n",
      "Iteration 11, loss = 0.10258973\n",
      "Iteration 76, loss = 0.03635866\n",
      "Iteration 72, loss = 0.03562543\n",
      "Iteration 22, loss = 0.22802576\n",
      "Iteration 2, loss = 0.41890660\n",
      "Iteration 81, loss = 0.03147677\n",
      "Iteration 9, loss = 0.05160980\n",
      "Iteration 11, loss = 0.10079623\n",
      "Iteration 77, loss = 0.03592574\n",
      "Iteration 4, loss = 0.08420246\n",
      "Iteration 13, loss = 0.08457985\n",
      "Iteration 7, loss = 0.06997917\n",
      "Iteration 23, loss = 0.24379026\n",
      "Iteration 73, loss = 0.03526509\n",
      "Iteration 12, loss = 0.09413687\n",
      "Iteration 82, loss = 0.03118361\n",
      "Iteration 78, loss = 0.03596628\n",
      "Iteration 3, loss = 0.30369444\n",
      "Iteration 24, loss = 0.25712416\n",
      "Iteration 74, loss = 0.03491734\n",
      "Iteration 1, loss = 0.64125405\n",
      "Iteration 10, loss = 0.08126702\n",
      "Iteration 12, loss = 0.09354980\n",
      "Iteration 5, loss = 0.06896200\n",
      "Iteration 83, loss = 0.03085739\n",
      "Iteration 8, loss = 0.06746771\n",
      "Iteration 14, loss = 0.07946512\n",
      "Iteration 13, loss = 0.08787283\n",
      "Iteration 79, loss = 0.03557104\n",
      "Iteration 25, loss = 0.27473832\n",
      "Iteration 75, loss = 0.03454549\n",
      "Iteration 84, loss = 0.03074657\n",
      "Iteration 4, loss = 0.28395243\n",
      "Iteration 2, loss = 0.36994311\n",
      "Iteration 80, loss = 0.03508461\n",
      "Iteration 11, loss = 0.11106329\n",
      "Iteration 26, loss = 0.28226018\n",
      "Iteration 6, loss = 0.06564900\n",
      "Iteration 13, loss = 0.08733888\n",
      "Iteration 9, loss = 0.06895013\n",
      "Iteration 76, loss = 0.03448877\n",
      "Iteration 14, loss = 0.08187236\n",
      "Iteration 15, loss = 0.07527063\n",
      "Iteration 85, loss = 0.03040910\n",
      "Iteration 81, loss = 0.03513445\n",
      "Iteration 27, loss = 0.26210888\n",
      "Iteration 5, loss = 0.28488754\n",
      "Iteration 77, loss = 0.03410313\n",
      "Iteration 3, loss = 0.27445810\n",
      "Iteration 86, loss = 0.03034073\n",
      "Iteration 12, loss = 0.09091200\n",
      "Iteration 10, loss = 0.08432854\n",
      "Iteration 7, loss = 0.06347152\n",
      "Iteration 82, loss = 0.03470141\n",
      "Iteration 14, loss = 0.08186933\n",
      "Iteration 15, loss = 0.07696747\n",
      "Iteration 28, loss = 0.25208884\n",
      "Iteration 16, loss = 0.07084555\n",
      "Iteration 78, loss = 0.03392804\n",
      "Iteration 87, loss = 0.03013994\n",
      "Iteration 83, loss = 0.03457908\n",
      "Iteration 6, loss = 0.28110238\n",
      "Iteration 4, loss = 0.29172915\n",
      "Iteration 29, loss = 0.21060668\n",
      "Iteration 13, loss = 0.07799990\n",
      "Iteration 79, loss = 0.03398673\n",
      "Iteration 11, loss = 0.08597075\n",
      "Iteration 8, loss = 0.06934186\n",
      "Iteration 88, loss = 0.02989719\n",
      "Iteration 16, loss = 0.07294067\n",
      "Iteration 15, loss = 0.07791343\n",
      "Iteration 84, loss = 0.03423824\n",
      "Iteration 17, loss = 0.06756052\n",
      "Iteration 30, loss = 0.19287184\n",
      "Iteration 80, loss = 0.03324614\n",
      "Iteration 89, loss = 0.02957787\n",
      "Iteration 7, loss = 0.27913540\n",
      "Iteration 5, loss = 0.26608436\n",
      "Iteration 85, loss = 0.03386961\n",
      "Iteration 12, loss = 0.07101338\n",
      "Iteration 14, loss = 0.06446458\n",
      "Iteration 31, loss = 0.18077589\n",
      "Iteration 9, loss = 0.08839084\n",
      "Iteration 17, loss = 0.06954498\n",
      "Iteration 81, loss = 0.03308687\n",
      "Iteration 90, loss = 0.02936610\n",
      "Iteration 16, loss = 0.07345610\n",
      "Iteration 18, loss = 0.06457249\n",
      "Iteration 86, loss = 0.03370150\n",
      "Iteration 32, loss = 0.19272832\n",
      "Iteration 6, loss = 0.23865058\n",
      "Iteration 8, loss = 0.28373959\n",
      "Iteration 91, loss = 0.02921859\n",
      "Iteration 82, loss = 0.03285168\n",
      "Iteration 13, loss = 0.06367395\n",
      "Iteration 10, loss = 0.08553495\n",
      "Iteration 15, loss = 0.05481779\n",
      "Iteration 87, loss = 0.03356067\n",
      "Iteration 18, loss = 0.06690165\n",
      "Iteration 33, loss = 0.24965851\n",
      "Iteration 19, loss = 0.06200588\n",
      "Iteration 17, loss = 0.07033274\n",
      "Iteration 92, loss = 0.02895647\n",
      "Iteration 83, loss = 0.03260354\n",
      "Iteration 7, loss = 0.24569228\n",
      "Iteration 88, loss = 0.03325593\n",
      "Iteration 9, loss = 0.28951059\n",
      "Iteration 34, loss = 0.28907652\n",
      "Iteration 14, loss = 0.06024842\n",
      "Iteration 11, loss = 0.07668658\n",
      "Iteration 93, loss = 0.02867826\n",
      "Iteration 16, loss = 0.04754398\n",
      "Iteration 19, loss = 0.06407407\n",
      "Iteration 84, loss = 0.03238362\n",
      "Iteration 89, loss = 0.03303983\n",
      "Iteration 35, loss = 0.27937921\n",
      "Iteration 20, loss = 0.06023018\n",
      "Iteration 18, loss = 0.06772884\n",
      "Iteration 8, loss = 0.29437728\n",
      "Iteration 94, loss = 0.02858866\n",
      "Iteration 10, loss = 0.27355198\n",
      "Iteration 85, loss = 0.03210752\n",
      "Iteration 15, loss = 0.06204945\n",
      "Iteration 90, loss = 0.03292995\n",
      "Iteration 12, loss = 0.07046015\n",
      "Iteration 36, loss = 0.24439246\n",
      "Iteration 20, loss = 0.06162574\n",
      "Iteration 17, loss = 0.04143243\n",
      "Iteration 95, loss = 0.02823440\n",
      "Iteration 86, loss = 0.03186051\n",
      "Iteration 21, loss = 0.05774189\n",
      "Iteration 91, loss = 0.03248858\n",
      "Iteration 9, loss = 0.28642015\n",
      "Iteration 19, loss = 0.06497600\n",
      "Iteration 37, loss = 0.20206235\n",
      "Iteration 11, loss = 0.24423892\n",
      "Iteration 16, loss = 0.06542547\n",
      "Iteration 96, loss = 0.02812042\n",
      "Iteration 13, loss = 0.06544192\n",
      "Iteration 21, loss = 0.05946292\n",
      "Iteration 87, loss = 0.03171409\n",
      "Iteration 92, loss = 0.03241391\n",
      "Iteration 18, loss = 0.03667918\n",
      "Iteration 38, loss = 0.18439490\n",
      "Iteration 97, loss = 0.02785927\n",
      "Iteration 22, loss = 0.05589638\n",
      "Iteration 10, loss = 0.26885584\n",
      "Iteration 93, loss = 0.03215328\n",
      "Iteration 12, loss = 0.21282637\n",
      "Iteration 20, loss = 0.06323302\n",
      "Iteration 88, loss = 0.03167232\n",
      "Iteration 17, loss = 0.06640791\n",
      "Iteration 39, loss = 0.18915484\n",
      "Iteration 14, loss = 0.05953931\n",
      "Iteration 22, loss = 0.05773081\n",
      "Iteration 98, loss = 0.02774107\n",
      "Iteration 19, loss = 0.03315182\n",
      "Iteration 94, loss = 0.03198684\n",
      "Iteration 89, loss = 0.03145555\n",
      "Iteration 40, loss = 0.19627513\n",
      "Iteration 11, loss = 0.25552451\n",
      "Iteration 23, loss = 0.05455258\n",
      "Iteration 99, loss = 0.02762937\n",
      "Iteration 13, loss = 0.25517252\n",
      "Iteration 18, loss = 0.06569086\n",
      "Iteration 21, loss = 0.06074818\n",
      "Iteration 95, loss = 0.03171640\n",
      "Iteration 15, loss = 0.05469317\n",
      "Iteration 23, loss = 0.05589950\n",
      "Iteration 90, loss = 0.03115832\n",
      "Iteration 41, loss = 0.19705213\n",
      "Iteration 20, loss = 0.03036931\n",
      "Iteration 100, loss = 0.02738428\n",
      "Iteration 96, loss = 0.03144564\n",
      "Iteration 12, loss = 0.24263355\n",
      "Iteration 24, loss = 0.05313672\n",
      "Iteration 91, loss = 0.03107169\n",
      "Iteration 42, loss = 0.20251073\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 0.29447403\n",
      "[CV 3/3; 27/36] END alpha=0.01, hidden_layer_sizes=(50,), learning_rate_init=0.1;, score=0.869 total time= 1.3min\n",
      "Iteration 19, loss = 0.06180673\n",
      "Iteration 101, loss = 0.02731443\n",
      "[CV 3/3; 30/36] START alpha=0.01, hidden_layer_sizes=(100,), learning_rate_init=0.1\n",
      "Iteration 22, loss = 0.05956318\n",
      "Iteration 16, loss = 0.04817070\n",
      "Iteration 24, loss = 0.05440225\n",
      "Iteration 97, loss = 0.03127117\n",
      "Iteration 21, loss = 0.02828864\n",
      "Iteration 92, loss = 0.03055838\n",
      "Iteration 102, loss = 0.02715588\n",
      "Iteration 13, loss = 0.24225158\n",
      "Iteration 25, loss = 0.05157813\n",
      "Iteration 15, loss = 0.29603416\n",
      "Iteration 98, loss = 0.03117712\n",
      "Iteration 20, loss = 0.05675997\n",
      "Iteration 93, loss = 0.03045489\n",
      "Iteration 25, loss = 0.05301682\n",
      "Iteration 17, loss = 0.04257052\n",
      "Iteration 103, loss = 0.02684969\n",
      "Iteration 23, loss = 0.05753265\n",
      "Iteration 22, loss = 0.02633764\n",
      "Iteration 99, loss = 0.03098911\n",
      "Iteration 14, loss = 0.27406296\n",
      "Iteration 1, loss = 0.64888828\n",
      "Iteration 94, loss = 0.03008936\n",
      "Iteration 26, loss = 0.05046620\n",
      "Iteration 104, loss = 0.02666495\n",
      "Iteration 16, loss = 0.26177634\n",
      "Iteration 21, loss = 0.05309577\n",
      "Iteration 100, loss = 0.03081413\n",
      "Iteration 26, loss = 0.05193914\n",
      "Iteration 18, loss = 0.03804565\n",
      "Iteration 24, loss = 0.05611320\n",
      "Iteration 95, loss = 0.03006458\n",
      "Iteration 105, loss = 0.02656971\n",
      "Iteration 23, loss = 0.02496454\n",
      "Iteration 15, loss = 0.28157764\n",
      "Iteration 101, loss = 0.03050458\n",
      "Iteration 2, loss = 0.37967308\n",
      "Iteration 17, loss = 0.23189481\n",
      "Iteration 27, loss = 0.04924681\n",
      "Iteration 22, loss = 0.04979199\n",
      "Iteration 106, loss = 0.02672279\n",
      "Iteration 96, loss = 0.02975598\n",
      "Iteration 27, loss = 0.05063540\n",
      "Iteration 19, loss = 0.03459453\n",
      "Iteration 102, loss = 0.03038811\n",
      "Iteration 25, loss = 0.05451557\n",
      "Iteration 16, loss = 0.27442810\n",
      "Iteration 24, loss = 0.02400579\n",
      "Iteration 107, loss = 0.02631056\n",
      "Iteration 97, loss = 0.02972435\n",
      "Iteration 3, loss = 0.27891984\n",
      "Iteration 18, loss = 0.21566993\n",
      "Iteration 103, loss = 0.03016841\n",
      "Iteration 28, loss = 0.04822598\n",
      "Iteration 23, loss = 0.05026470\n",
      "Iteration 28, loss = 0.04961554\n",
      "Iteration 20, loss = 0.03348234\n",
      "Iteration 108, loss = 0.02601471\n",
      "Iteration 98, loss = 0.02946354\n",
      "Iteration 26, loss = 0.05326536\n",
      "Iteration 104, loss = 0.03005343\n",
      "Iteration 17, loss = 0.25181033\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 25, loss = 0.02330064\n",
      "Iteration 4, loss = 0.27441536\n",
      "[CV 2/3; 30/36] END alpha=0.01, hidden_layer_sizes=(100,), learning_rate_init=0.1;, score=0.878 total time=  51.3s\n",
      "[CV 1/3; 31/36] START alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate_init=0.001\n",
      "Iteration 109, loss = 0.02594392\n",
      "Iteration 19, loss = 0.21425508\n",
      "Iteration 24, loss = 0.05449062\n",
      "Iteration 99, loss = 0.02917370\n",
      "Iteration 29, loss = 0.04711157\n",
      "Iteration 29, loss = 0.04880194\n",
      "Iteration 105, loss = 0.02983870\n",
      "Iteration 21, loss = 0.03322071\n",
      "Iteration 110, loss = 0.02583060\n",
      "Iteration 100, loss = 0.02889915\n",
      "Iteration 26, loss = 0.02290891\n",
      "Iteration 5, loss = 0.26388917\n",
      "Iteration 27, loss = 0.05217740\n",
      "Iteration 106, loss = 0.02958136\n",
      "Iteration 20, loss = 0.27151919\n",
      "Iteration 25, loss = 0.06153498\n",
      "Iteration 30, loss = 0.04764407\n",
      "Iteration 30, loss = 0.04619766\n",
      "Iteration 111, loss = 0.02582412\n",
      "Iteration 22, loss = 0.03352591\n",
      "Iteration 101, loss = 0.02916933\n",
      "Iteration 107, loss = 0.02936389\n",
      "Iteration 1, loss = 0.64106438\n",
      "Iteration 6, loss = 0.26541198\n",
      "Iteration 112, loss = 0.02566021\n",
      "Iteration 27, loss = 0.02245708\n",
      "Iteration 28, loss = 0.05111335\n",
      "Iteration 21, loss = 0.32602528\n",
      "Iteration 102, loss = 0.02879784\n",
      "Iteration 26, loss = 0.08104603\n",
      "Iteration 108, loss = 0.02932553\n",
      "Iteration 31, loss = 0.04715409\n",
      "Iteration 31, loss = 0.04558456\n",
      "Iteration 23, loss = 0.03707600\n",
      "Iteration 113, loss = 0.02530496\n",
      "Iteration 2, loss = 0.41066920\n",
      "Iteration 103, loss = 0.02847879\n",
      "Iteration 109, loss = 0.02916240\n",
      "Iteration 7, loss = 0.25586119\n",
      "Iteration 28, loss = 0.02304700\n",
      "Iteration 22, loss = 0.29968439\n",
      "Iteration 27, loss = 0.09902151\n",
      "Iteration 29, loss = 0.05033234\n",
      "Iteration 114, loss = 0.02511516\n",
      "Iteration 32, loss = 0.04620505\n",
      "Iteration 32, loss = 0.04455038\n",
      "Iteration 110, loss = 0.02893823\n",
      "Iteration 104, loss = 0.02829261\n",
      "Iteration 24, loss = 0.05369800\n",
      "Iteration 3, loss = 0.23161435\n",
      "Iteration 115, loss = 0.02503885\n",
      "Iteration 8, loss = 0.27209835\n",
      "Iteration 111, loss = 0.02896426\n",
      "Iteration 29, loss = 0.04016227\n",
      "Iteration 105, loss = 0.02821714\n",
      "Iteration 23, loss = 0.25244142\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 28, loss = 0.08680292\n",
      "[CV 1/3; 30/36] END alpha=0.01, hidden_layer_sizes=(100,), learning_rate_init=0.1;, score=0.886 total time= 1.2min\n",
      "Iteration 30, loss = 0.04916834\n",
      "Iteration 33, loss = 0.04509769\n",
      "[CV 2/3; 31/36] START alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate_init=0.001\n",
      "Iteration 116, loss = 0.02484423\n",
      "Iteration 25, loss = 0.10980766\n",
      "Iteration 33, loss = 0.04380958\n",
      "Iteration 112, loss = 0.02867955\n",
      "Iteration 4, loss = 0.14155354\n",
      "Iteration 106, loss = 0.02806601\n",
      "Iteration 9, loss = 0.25316722\n",
      "Iteration 117, loss = 0.02472734\n",
      "Iteration 30, loss = 0.19106818\n",
      "Iteration 29, loss = 0.07809409\n",
      "Iteration 113, loss = 0.02851633\n",
      "Iteration 34, loss = 0.04445705\n",
      "Iteration 107, loss = 0.02772783\n",
      "Iteration 31, loss = 0.04905596\n",
      "Iteration 26, loss = 0.12877702\n",
      "Iteration 34, loss = 0.04309145\n",
      "Iteration 118, loss = 0.02461643\n",
      "Iteration 5, loss = 0.09679535\n",
      "Iteration 114, loss = 0.02835310\n",
      "Iteration 10, loss = 0.24224534\n",
      "Iteration 1, loss = 0.64367213\n",
      "Iteration 108, loss = 0.02743511\n",
      "Iteration 30, loss = 0.06680940\n",
      "Iteration 31, loss = 0.17818026\n",
      "Iteration 35, loss = 0.04383295\n",
      "Iteration 119, loss = 0.02458156\n",
      "Iteration 32, loss = 0.04764402\n",
      "Iteration 115, loss = 0.02805797\n",
      "Iteration 27, loss = 0.10373637\n",
      "Iteration 35, loss = 0.04235042\n",
      "Iteration 109, loss = 0.02746960\n",
      "Iteration 6, loss = 0.06991623\n",
      "Iteration 11, loss = 0.21940253\n",
      "Iteration 120, loss = 0.02449407\n",
      "Iteration 116, loss = 0.02813235\n",
      "Iteration 2, loss = 0.41020485\n",
      "Iteration 31, loss = 0.05747159\n",
      "Iteration 36, loss = 0.04321404\n",
      "Iteration 32, loss = 0.10558653\n",
      "Iteration 110, loss = 0.02730833\n",
      "Iteration 121, loss = 0.02430680\n",
      "Iteration 28, loss = 0.08819389\n",
      "Iteration 33, loss = 0.04692949\n",
      "Iteration 36, loss = 0.04196400\n",
      "Iteration 7, loss = 0.05462444\n",
      "Iteration 117, loss = 0.02788296\n",
      "Iteration 12, loss = 0.20080990\n",
      "Iteration 111, loss = 0.02711461\n",
      "Iteration 122, loss = 0.02431373\n",
      "Iteration 32, loss = 0.04978952\n",
      "Iteration 3, loss = 0.23021733\n",
      "Iteration 37, loss = 0.04249232\n",
      "Iteration 33, loss = 0.08361586\n",
      "Iteration 118, loss = 0.02775951\n",
      "Iteration 29, loss = 0.07272929\n",
      "Iteration 112, loss = 0.02713100\n",
      "Iteration 34, loss = 0.04604093\n",
      "Iteration 8, loss = 0.04476872\n",
      "Iteration 123, loss = 0.02408058\n",
      "Iteration 37, loss = 0.04119493\n",
      "Iteration 13, loss = 0.19566089\n",
      "Iteration 119, loss = 0.02758497\n",
      "Iteration 33, loss = 0.04417271\n",
      "Iteration 38, loss = 0.04197264\n",
      "Iteration 4, loss = 0.13938583\n",
      "Iteration 113, loss = 0.02684859\n",
      "Iteration 124, loss = 0.02394189\n",
      "Iteration 34, loss = 0.07071727\n",
      "Iteration 120, loss = 0.02750034\n",
      "Iteration 30, loss = 0.06154620\n",
      "Iteration 9, loss = 0.03842011\n",
      "Iteration 38, loss = 0.04060307\n",
      "Iteration 35, loss = 0.04517251\n",
      "Iteration 14, loss = 0.19656801\n",
      "Iteration 114, loss = 0.02663163\n",
      "Iteration 125, loss = 0.02386859\n",
      "Iteration 34, loss = 0.03955381\n",
      "Iteration 39, loss = 0.04134823\n",
      "Iteration 121, loss = 0.02748746\n",
      "Iteration 5, loss = 0.09547711\n",
      "Iteration 35, loss = 0.06167469\n",
      "Iteration 126, loss = 0.02365507\n",
      "Iteration 115, loss = 0.02655025\n",
      "Iteration 31, loss = 0.05333814\n",
      "Iteration 10, loss = 0.03295971\n",
      "Iteration 122, loss = 0.02738938\n",
      "Iteration 15, loss = 0.22602918\n",
      "Iteration 39, loss = 0.03997723\n",
      "Iteration 36, loss = 0.04456110\n",
      "Iteration 35, loss = 0.03598862\n",
      "Iteration 40, loss = 0.04073389\n",
      "Iteration 127, loss = 0.02353458\n",
      "Iteration 116, loss = 0.02638727\n",
      "Iteration 6, loss = 0.07068393\n",
      "Iteration 123, loss = 0.02699775\n",
      "Iteration 36, loss = 0.05467569\n",
      "Iteration 32, loss = 0.04693233\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 0.02937618\n",
      "[CV 3/3; 29/36] END alpha=0.01, hidden_layer_sizes=(100,), learning_rate_init=0.01;, score=0.904 total time= 1.6min\n",
      "Iteration 128, loss = 0.02337667\n",
      "[CV 3/3; 31/36] START alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate_init=0.001\n",
      "Iteration 117, loss = 0.02623160\n",
      "Iteration 16, loss = 0.28211338\n",
      "Iteration 40, loss = 0.03987335\n",
      "Iteration 124, loss = 0.02719944\n",
      "Iteration 37, loss = 0.04392896\n",
      "Iteration 36, loss = 0.03307670\n",
      "Iteration 41, loss = 0.04022599\n",
      "Iteration 129, loss = 0.02331148\n",
      "Iteration 7, loss = 0.05738786\n",
      "Iteration 118, loss = 0.02614920\n",
      "Iteration 37, loss = 0.04913968\n",
      "Iteration 125, loss = 0.02693522\n",
      "Iteration 12, loss = 0.02660599\n",
      "Iteration 17, loss = 0.28904162\n",
      "Iteration 130, loss = 0.02311957\n",
      "Iteration 41, loss = 0.03887344\n",
      "Iteration 37, loss = 0.03069138\n",
      "Iteration 119, loss = 0.02614545\n",
      "Iteration 42, loss = 0.03979231\n",
      "Iteration 38, loss = 0.04341196\n",
      "Iteration 126, loss = 0.02693896\n",
      "Iteration 1, loss = 0.64391010\n",
      "Iteration 8, loss = 0.04746098\n",
      "Iteration 38, loss = 0.04462131\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 131, loss = 0.02312070\n",
      "Iteration 13, loss = 0.02389706\n",
      "[CV 1/3; 29/36] END alpha=0.01, hidden_layer_sizes=(100,), learning_rate_init=0.01;, score=0.894 total time= 2.0min\n",
      "Iteration 120, loss = 0.02570768\n",
      "[CV 1/3; 32/36] START alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate_init=0.01\n",
      "Iteration 18, loss = 0.25920445\n",
      "Iteration 127, loss = 0.02682879\n",
      "Iteration 42, loss = 0.03812929\n",
      "Iteration 38, loss = 0.02870566\n",
      "Iteration 43, loss = 0.03914112\n",
      "Iteration 132, loss = 0.02292381\n",
      "Iteration 39, loss = 0.04282166\n",
      "Iteration 121, loss = 0.02572312\n",
      "Iteration 2, loss = 0.41723675\n",
      "Iteration 128, loss = 0.02635013\n",
      "Iteration 9, loss = 0.04296571\n",
      "Iteration 14, loss = 0.02187689\n",
      "Iteration 19, loss = 0.20456877\n",
      "Iteration 133, loss = 0.02283352\n",
      "Iteration 122, loss = 0.02548879\n",
      "Iteration 39, loss = 0.02707267\n",
      "Iteration 129, loss = 0.02619214\n",
      "Iteration 43, loss = 0.03768550\n",
      "Iteration 44, loss = 0.03893281\n",
      "Iteration 134, loss = 0.02273635\n",
      "Iteration 40, loss = 0.04230439\n",
      "Iteration 1, loss = 0.42347558\n",
      "Iteration 3, loss = 0.23878226\n",
      "Iteration 10, loss = 0.03745943\n",
      "Iteration 15, loss = 0.01974230\n",
      "Iteration 123, loss = 0.02542518\n",
      "Iteration 130, loss = 0.02592330\n",
      "Iteration 20, loss = 0.16266754\n",
      "Iteration 135, loss = 0.02288371\n",
      "Iteration 40, loss = 0.02570950\n",
      "Iteration 45, loss = 0.03848933\n",
      "Iteration 44, loss = 0.03727902\n",
      "Iteration 131, loss = 0.02581487\n",
      "Iteration 124, loss = 0.02522677\n",
      "Iteration 2, loss = 0.15107597\n",
      "Iteration 4, loss = 0.14676950\n",
      "Iteration 41, loss = 0.04153172\n",
      "Iteration 136, loss = 0.02263482\n",
      "Iteration 16, loss = 0.01785524\n",
      "Iteration 11, loss = 0.03312640\n",
      "Iteration 21, loss = 0.17065020\n",
      "Iteration 132, loss = 0.02577766\n",
      "Iteration 125, loss = 0.02523174\n",
      "Iteration 41, loss = 0.02466267\n",
      "Iteration 46, loss = 0.03781596\n",
      "Iteration 45, loss = 0.03689104\n",
      "Iteration 137, loss = 0.02240309\n",
      "Iteration 3, loss = 0.09623580\n",
      "Iteration 5, loss = 0.09874321\n",
      "Iteration 133, loss = 0.02555665\n",
      "Iteration 42, loss = 0.04100663\n",
      "Iteration 126, loss = 0.02492052\n",
      "Iteration 17, loss = 0.01671693\n",
      "Iteration 12, loss = 0.03011094\n",
      "Iteration 22, loss = 0.20332563\n",
      "Iteration 138, loss = 0.02224294\n",
      "Iteration 42, loss = 0.02362078\n",
      "Iteration 47, loss = 0.03744223\n",
      "Iteration 134, loss = 0.02572288\n",
      "Iteration 46, loss = 0.03623019\n",
      "Iteration 127, loss = 0.02485435\n",
      "Iteration 139, loss = 0.02223493\n",
      "Iteration 4, loss = 0.06046032\n",
      "Iteration 6, loss = 0.07182930\n",
      "Iteration 18, loss = 0.01545580\n",
      "Iteration 43, loss = 0.04038625\n",
      "Iteration 135, loss = 0.02536638\n",
      "Iteration 23, loss = 0.25045404\n",
      "Iteration 13, loss = 0.02780890\n",
      "Iteration 128, loss = 0.02470366\n",
      "Iteration 43, loss = 0.02278418\n",
      "Iteration 48, loss = 0.03704561\n",
      "Iteration 140, loss = 0.02200653\n",
      "Iteration 47, loss = 0.03606061\n",
      "Iteration 136, loss = 0.02527595\n",
      "Iteration 5, loss = 0.05191514\n",
      "Iteration 7, loss = 0.05742807\n",
      "Iteration 129, loss = 0.02457121\n",
      "Iteration 141, loss = 0.02202420\n",
      "Iteration 19, loss = 0.01497386\n",
      "Iteration 24, loss = 0.24948429\n",
      "Iteration 44, loss = 0.04026227\n",
      "Iteration 14, loss = 0.02559426\n",
      "Iteration 137, loss = 0.02551940\n",
      "Iteration 44, loss = 0.02193496\n",
      "Iteration 49, loss = 0.03666701\n",
      "Iteration 130, loss = 0.02436998\n",
      "Iteration 142, loss = 0.02208754\n",
      "Iteration 48, loss = 0.03537214\n",
      "Iteration 6, loss = 0.04613731\n",
      "Iteration 8, loss = 0.04659181\n",
      "Iteration 138, loss = 0.02530709\n",
      "Iteration 20, loss = 0.01408923\n",
      "Iteration 25, loss = 0.23787483\n",
      "Iteration 131, loss = 0.02429764\n",
      "Iteration 143, loss = 0.02180560\n",
      "Iteration 45, loss = 0.03999870\n",
      "Iteration 45, loss = 0.02126592\n",
      "Iteration 50, loss = 0.03635257\n",
      "Iteration 15, loss = 0.02333985\n",
      "Iteration 139, loss = 0.02532946\n",
      "Iteration 49, loss = 0.03500134\n",
      "Iteration 132, loss = 0.02431798\n",
      "Iteration 7, loss = 0.04970406\n",
      "Iteration 144, loss = 0.02164707\n",
      "Iteration 9, loss = 0.03947210\n",
      "Iteration 21, loss = 0.01349200\n",
      "Iteration 26, loss = 0.23637226\n",
      "Iteration 140, loss = 0.02507341\n",
      "Iteration 46, loss = 0.02097390\n",
      "Iteration 51, loss = 0.03587177\n",
      "Iteration 46, loss = 0.03925337\n",
      "Iteration 16, loss = 0.02159064\n",
      "Iteration 145, loss = 0.02159266\n",
      "Iteration 133, loss = 0.02408954\n",
      "Iteration 141, loss = 0.02484700\n",
      "Iteration 50, loss = 0.03445342\n",
      "Iteration 8, loss = 0.05678121\n",
      "Iteration 10, loss = 0.03396431\n",
      "Iteration 22, loss = 0.01316259\n",
      "Iteration 146, loss = 0.02148711\n",
      "Iteration 27, loss = 0.24765590\n",
      "Iteration 134, loss = 0.02399492\n",
      "Iteration 47, loss = 0.02042510\n",
      "Iteration 52, loss = 0.03537860\n",
      "Iteration 142, loss = 0.02459592\n",
      "Iteration 17, loss = 0.01968192\n",
      "Iteration 47, loss = 0.03881839\n",
      "Iteration 147, loss = 0.02139717\n",
      "Iteration 135, loss = 0.02381919\n",
      "Iteration 9, loss = 0.06632176\n",
      "Iteration 51, loss = 0.03423518\n",
      "Iteration 143, loss = 0.02447255\n",
      "Iteration 11, loss = 0.03026998\n",
      "Iteration 23, loss = 0.01268887\n",
      "Iteration 28, loss = 0.21502308\n",
      "Iteration 48, loss = 0.01999585\n",
      "Iteration 53, loss = 0.03506343\n",
      "Iteration 148, loss = 0.02147081\n",
      "Iteration 136, loss = 0.02377247\n",
      "Iteration 18, loss = 0.01872610\n",
      "Iteration 48, loss = 0.03815688\n",
      "Iteration 144, loss = 0.02440093\n",
      "Iteration 10, loss = 0.05966080\n",
      "Iteration 149, loss = 0.02142343\n",
      "Iteration 52, loss = 0.03391613\n",
      "Iteration 12, loss = 0.02761325\n",
      "Iteration 24, loss = 0.01239448\n",
      "Iteration 137, loss = 0.02365021\n",
      "Iteration 29, loss = 0.18819910\n",
      "Iteration 145, loss = 0.02517756\n",
      "Iteration 49, loss = 0.01947611\n",
      "Iteration 54, loss = 0.03454536\n",
      "Iteration 150, loss = 0.02140206\n",
      "Iteration 19, loss = 0.01763193\n",
      "Iteration 49, loss = 0.03785653\n",
      "Iteration 138, loss = 0.02368684\n",
      "Iteration 146, loss = 0.02428252\n",
      "Iteration 11, loss = 0.05769827\n",
      "Iteration 53, loss = 0.03354213\n",
      "Iteration 13, loss = 0.02539963\n",
      "Iteration 30, loss = 0.15211965\n",
      "Iteration 25, loss = 0.01210672\n",
      "Iteration 151, loss = 0.02111416\n",
      "Iteration 50, loss = 0.01921222\n",
      "Iteration 55, loss = 0.03418194\n",
      "Iteration 139, loss = 0.02342442\n",
      "Iteration 147, loss = 0.02431786\n",
      "Iteration 20, loss = 0.01662707\n",
      "Iteration 152, loss = 0.02099365\n",
      "Iteration 50, loss = 0.03727237\n",
      "Iteration 12, loss = 0.05105344\n",
      "Iteration 140, loss = 0.02330991\n",
      "Iteration 148, loss = 0.02406120\n",
      "Iteration 54, loss = 0.03309657\n",
      "Iteration 14, loss = 0.02294106\n",
      "Iteration 26, loss = 0.01191973\n",
      "Iteration 31, loss = 0.13447994\n",
      "Iteration 51, loss = 0.01918730\n",
      "Iteration 56, loss = 0.03390408\n",
      "Iteration 153, loss = 0.02095269\n",
      "Iteration 149, loss = 0.02392143\n",
      "Iteration 141, loss = 0.02323987\n",
      "Iteration 21, loss = 0.01580576\n",
      "Iteration 13, loss = 0.04432077\n",
      "Iteration 51, loss = 0.03694077\n",
      "Iteration 154, loss = 0.02085233\n",
      "Iteration 55, loss = 0.03253141\n",
      "Iteration 32, loss = 0.13654617\n",
      "Iteration 27, loss = 0.01177913\n",
      "Iteration 15, loss = 0.02139048\n",
      "Iteration 52, loss = 0.01909414\n",
      "Iteration 150, loss = 0.02380097\n",
      "Iteration 57, loss = 0.03354749\n",
      "Iteration 142, loss = 0.02309012\n",
      "Iteration 155, loss = 0.02072535\n",
      "Iteration 22, loss = 0.01499472\n",
      "Iteration 151, loss = 0.02375029\n",
      "Iteration 14, loss = 0.04061657\n",
      "Iteration 143, loss = 0.02299456\n",
      "Iteration 52, loss = 0.03663545\n",
      "Iteration 33, loss = 0.21389602\n",
      "Iteration 28, loss = 0.01151638\n",
      "Iteration 16, loss = 0.02017296\n",
      "Iteration 56, loss = 0.03239504\n",
      "Iteration 53, loss = 0.01935830\n",
      "Iteration 156, loss = 0.02069901\n",
      "Iteration 58, loss = 0.03323009\n",
      "Iteration 152, loss = 0.02356220\n",
      "Iteration 144, loss = 0.02305298\n",
      "Iteration 23, loss = 0.01451055\n",
      "Iteration 157, loss = 0.02052533\n",
      "Iteration 15, loss = 0.04098591\n",
      "Iteration 34, loss = 0.27100527\n",
      "Iteration 153, loss = 0.02349552\n",
      "Iteration 29, loss = 0.01129545\n",
      "Iteration 17, loss = 0.01825771\n",
      "Iteration 53, loss = 0.03609706\n",
      "Iteration 57, loss = 0.03195989\n",
      "Iteration 54, loss = 0.02587483\n",
      "Iteration 145, loss = 0.02295886\n",
      "Iteration 59, loss = 0.03292635\n",
      "Iteration 158, loss = 0.02049195\n",
      "Iteration 154, loss = 0.02325292\n",
      "Iteration 24, loss = 0.01428107\n",
      "Iteration 146, loss = 0.02271470\n",
      "Iteration 16, loss = 0.04681500\n",
      "Iteration 159, loss = 0.02041678\n",
      "Iteration 35, loss = 0.26608601\n",
      "Iteration 30, loss = 0.01123127\n",
      "Iteration 18, loss = 0.01740392\n",
      "Iteration 58, loss = 0.03168980\n",
      "Iteration 55, loss = 0.15097952\n",
      "Iteration 54, loss = 0.03580902\n",
      "Iteration 60, loss = 0.03263811\n",
      "Iteration 155, loss = 0.02335880\n",
      "Iteration 147, loss = 0.02257515\n",
      "Iteration 160, loss = 0.02034770\n",
      "Iteration 156, loss = 0.02315975\n",
      "Iteration 25, loss = 0.01381703\n",
      "Iteration 17, loss = 0.06208514\n",
      "Iteration 36, loss = 0.21215494\n",
      "Iteration 31, loss = 0.01099671\n",
      "Iteration 19, loss = 0.01653358\n",
      "Iteration 56, loss = 0.20558495\n",
      "Iteration 148, loss = 0.02258415\n",
      "Iteration 61, loss = 0.03216608\n",
      "Iteration 59, loss = 0.03136542\n",
      "Iteration 161, loss = 0.02035672\n",
      "Iteration 55, loss = 0.03520640\n",
      "Iteration 157, loss = 0.02306756\n",
      "Iteration 149, loss = 0.02251227\n",
      "Iteration 162, loss = 0.02031027\n",
      "Iteration 18, loss = 0.07820156\n",
      "Iteration 26, loss = 0.01341042\n",
      "Iteration 37, loss = 0.17590927\n",
      "Iteration 32, loss = 0.01087081\n",
      "Iteration 20, loss = 0.01561181\n",
      "Iteration 158, loss = 0.02297470\n",
      "Iteration 57, loss = 0.11325404\n",
      "Iteration 62, loss = 0.03198025\n",
      "Iteration 60, loss = 0.03094141\n",
      "Iteration 56, loss = 0.03502554\n",
      "Iteration 163, loss = 0.02043407\n",
      "Iteration 150, loss = 0.02223993\n",
      "Iteration 159, loss = 0.02299011\n",
      "Iteration 19, loss = 0.07090713\n",
      "Iteration 38, loss = 0.16658248\n",
      "Iteration 27, loss = 0.01296712\n",
      "Iteration 164, loss = 0.02019923\n",
      "Iteration 33, loss = 0.01071479\n",
      "Iteration 151, loss = 0.02234314\n",
      "Iteration 21, loss = 0.01497050\n",
      "Iteration 58, loss = 0.09048396\n",
      "Iteration 63, loss = 0.03189566\n",
      "Iteration 61, loss = 0.03073274\n",
      "Iteration 160, loss = 0.02275323\n",
      "Iteration 57, loss = 0.03502435\n",
      "Iteration 165, loss = 0.02003206\n",
      "Iteration 152, loss = 0.02235115\n",
      "Iteration 20, loss = 0.05578154\n",
      "Iteration 161, loss = 0.02283969\n",
      "Iteration 39, loss = 0.17247739\n",
      "Iteration 34, loss = 0.01060830\n",
      "Iteration 28, loss = 0.01264527\n",
      "Iteration 59, loss = 0.07573229\n",
      "Iteration 22, loss = 0.01448729\n",
      "Iteration 64, loss = 0.03126326\n",
      "Iteration 166, loss = 0.01997809\n",
      "Iteration 62, loss = 0.03036304\n",
      "Iteration 153, loss = 0.02228590\n",
      "Iteration 162, loss = 0.02278593\n",
      "Iteration 58, loss = 0.03441139\n",
      "Iteration 167, loss = 0.01989956\n",
      "Iteration 21, loss = 0.05009268\n",
      "Iteration 40, loss = 0.16414359\n",
      "Iteration 154, loss = 0.02199637\n",
      "Iteration 35, loss = 0.01050374\n",
      "Iteration 65, loss = 0.03109780\n",
      "Iteration 29, loss = 0.01242549\n",
      "Iteration 60, loss = 0.06620638\n",
      "Iteration 23, loss = 0.01414781\n",
      "Iteration 163, loss = 0.02274819\n",
      "Iteration 63, loss = 0.03004670\n",
      "Iteration 168, loss = 0.01980216\n",
      "Iteration 59, loss = 0.03404426\n",
      "Iteration 155, loss = 0.02189352\n",
      "Iteration 164, loss = 0.02256822\n",
      "Iteration 22, loss = 0.04629931\n",
      "Iteration 41, loss = 0.16612983\n",
      "Iteration 66, loss = 0.03071854\n",
      "Iteration 36, loss = 0.01037119\n",
      "Iteration 169, loss = 0.01974307\n",
      "Iteration 24, loss = 0.01361008\n",
      "Iteration 61, loss = 0.05870664\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 30, loss = 0.01221264\n",
      "[CV 2/3; 29/36] END alpha=0.01, hidden_layer_sizes=(100,), learning_rate_init=0.01;, score=0.891 total time= 3.0min\n",
      "Iteration 156, loss = 0.02182832\n",
      "Iteration 165, loss = 0.02244805\n",
      "[CV 2/3; 32/36] START alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate_init=0.01\n",
      "Iteration 64, loss = 0.02984338\n",
      "Iteration 170, loss = 0.01974310\n",
      "Iteration 60, loss = 0.03374978\n",
      "Iteration 23, loss = 0.04255858\n",
      "Iteration 157, loss = 0.02169403\n",
      "Iteration 42, loss = 0.20404579\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 166, loss = 0.02222630\n",
      "Iteration 67, loss = 0.03054159\n",
      "Iteration 37, loss = 0.01030550\n",
      "[CV 3/3; 30/36] END alpha=0.01, hidden_layer_sizes=(100,), learning_rate_init=0.1;, score=0.886 total time= 2.1min\n",
      "Iteration 25, loss = 0.01326422\n",
      "[CV 3/3; 32/36] START alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate_init=0.01\n",
      "Iteration 31, loss = 0.01206948\n",
      "Iteration 171, loss = 0.01967132\n",
      "Iteration 65, loss = 0.02952625\n",
      "Iteration 167, loss = 0.02231176\n",
      "Iteration 158, loss = 0.02161931\n",
      "Iteration 61, loss = 0.03364219\n",
      "Iteration 172, loss = 0.01961044\n",
      "Iteration 24, loss = 0.03901285\n",
      "Iteration 1, loss = 0.43997658\n",
      "Iteration 68, loss = 0.03036378\n",
      "Iteration 38, loss = 0.01022349\n",
      "Iteration 26, loss = 0.01292565\n",
      "Iteration 168, loss = 0.02234693\n",
      "Iteration 159, loss = 0.02174279\n",
      "Iteration 32, loss = 0.01192720\n",
      "Iteration 173, loss = 0.01958991\n",
      "Iteration 66, loss = 0.02925466\n",
      "Iteration 169, loss = 0.02227724\n",
      "Iteration 160, loss = 0.02159710\n",
      "Iteration 1, loss = 0.43715959\n",
      "Iteration 62, loss = 0.03307110\n",
      "Iteration 25, loss = 0.03604456\n",
      "Iteration 2, loss = 0.15960529\n",
      "Iteration 69, loss = 0.03004843\n",
      "Iteration 174, loss = 0.01955356\n",
      "Iteration 39, loss = 0.01011924\n",
      "Iteration 27, loss = 0.01271535\n",
      "Iteration 33, loss = 0.01172841\n",
      "Iteration 170, loss = 0.02218362\n",
      "Iteration 161, loss = 0.02146005\n",
      "Iteration 67, loss = 0.02915038\n",
      "Iteration 175, loss = 0.01945510\n",
      "Iteration 2, loss = 0.15485343\n",
      "Iteration 26, loss = 0.03358021\n",
      "Iteration 3, loss = 0.08864626\n",
      "Iteration 171, loss = 0.02216977\n",
      "Iteration 63, loss = 0.03273897\n",
      "Iteration 70, loss = 0.02980329\n",
      "Iteration 162, loss = 0.02144327\n",
      "Iteration 40, loss = 0.00996089\n",
      "Iteration 28, loss = 0.01241314\n",
      "Iteration 176, loss = 0.01943808\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 34, loss = 0.01168608\n",
      "[CV 1/3; 25/36] END alpha=0.01, hidden_layer_sizes=(50,), learning_rate_init=0.001;, score=0.901 total time= 5.3min\n",
      "[CV 1/3; 33/36] START alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate_init=0.1\n",
      "Iteration 172, loss = 0.02171512\n",
      "Iteration 68, loss = 0.02890944\n",
      "Iteration 163, loss = 0.02130475\n",
      "Iteration 3, loss = 0.08569623\n",
      "Iteration 27, loss = 0.03159778\n",
      "Iteration 71, loss = 0.02950692\n",
      "Iteration 4, loss = 0.06741117\n",
      "Iteration 64, loss = 0.03233566\n",
      "Iteration 41, loss = 0.00993263\n",
      "Iteration 29, loss = 0.01217690\n",
      "Iteration 173, loss = 0.02177955\n",
      "Iteration 164, loss = 0.02116380\n",
      "Iteration 35, loss = 0.01139079\n",
      "Iteration 69, loss = 0.02835756\n",
      "Iteration 4, loss = 0.05794673\n",
      "Iteration 174, loss = 0.02185246\n",
      "Iteration 72, loss = 0.02916687\n",
      "Iteration 28, loss = 0.02988245\n",
      "Iteration 5, loss = 0.05240760\n",
      "Iteration 165, loss = 0.02106755\n",
      "Iteration 42, loss = 0.00987495\n",
      "Iteration 30, loss = 0.01199108\n",
      "Iteration 65, loss = 0.03220217\n",
      "Iteration 1, loss = 1.19080409\n",
      "Iteration 175, loss = 0.02174826\n",
      "Iteration 36, loss = 0.01129099\n",
      "Iteration 166, loss = 0.02094741\n",
      "Iteration 70, loss = 0.02834794\n",
      "Iteration 5, loss = 0.04946168\n",
      "Iteration 73, loss = 0.02885532\n",
      "Iteration 29, loss = 0.02787993\n",
      "Iteration 6, loss = 0.03964850\n",
      "Iteration 176, loss = 0.02165943\n",
      "Iteration 43, loss = 0.00980812\n",
      "Iteration 31, loss = 0.01185155\n",
      "Iteration 167, loss = 0.02093337\n",
      "Iteration 66, loss = 0.03198680\n",
      "Iteration 2, loss = 0.48011421\n",
      "Iteration 37, loss = 0.01111144\n",
      "Iteration 177, loss = 0.02147580\n",
      "Iteration 71, loss = 0.02789880\n",
      "Iteration 6, loss = 0.04566010\n",
      "Iteration 74, loss = 0.02871351\n",
      "Iteration 168, loss = 0.02090377\n",
      "Iteration 30, loss = 0.02642706\n",
      "Iteration 7, loss = 0.03928202\n",
      "Iteration 44, loss = 0.00970972\n",
      "Iteration 32, loss = 0.01174357\n",
      "Iteration 178, loss = 0.02143533\n",
      "Iteration 3, loss = 0.35705421\n",
      "Iteration 67, loss = 0.03147382\n",
      "Iteration 38, loss = 0.01098915\n",
      "Iteration 169, loss = 0.02086072\n",
      "Iteration 7, loss = 0.04989187\n",
      "Iteration 75, loss = 0.02855338\n",
      "Iteration 72, loss = 0.02780429\n",
      "Iteration 179, loss = 0.02142303\n",
      "Iteration 31, loss = 0.02476226\n",
      "Iteration 8, loss = 0.04750503\n",
      "Iteration 45, loss = 0.00968653\n",
      "Iteration 33, loss = 0.01156681\n",
      "Iteration 170, loss = 0.02089078\n",
      "Iteration 180, loss = 0.02135443\n",
      "Iteration 4, loss = 0.30275967\n",
      "Iteration 68, loss = 0.03136620\n",
      "Iteration 39, loss = 0.01091920\n",
      "Iteration 76, loss = 0.02820694\n",
      "Iteration 8, loss = 0.06214815\n",
      "Iteration 73, loss = 0.02772849\n",
      "Iteration 171, loss = 0.02087400\n",
      "Iteration 32, loss = 0.02360110\n",
      "Iteration 181, loss = 0.02113593\n",
      "Iteration 46, loss = 0.00955415\n",
      "Iteration 9, loss = 0.07051389\n",
      "Iteration 34, loss = 0.01135648\n",
      "Iteration 172, loss = 0.02054129\n",
      "Iteration 5, loss = 0.26447903\n",
      "Iteration 69, loss = 0.03099924\n",
      "Iteration 40, loss = 0.01080364\n",
      "Iteration 182, loss = 0.02130493\n",
      "Iteration 77, loss = 0.02794429\n",
      "Iteration 9, loss = 0.06723646\n",
      "Iteration 74, loss = 0.02741717\n",
      "Iteration 33, loss = 0.02218432\n",
      "Iteration 47, loss = 0.00948166\n",
      "Iteration 10, loss = 0.06008788\n",
      "Iteration 173, loss = 0.02053947\n",
      "Iteration 35, loss = 0.01124425\n",
      "Iteration 183, loss = 0.02113928\n",
      "Iteration 6, loss = 0.24267650\n",
      "Iteration 70, loss = 0.03059555\n",
      "Iteration 41, loss = 0.01079423\n",
      "Iteration 78, loss = 0.02771704\n",
      "Iteration 174, loss = 0.02041950\n",
      "Iteration 10, loss = 0.06618347\n",
      "Iteration 184, loss = 0.02119923\n",
      "Iteration 34, loss = 0.02123291\n",
      "Iteration 75, loss = 0.02711138\n",
      "Iteration 48, loss = 0.00941759\n",
      "Iteration 11, loss = 0.06198293\n",
      "Iteration 36, loss = 0.01112299\n",
      "Iteration 175, loss = 0.02046081\n",
      "Iteration 185, loss = 0.02110933\n",
      "Iteration 7, loss = 0.24347293\n",
      "Iteration 79, loss = 0.02770103\n",
      "Iteration 42, loss = 0.01065781\n",
      "Iteration 71, loss = 0.03034954\n",
      "Iteration 11, loss = 0.05508889\n",
      "Iteration 35, loss = 0.02039533\n",
      "Iteration 49, loss = 0.00931475\n",
      "Iteration 176, loss = 0.02029553\n",
      "Iteration 76, loss = 0.02690048\n",
      "Iteration 186, loss = 0.02102734\n",
      "Iteration 12, loss = 0.06161473\n",
      "Iteration 37, loss = 0.01101576\n",
      "Iteration 187, loss = 0.02109538\n",
      "Iteration 177, loss = 0.02017320\n",
      "Iteration 8, loss = 0.25390788\n",
      "Iteration 80, loss = 0.02740370\n",
      "Iteration 12, loss = 0.04895940\n",
      "Iteration 43, loss = 0.01055766\n",
      "Iteration 72, loss = 0.03003295\n",
      "Iteration 36, loss = 0.01994922\n",
      "Iteration 50, loss = 0.00927010\n",
      "Iteration 77, loss = 0.02689038\n",
      "Iteration 13, loss = 0.05396473\n",
      "Iteration 38, loss = 0.01093446\n",
      "Iteration 188, loss = 0.02116722\n",
      "Iteration 178, loss = 0.02015620\n",
      "Iteration 81, loss = 0.02721376\n",
      "Iteration 9, loss = 0.24051897\n",
      "Iteration 13, loss = 0.04945834\n",
      "Iteration 189, loss = 0.02082724\n",
      "Iteration 44, loss = 0.01046749\n",
      "Iteration 37, loss = 0.01884336\n",
      "Iteration 73, loss = 0.02997739\n",
      "Iteration 179, loss = 0.02024795\n",
      "Iteration 51, loss = 0.00926374\n",
      "Iteration 78, loss = 0.02661571\n",
      "Iteration 14, loss = 0.05047508\n",
      "Iteration 39, loss = 0.01079267\n",
      "Iteration 190, loss = 0.02089344\n",
      "Iteration 180, loss = 0.02014617\n",
      "Iteration 82, loss = 0.02699353\n",
      "Iteration 10, loss = 0.22921172\n",
      "Iteration 14, loss = 0.04811489\n",
      "Iteration 38, loss = 0.01815329\n",
      "Iteration 45, loss = 0.01053704\n",
      "Iteration 52, loss = 0.00915818\n",
      "Iteration 74, loss = 0.02960757\n",
      "Iteration 191, loss = 0.02080424\n",
      "Iteration 15, loss = 0.04646193\n",
      "Iteration 181, loss = 0.02008662\n",
      "Iteration 79, loss = 0.02638817\n",
      "Iteration 40, loss = 0.01074817\n",
      "Iteration 83, loss = 0.02691426\n",
      "Iteration 192, loss = 0.02055279\n",
      "Iteration 11, loss = 0.21479702\n",
      "Iteration 15, loss = 0.04711561\n",
      "Iteration 182, loss = 0.01993303\n",
      "Iteration 39, loss = 0.01716395\n",
      "Iteration 53, loss = 0.00918225\n",
      "Iteration 46, loss = 0.01029365\n",
      "Iteration 75, loss = 0.02941574\n",
      "Iteration 16, loss = 0.04180124\n",
      "Iteration 41, loss = 0.01084938\n",
      "Iteration 193, loss = 0.02065806\n",
      "Iteration 80, loss = 0.02605415\n",
      "Iteration 183, loss = 0.01980061\n",
      "Iteration 84, loss = 0.02651678\n",
      "Iteration 16, loss = 0.04687298\n",
      "Iteration 40, loss = 0.01607715\n",
      "Iteration 12, loss = 0.22438697\n",
      "Iteration 194, loss = 0.02048542\n",
      "Iteration 54, loss = 0.00907306\n",
      "Iteration 47, loss = 0.01020139\n",
      "Iteration 184, loss = 0.01985011\n",
      "Iteration 17, loss = 0.03914311\n",
      "Iteration 76, loss = 0.02922921\n",
      "Iteration 42, loss = 0.01062001\n",
      "Iteration 81, loss = 0.02587775\n",
      "Iteration 195, loss = 0.02034416\n",
      "Iteration 85, loss = 0.02649066\n",
      "Iteration 185, loss = 0.01969679\n",
      "Iteration 17, loss = 0.04848668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 0.01569679\n",
      "Iteration 13, loss = 0.21265411\n",
      "Iteration 55, loss = 0.00896337\n",
      "[CV 3/3; 32/36] END alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate_init=0.01;, score=0.902 total time=  52.9s\n",
      "[CV 2/3; 33/36] START alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate_init=0.1\n",
      "Iteration 196, loss = 0.02045087\n",
      "Iteration 48, loss = 0.01011205\n",
      "Iteration 18, loss = 0.03582240\n",
      "Iteration 43, loss = 0.01047441\n",
      "Iteration 77, loss = 0.02894262\n",
      "Iteration 82, loss = 0.02577006\n",
      "Iteration 186, loss = 0.01993936\n",
      "Iteration 86, loss = 0.02620268\n",
      "Iteration 197, loss = 0.02066671\n",
      "Iteration 42, loss = 0.01501172\n",
      "Iteration 56, loss = 0.00894382\n",
      "Iteration 14, loss = 0.20528947\n",
      "Iteration 187, loss = 0.01978265\n",
      "Iteration 49, loss = 0.01001066\n",
      "Iteration 19, loss = 0.03301455\n",
      "Iteration 44, loss = 0.01045326\n",
      "Iteration 198, loss = 0.02036260\n",
      "Iteration 83, loss = 0.02609674\n",
      "Iteration 78, loss = 0.02881651\n",
      "Iteration 87, loss = 0.02598649\n",
      "Iteration 1, loss = 0.83193467\n",
      "Iteration 188, loss = 0.01961693\n",
      "Iteration 43, loss = 0.01493372\n",
      "Iteration 199, loss = 0.02027849\n",
      "Iteration 57, loss = 0.00892146\n",
      "Iteration 15, loss = 0.19704637\n",
      "Iteration 20, loss = 0.03063426\n",
      "Iteration 50, loss = 0.01003125\n",
      "Iteration 45, loss = 0.01072304\n",
      "Iteration 189, loss = 0.01953561\n",
      "Iteration 84, loss = 0.02533212\n",
      "Iteration 79, loss = 0.02858740\n",
      "Iteration 200, loss = 0.02033428\n",
      "Iteration 88, loss = 0.02580900\n",
      "Iteration 2, loss = 0.41827677\n",
      "Iteration 44, loss = 0.01439263\n",
      "Iteration 58, loss = 0.00885242\n",
      "Iteration 190, loss = 0.01953968\n",
      "Iteration 16, loss = 0.19980395\n",
      "Iteration 201, loss = 0.02026875\n",
      "Iteration 21, loss = 0.02817338\n",
      "Iteration 46, loss = 0.01036447\n",
      "Iteration 51, loss = 0.01004269\n",
      "Iteration 85, loss = 0.02522066\n",
      "Iteration 89, loss = 0.02571065\n",
      "Iteration 3, loss = 0.30088175\n",
      "Iteration 80, loss = 0.02812152\n",
      "Iteration 191, loss = 0.01952274\n",
      "Iteration 202, loss = 0.02017607\n",
      "Iteration 45, loss = 0.01389577\n",
      "Iteration 59, loss = 0.00879882\n",
      "Iteration 22, loss = 0.02636841\n",
      "Iteration 17, loss = 0.22825138\n",
      "Iteration 47, loss = 0.01006337\n",
      "Iteration 192, loss = 0.01938796\n",
      "Iteration 52, loss = 0.00989592\n",
      "Iteration 203, loss = 0.02013767\n",
      "Iteration 90, loss = 0.02543849\n",
      "Iteration 86, loss = 0.02499036\n",
      "Iteration 4, loss = 0.24475391\n",
      "Iteration 81, loss = 0.02812972\n",
      "Iteration 46, loss = 0.01478112\n",
      "Iteration 60, loss = 0.00874436\n",
      "Iteration 204, loss = 0.01997502\n",
      "Iteration 193, loss = 0.01945385\n",
      "Iteration 23, loss = 0.02473145\n",
      "Iteration 18, loss = 0.21653266\n",
      "Iteration 48, loss = 0.01008020\n",
      "Iteration 53, loss = 0.00973590\n",
      "Iteration 91, loss = 0.02519704\n",
      "Iteration 205, loss = 0.01983437\n",
      "Iteration 5, loss = 0.24305657\n",
      "Iteration 194, loss = 0.01932725\n",
      "Iteration 87, loss = 0.02490840\n",
      "Iteration 47, loss = 0.12983060\n",
      "Iteration 82, loss = 0.02767570\n",
      "Iteration 61, loss = 0.00869953\n",
      "Iteration 24, loss = 0.02342497\n",
      "Iteration 206, loss = 0.01985692\n",
      "Iteration 49, loss = 0.00997892\n",
      "Iteration 195, loss = 0.01923616\n",
      "Iteration 19, loss = 0.19260720\n",
      "Iteration 54, loss = 0.00968983\n",
      "Iteration 92, loss = 0.02509155\n",
      "Iteration 6, loss = 0.23446305\n",
      "Iteration 88, loss = 0.02469106\n",
      "Iteration 207, loss = 0.02005273\n",
      "Iteration 48, loss = 0.16265485\n",
      "Iteration 196, loss = 0.01953898\n",
      "Iteration 62, loss = 0.00867170\n",
      "Iteration 83, loss = 0.02773193\n",
      "Iteration 25, loss = 0.02279753\n",
      "Iteration 50, loss = 0.00991937\n",
      "Iteration 20, loss = 0.19152277\n",
      "Iteration 208, loss = 0.01993339\n",
      "Iteration 93, loss = 0.02496068\n",
      "Iteration 55, loss = 0.00959450\n",
      "Iteration 197, loss = 0.01964639\n",
      "Iteration 7, loss = 0.23201808\n",
      "Iteration 89, loss = 0.02456755\n",
      "Iteration 49, loss = 0.07997910\n",
      "Iteration 63, loss = 0.00861054\n",
      "Iteration 209, loss = 0.01995791\n",
      "Iteration 84, loss = 0.02773014\n",
      "Iteration 198, loss = 0.01946706\n",
      "Iteration 26, loss = 0.02138681\n",
      "Iteration 51, loss = 0.00988104\n",
      "Iteration 21, loss = 0.20084394\n",
      "Iteration 94, loss = 0.02480248\n",
      "Iteration 56, loss = 0.00955424\n",
      "Iteration 8, loss = 0.22169982\n",
      "Iteration 210, loss = 0.01972578\n",
      "Iteration 90, loss = 0.02440171\n",
      "Iteration 199, loss = 0.01927122\n",
      "Iteration 50, loss = 0.06847670\n",
      "Iteration 64, loss = 0.00856346\n",
      "Iteration 85, loss = 0.02733234\n",
      "Iteration 211, loss = 0.01979651\n",
      "Iteration 27, loss = 0.02015395\n",
      "Iteration 52, loss = 0.00973061\n",
      "Iteration 95, loss = 0.02475582\n",
      "Iteration 22, loss = 0.18368548\n",
      "Iteration 200, loss = 0.01924949\n",
      "Iteration 9, loss = 0.23145170\n",
      "Iteration 57, loss = 0.00947005\n",
      "Iteration 91, loss = 0.02410938\n",
      "Iteration 51, loss = 0.06237617\n",
      "Iteration 212, loss = 0.02002931\n",
      "Iteration 65, loss = 0.00861663\n",
      "Iteration 201, loss = 0.01905294\n",
      "Iteration 28, loss = 0.04601830\n",
      "Iteration 86, loss = 0.02715007\n",
      "Iteration 53, loss = 0.00972252\n",
      "Iteration 96, loss = 0.02458250\n",
      "Iteration 213, loss = 0.01992347\n",
      "Iteration 23, loss = 0.17438131\n",
      "Iteration 10, loss = 0.21948372\n",
      "Iteration 58, loss = 0.00952052\n",
      "Iteration 202, loss = 0.01892073\n",
      "Iteration 52, loss = 0.05542943\n",
      "Iteration 92, loss = 0.02407951\n",
      "Iteration 66, loss = 0.00843776\n",
      "Iteration 214, loss = 0.01971040\n",
      "Iteration 29, loss = 0.14021958\n",
      "Iteration 97, loss = 0.02431346\n",
      "Iteration 54, loss = 0.00976560\n",
      "Iteration 87, loss = 0.02700425\n",
      "Iteration 203, loss = 0.01888234\n",
      "Iteration 24, loss = 0.18012790\n",
      "Iteration 11, loss = 0.20286808\n",
      "Iteration 215, loss = 0.01941041\n",
      "Iteration 59, loss = 0.00944943\n",
      "Iteration 53, loss = 0.05077344\n",
      "Iteration 93, loss = 0.02420608\n",
      "Iteration 67, loss = 0.00851443\n",
      "Iteration 204, loss = 0.01877614\n",
      "Iteration 30, loss = 0.09410060\n",
      "Iteration 216, loss = 0.01928203\n",
      "Iteration 98, loss = 0.02423486\n",
      "Iteration 55, loss = 0.00957968\n",
      "Iteration 88, loss = 0.02674262\n",
      "Iteration 12, loss = 0.19660022\n",
      "Iteration 25, loss = 0.18697023\n",
      "Iteration 205, loss = 0.01888275\n",
      "Iteration 54, loss = 0.04652771\n",
      "Iteration 60, loss = 0.00937216\n",
      "Iteration 68, loss = 0.00839175\n",
      "Iteration 217, loss = 0.01942810\n",
      "Iteration 94, loss = 0.02384075\n",
      "Iteration 31, loss = 0.06958682\n",
      "Iteration 99, loss = 0.02408666\n",
      "Iteration 56, loss = 0.00948556\n",
      "Iteration 206, loss = 0.01882481\n",
      "Iteration 218, loss = 0.01935595\n",
      "Iteration 13, loss = 0.21973282\n",
      "Iteration 89, loss = 0.02654711\n",
      "Iteration 26, loss = 0.18029907\n",
      "Iteration 55, loss = 0.04311603\n",
      "Iteration 69, loss = 0.00828920\n",
      "Iteration 61, loss = 0.00928601\n",
      "Iteration 207, loss = 0.01875858\n",
      "Iteration 95, loss = 0.02366831\n",
      "Iteration 219, loss = 0.01925107\n",
      "Iteration 32, loss = 0.05681986\n",
      "Iteration 100, loss = 0.02391950\n",
      "Iteration 57, loss = 0.00939183\n",
      "Iteration 14, loss = 0.24356936\n",
      "Iteration 208, loss = 0.01866821\n",
      "Iteration 90, loss = 0.02640265\n",
      "Iteration 220, loss = 0.01929466\n",
      "Iteration 27, loss = 0.17783882\n",
      "Iteration 56, loss = 0.04014240\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 70, loss = 0.00832676\n",
      "[CV 1/3; 32/36] END alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate_init=0.01;, score=0.881 total time= 2.8min\n",
      "Iteration 62, loss = 0.00924187\n",
      "[CV 3/3; 33/36] START alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate_init=0.1\n",
      "Iteration 96, loss = 0.02380847\n",
      "Iteration 101, loss = 0.02376874\n",
      "Iteration 33, loss = 0.05000934\n",
      "Iteration 221, loss = 0.01926981\n",
      "Iteration 209, loss = 0.01861726\n",
      "Iteration 58, loss = 0.00949431\n",
      "Iteration 15, loss = 0.23841657\n",
      "Iteration 91, loss = 0.02608341\n",
      "Iteration 28, loss = 0.19462189\n",
      "Iteration 71, loss = 0.00834320\n",
      "Iteration 222, loss = 0.01958011\n",
      "Iteration 210, loss = 0.01859058\n",
      "Iteration 63, loss = 0.00916418\n",
      "Iteration 102, loss = 0.02368168\n",
      "Iteration 97, loss = 0.02350258\n",
      "Iteration 34, loss = 0.04620996\n",
      "Iteration 59, loss = 0.00937725\n",
      "Iteration 223, loss = 0.01918281\n",
      "Iteration 211, loss = 0.01847944\n",
      "Iteration 16, loss = 0.23903764\n",
      "Iteration 1, loss = 0.91054247\n",
      "Iteration 92, loss = 0.02596374\n",
      "Iteration 72, loss = 0.00834481\n",
      "Iteration 29, loss = 0.21063061\n",
      "Iteration 64, loss = 0.00917158\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 224, loss = 0.01903589\n",
      "Iteration 103, loss = 0.02352672\n",
      "[CV 2/3; 31/36] END alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate_init=0.001;, score=0.895 total time= 3.4min\n",
      "Iteration 212, loss = 0.01849910\n",
      "Iteration 35, loss = 0.04249649\n",
      "Iteration 98, loss = 0.02317695\n",
      "[CV 1/3; 34/36] START alpha=0.01, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.001\n",
      "Iteration 60, loss = 0.00935258\n",
      "Iteration 17, loss = 0.20634851\n",
      "Iteration 2, loss = 0.45125551\n",
      "Iteration 225, loss = 0.01906678\n",
      "Iteration 73, loss = 0.00811608\n",
      "Iteration 213, loss = 0.01847017\n",
      "Iteration 93, loss = 0.02589735\n",
      "Iteration 30, loss = 0.20355036\n",
      "Iteration 104, loss = 0.02340786\n",
      "Iteration 36, loss = 0.03944581\n",
      "Iteration 226, loss = 0.01894396\n",
      "Iteration 99, loss = 0.02298458\n",
      "Iteration 61, loss = 0.00933238\n",
      "Iteration 214, loss = 0.01843586\n",
      "Iteration 18, loss = 0.24289547\n",
      "Iteration 3, loss = 0.33503271\n",
      "Iteration 74, loss = 0.00817797\n",
      "Iteration 227, loss = 0.01904327\n",
      "Iteration 31, loss = 0.18240768\n",
      "Iteration 94, loss = 0.02569956\n",
      "Iteration 105, loss = 0.02327978\n",
      "Iteration 215, loss = 0.01835293\n",
      "Iteration 37, loss = 0.03644275\n",
      "Iteration 100, loss = 0.02287073\n",
      "Iteration 62, loss = 0.00917165\n",
      "Iteration 1, loss = 0.62730959\n",
      "Iteration 228, loss = 0.01907235\n",
      "Iteration 19, loss = 0.22719616\n",
      "Iteration 4, loss = 0.26353660\n",
      "Iteration 216, loss = 0.01862850\n",
      "Iteration 75, loss = 0.00813460\n",
      "Iteration 106, loss = 0.02312722\n",
      "Iteration 32, loss = 0.17976958\n",
      "Iteration 229, loss = 0.01911548\n",
      "Iteration 95, loss = 0.02552637\n",
      "Iteration 38, loss = 0.03399314\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 101, loss = 0.02270962\n",
      "[CV 2/3; 32/36] END alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate_init=0.01;, score=0.886 total time= 1.9min\n",
      "Iteration 63, loss = 0.00913339\n",
      "Iteration 217, loss = 0.01851341\n",
      "[CV 2/3; 34/36] START alpha=0.01, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.001\n",
      "Iteration 20, loss = 0.20366475\n",
      "Iteration 5, loss = 0.23261989\n",
      "Iteration 230, loss = 0.01894210\n",
      "Iteration 76, loss = 0.00804940\n",
      "Iteration 2, loss = 0.30772100\n",
      "Iteration 107, loss = 0.02296966\n",
      "Iteration 218, loss = 0.01826300\n",
      "Iteration 33, loss = 0.16522903\n",
      "Iteration 96, loss = 0.02544636\n",
      "Iteration 231, loss = 0.01896356\n",
      "Iteration 64, loss = 0.00906650\n",
      "Iteration 102, loss = 0.02285937\n",
      "Iteration 21, loss = 0.18746876\n",
      "Iteration 6, loss = 0.23560190\n",
      "Iteration 219, loss = 0.01816234\n",
      "Iteration 77, loss = 0.00798073\n",
      "Iteration 232, loss = 0.01906783\n",
      "Iteration 108, loss = 0.02304080\n",
      "Iteration 34, loss = 0.15099562\n",
      "Iteration 3, loss = 0.13924191\n",
      "Iteration 97, loss = 0.02523656\n",
      "Iteration 220, loss = 0.01811866\n",
      "Iteration 65, loss = 0.00902829\n",
      "Iteration 103, loss = 0.02275231\n",
      "Iteration 22, loss = 0.21129239\n",
      "Iteration 233, loss = 0.01904377\n",
      "Iteration 7, loss = 0.25013358\n",
      "Iteration 1, loss = 0.63140995\n",
      "Iteration 78, loss = 0.00793856\n",
      "Iteration 109, loss = 0.02281504\n",
      "Iteration 221, loss = 0.01816581\n",
      "Iteration 234, loss = 0.01882084\n",
      "Iteration 35, loss = 0.15429674\n",
      "Iteration 66, loss = 0.00906563\n",
      "Iteration 98, loss = 0.02528718\n",
      "Iteration 104, loss = 0.02279213\n",
      "Iteration 23, loss = 0.21239187\n",
      "Iteration 8, loss = 0.22558348\n",
      "Iteration 222, loss = 0.01831845\n",
      "Iteration 4, loss = 0.07198725\n",
      "Iteration 235, loss = 0.01873833\n",
      "Iteration 79, loss = 0.00798002\n",
      "Iteration 110, loss = 0.02265333\n",
      "Iteration 2, loss = 0.32309802\n",
      "Iteration 36, loss = 0.17289393\n",
      "Iteration 223, loss = 0.01825177\n",
      "Iteration 236, loss = 0.01861382\n",
      "Iteration 67, loss = 0.00893370\n",
      "Iteration 9, loss = 0.20586199\n",
      "Iteration 24, loss = 0.21116175\n",
      "Iteration 105, loss = 0.02258633\n",
      "Iteration 99, loss = 0.02501090\n",
      "Iteration 80, loss = 0.00793153\n",
      "Iteration 111, loss = 0.02236028\n",
      "Iteration 224, loss = 0.01811753\n",
      "Iteration 237, loss = 0.01859004\n",
      "Iteration 5, loss = 0.04454516\n",
      "Iteration 37, loss = 0.17537780\n",
      "Iteration 68, loss = 0.00893773\n",
      "Iteration 10, loss = 0.21936806\n",
      "Iteration 25, loss = 0.21748001\n",
      "Iteration 3, loss = 0.14359049\n",
      "Iteration 225, loss = 0.01805371\n",
      "Iteration 238, loss = 0.01852400\n",
      "Iteration 106, loss = 0.02227666\n",
      "Iteration 100, loss = 0.02484113\n",
      "Iteration 81, loss = 0.00793011\n",
      "Iteration 112, loss = 0.02234360\n",
      "Iteration 239, loss = 0.01876585\n",
      "Iteration 226, loss = 0.01799931\n",
      "Iteration 38, loss = 0.18238777\n",
      "Iteration 69, loss = 0.00889131\n",
      "Iteration 11, loss = 0.21978919\n",
      "Iteration 26, loss = 0.22215306\n",
      "Iteration 6, loss = 0.03254309\n",
      "Iteration 107, loss = 0.02204937\n",
      "Iteration 101, loss = 0.02468825\n",
      "Iteration 82, loss = 0.00796482\n",
      "Iteration 240, loss = 0.01864729\n",
      "Iteration 113, loss = 0.02216567\n",
      "Iteration 227, loss = 0.01794232\n",
      "Iteration 4, loss = 0.08018376\n",
      "Iteration 241, loss = 0.01849880\n",
      "Iteration 39, loss = 0.21405668\n",
      "Iteration 70, loss = 0.00882266\n",
      "Iteration 228, loss = 0.01827093\n",
      "Iteration 12, loss = 0.20981513\n",
      "Iteration 27, loss = 0.19952149\n",
      "Iteration 108, loss = 0.02224315\n",
      "Iteration 83, loss = 0.00788619\n",
      "Iteration 114, loss = 0.02224378\n",
      "Iteration 102, loss = 0.02452666\n",
      "Iteration 7, loss = 0.02354327\n",
      "Iteration 242, loss = 0.01855178\n",
      "Iteration 229, loss = 0.01826109\n",
      "Iteration 5, loss = 0.05059238\n",
      "Iteration 71, loss = 0.00875137\n",
      "Iteration 40, loss = 0.19235476\n",
      "Iteration 28, loss = 0.18686195\n",
      "Iteration 13, loss = 0.19713760\n",
      "Iteration 243, loss = 0.01882885\n",
      "Iteration 109, loss = 0.02195681\n",
      "Iteration 84, loss = 0.00775366\n",
      "Iteration 115, loss = 0.02219424\n",
      "Iteration 230, loss = 0.01817583\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/3; 25/36] END alpha=0.01, hidden_layer_sizes=(50,), learning_rate_init=0.001;, score=0.905 total time= 7.3min\n",
      "Iteration 103, loss = 0.02435437\n",
      "[CV 3/3; 34/36] START alpha=0.01, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.001\n",
      "Iteration 244, loss = 0.01887827\n",
      "Iteration 8, loss = 0.01797783\n",
      "Iteration 72, loss = 0.00891302\n",
      "Iteration 29, loss = 0.24103364\n",
      "Iteration 14, loss = 0.19516602\n",
      "Iteration 41, loss = 0.17576211\n",
      "Iteration 110, loss = 0.02191623\n",
      "Iteration 85, loss = 0.00782333\n",
      "Iteration 116, loss = 0.02195587\n",
      "Iteration 6, loss = 0.03972415\n",
      "Iteration 245, loss = 0.01890678\n",
      "Iteration 104, loss = 0.02429632\n",
      "Iteration 246, loss = 0.01859694\n",
      "Iteration 73, loss = 0.00868870\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 0.18626128\n",
      "Iteration 30, loss = 0.24894546\n",
      "Iteration 42, loss = 0.16139349\n",
      "[CV 3/3; 31/36] END alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate_init=0.001;, score=0.907 total time= 3.7min\n",
      "Iteration 117, loss = 0.02177196\n",
      "Iteration 86, loss = 0.00776542\n",
      "[CV 1/3; 35/36] START alpha=0.01, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.01\n",
      "Iteration 111, loss = 0.02164111\n",
      "Iteration 9, loss = 0.01529394\n",
      "Iteration 247, loss = 0.01839818\n",
      "Iteration 1, loss = 0.63151294\n",
      "Iteration 105, loss = 0.02408375\n",
      "Iteration 7, loss = 0.02956810\n",
      "Iteration 16, loss = 0.19423735\n",
      "Iteration 31, loss = 0.22001301\n",
      "Iteration 248, loss = 0.01831792\n",
      "Iteration 43, loss = 0.18255492\n",
      "Iteration 118, loss = 0.02163088\n",
      "Iteration 87, loss = 0.00843013\n",
      "Iteration 112, loss = 0.02147559\n",
      "Iteration 10, loss = 0.01368486\n",
      "Iteration 249, loss = 0.01835169\n",
      "Iteration 106, loss = 0.02390235\n",
      "Iteration 17, loss = 0.18483118\n",
      "Iteration 32, loss = 0.22217096\n",
      "Iteration 2, loss = 0.32477044\n",
      "Iteration 8, loss = 0.02524136\n",
      "Iteration 44, loss = 0.19979073\n",
      "Iteration 119, loss = 0.02194533\n",
      "Iteration 88, loss = 0.00779796\n",
      "Iteration 250, loss = 0.01830594\n",
      "Iteration 113, loss = 0.02187324\n",
      "Iteration 1, loss = 0.43887398\n",
      "Iteration 107, loss = 0.02397451\n",
      "Iteration 18, loss = 0.19870045\n",
      "Iteration 251, loss = 0.01918632\n",
      "Iteration 33, loss = 0.21821592\n",
      "Iteration 11, loss = 0.01268888\n",
      "Iteration 120, loss = 0.02159012\n",
      "Iteration 89, loss = 0.00768740\n",
      "Iteration 45, loss = 0.18466892\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/3; 33/36] END alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate_init=0.1;, score=0.883 total time= 2.4min\n",
      "Iteration 114, loss = 0.02181922\n",
      "Iteration 3, loss = 0.14854411\n",
      "[CV 2/3; 35/36] START alpha=0.01, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.01\n",
      "Iteration 9, loss = 0.02116555\n",
      "Iteration 252, loss = 0.01860088\n",
      "Iteration 19, loss = 0.20931106\n",
      "Iteration 2, loss = 0.14569639\n",
      "Iteration 108, loss = 0.02427180\n",
      "Iteration 34, loss = 0.22503680\n",
      "Iteration 121, loss = 0.02147679\n",
      "Iteration 253, loss = 0.01823346\n",
      "Iteration 90, loss = 0.00764707\n",
      "Iteration 115, loss = 0.02155340\n",
      "Iteration 12, loss = 0.01212569\n",
      "Iteration 254, loss = 0.01816486\n",
      "Iteration 20, loss = 0.19488381\n",
      "Iteration 10, loss = 0.01778003\n",
      "Iteration 4, loss = 0.07489096\n",
      "Iteration 35, loss = 0.21435612\n",
      "Iteration 109, loss = 0.02371963\n",
      "Iteration 122, loss = 0.02153378\n",
      "Iteration 91, loss = 0.00774889\n",
      "Iteration 3, loss = 0.08623701\n",
      "Iteration 255, loss = 0.01801348\n",
      "Iteration 116, loss = 0.02118246\n",
      "Iteration 1, loss = 0.41985195\n",
      "Iteration 21, loss = 0.18248655\n",
      "Iteration 13, loss = 0.01161491\n",
      "Iteration 256, loss = 0.01796960\n",
      "Iteration 123, loss = 0.02133908\n",
      "Iteration 36, loss = 0.20292287\n",
      "Iteration 110, loss = 0.02335778\n",
      "Iteration 92, loss = 0.00784492\n",
      "Iteration 11, loss = 0.01559544\n",
      "Iteration 5, loss = 0.04563918\n",
      "Iteration 117, loss = 0.02122836\n",
      "Iteration 257, loss = 0.01799490\n",
      "Iteration 4, loss = 0.06393872\n",
      "Iteration 22, loss = 0.21573653\n",
      "Iteration 124, loss = 0.02124206\n",
      "Iteration 2, loss = 0.15232982\n",
      "Iteration 37, loss = 0.19008429\n",
      "Iteration 258, loss = 0.01837700\n",
      "Iteration 93, loss = 0.00764665\n",
      "Iteration 111, loss = 0.02384099\n",
      "Iteration 14, loss = 0.01123274\n",
      "Iteration 118, loss = 0.02120401\n",
      "Iteration 6, loss = 0.03129869\n",
      "Iteration 12, loss = 0.01417590\n",
      "Iteration 259, loss = 0.01838893\n",
      "Iteration 23, loss = 0.23616168\n",
      "Iteration 125, loss = 0.02106561\n",
      "Iteration 38, loss = 0.20776799\n",
      "Iteration 5, loss = 0.06081646\n",
      "Iteration 94, loss = 0.00755727\n",
      "Iteration 112, loss = 0.02351554\n",
      "Iteration 260, loss = 0.01792376\n",
      "Iteration 3, loss = 0.08136629\n",
      "Iteration 119, loss = 0.02091146\n",
      "Iteration 24, loss = 0.21284545\n",
      "Iteration 15, loss = 0.01100395\n",
      "Iteration 126, loss = 0.02100691\n",
      "Iteration 261, loss = 0.01794739\n",
      "Iteration 39, loss = 0.19725589\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/3; 33/36] END alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate_init=0.1;, score=0.873 total time= 2.0min\n",
      "Iteration 95, loss = 0.00747516\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 7, loss = 0.02411639\n",
      "Iteration 13, loss = 0.01317368\n",
      "[CV 3/3; 35/36] START alpha=0.01, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.01\n",
      "[CV 1/3; 31/36] END alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate_init=0.001;, score=0.910 total time= 4.7min\n",
      "[CV 1/3; 36/36] START alpha=0.01, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.1\n",
      "Iteration 113, loss = 0.02313955\n",
      "Iteration 262, loss = 0.01785218\n",
      "Iteration 6, loss = 0.06286884\n",
      "Iteration 120, loss = 0.02079174\n",
      "Iteration 25, loss = 0.18896327\n",
      "Iteration 4, loss = 0.05840074\n",
      "Iteration 127, loss = 0.02087739\n",
      "Iteration 263, loss = 0.01787077\n",
      "Iteration 16, loss = 0.01078164\n",
      "Iteration 114, loss = 0.02310739\n",
      "Iteration 14, loss = 0.01258992\n",
      "Iteration 8, loss = 0.01959139\n",
      "Iteration 121, loss = 0.02078020\n",
      "Iteration 264, loss = 0.01774977\n",
      "Iteration 26, loss = 0.18166590\n",
      "Iteration 128, loss = 0.02076278\n",
      "Iteration 7, loss = 0.05262636\n",
      "Iteration 5, loss = 0.05253335\n",
      "Iteration 1, loss = 0.42882655\n",
      "Iteration 265, loss = 0.01780988\n",
      "Iteration 1, loss = 1.61663105\n",
      "Iteration 115, loss = 0.02290186\n",
      "Iteration 17, loss = 0.01052458\n",
      "Iteration 122, loss = 0.02058337\n",
      "Iteration 27, loss = 0.18905576\n",
      "Iteration 129, loss = 0.02080064\n",
      "Iteration 266, loss = 0.01775855\n",
      "Iteration 9, loss = 0.01669744\n",
      "Iteration 15, loss = 0.01215116\n",
      "Iteration 8, loss = 0.05611884\n",
      "Iteration 267, loss = 0.01776557\n",
      "Iteration 6, loss = 0.06277863Iteration 2, loss = 0.16275230\n",
      "\n",
      "Iteration 116, loss = 0.02262634\n",
      "Iteration 28, loss = 0.18160193\n",
      "Iteration 123, loss = 0.02053215\n",
      "Iteration 130, loss = 0.02063130\n",
      "Iteration 2, loss = 0.77017191\n",
      "Iteration 18, loss = 0.01040384\n",
      "Iteration 268, loss = 0.01770144\n",
      "Iteration 16, loss = 0.01166084\n",
      "Iteration 10, loss = 0.01522547\n",
      "Iteration 269, loss = 0.01760787\n",
      "Iteration 29, loss = 0.18217311\n",
      "Iteration 131, loss = 0.02052455\n",
      "Iteration 124, loss = 0.02048253\n",
      "Iteration 117, loss = 0.02247494\n",
      "Iteration 9, loss = 0.05298192\n",
      "Iteration 3, loss = 0.08119165\n",
      "Iteration 7, loss = 0.06109125\n",
      "Iteration 3, loss = 0.50518963\n",
      "Iteration 270, loss = 0.01763206\n",
      "Iteration 19, loss = 0.01041642\n",
      "Iteration 30, loss = 0.19193726\n",
      "Iteration 132, loss = 0.02048982\n",
      "Iteration 125, loss = 0.02052556\n",
      "Iteration 17, loss = 0.01160472\n",
      "Iteration 11, loss = 0.01398730\n",
      "Iteration 118, loss = 0.02251446\n",
      "Iteration 271, loss = 0.01767558\n",
      "Iteration 10, loss = 0.04640611\n",
      "Iteration 272, loss = 0.01784588\n",
      "Iteration 8, loss = 0.06504248\n",
      "Iteration 4, loss = 0.05550795\n",
      "Iteration 133, loss = 0.02039760\n",
      "Iteration 31, loss = 0.19152190\n",
      "Iteration 4, loss = 0.37657217\n",
      "Iteration 126, loss = 0.02030180\n",
      "Iteration 20, loss = 0.01014268\n",
      "Iteration 119, loss = 0.02267016\n",
      "Iteration 273, loss = 0.01766118\n",
      "Iteration 18, loss = 0.01145464\n",
      "Iteration 12, loss = 0.01331657\n",
      "Iteration 134, loss = 0.02044536\n",
      "Iteration 32, loss = 0.20432932\n",
      "Iteration 274, loss = 0.01767097\n",
      "Iteration 11, loss = 0.05201956\n",
      "Iteration 127, loss = 0.02020392\n",
      "Iteration 5, loss = 0.05372559\n",
      "Iteration 9, loss = 0.06973426\n",
      "Iteration 120, loss = 0.02247598\n",
      "Iteration 5, loss = 0.31525148\n",
      "Iteration 275, loss = 0.01754368\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/3; 25/36] END alpha=0.01, hidden_layer_sizes=(50,), learning_rate_init=0.001;, score=0.896 total time= 8.3min\n",
      "Iteration 21, loss = 0.00991988\n",
      "[CV 2/3; 36/36] START alpha=0.01, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.1\n",
      "Iteration 135, loss = 0.02017181\n",
      "Iteration 33, loss = 0.18581677\n",
      "Iteration 19, loss = 0.01112333\n",
      "Iteration 13, loss = 0.01255420\n",
      "Iteration 128, loss = 0.02014388\n",
      "Iteration 12, loss = 0.05141040\n",
      "Iteration 121, loss = 0.02216876\n",
      "Iteration 10, loss = 0.06718866\n",
      "Iteration 6, loss = 0.07415728\n",
      "Iteration 6, loss = 0.28168911\n",
      "Iteration 136, loss = 0.02025042\n",
      "Iteration 34, loss = 0.18225007\n",
      "Iteration 22, loss = 0.00975513\n",
      "Iteration 129, loss = 0.02012961\n",
      "Iteration 14, loss = 0.01223664\n",
      "Iteration 20, loss = 0.01076843\n",
      "Iteration 122, loss = 0.02236512\n",
      "Iteration 137, loss = 0.01996391\n",
      "Iteration 35, loss = 0.18196801\n",
      "Iteration 13, loss = 0.05765695\n",
      "Iteration 1, loss = 1.34390936\n",
      "Iteration 11, loss = 0.05402826\n",
      "Iteration 7, loss = 0.06283497\n",
      "Iteration 7, loss = 0.27102585\n",
      "Iteration 130, loss = 0.02002595\n",
      "Iteration 23, loss = 0.00960827\n",
      "Iteration 123, loss = 0.02221508\n",
      "Iteration 138, loss = 0.01990304\n",
      "Iteration 36, loss = 0.18403901\n",
      "Iteration 21, loss = 0.01054851\n",
      "Iteration 15, loss = 0.01178353\n",
      "Iteration 14, loss = 0.05764800\n",
      "Iteration 2, loss = 0.57751074\n",
      "Iteration 131, loss = 0.01988903\n",
      "Iteration 8, loss = 0.06694940\n",
      "Iteration 12, loss = 0.05372344\n",
      "Iteration 8, loss = 0.27167563\n",
      "Iteration 139, loss = 0.01992164\n",
      "Iteration 124, loss = 0.02235443\n",
      "Iteration 37, loss = 0.19330107\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/3; 33/36] END alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate_init=0.1;, score=0.901 total time= 1.9min\n",
      "Iteration 24, loss = 0.00982827\n",
      "[CV 3/3; 36/36] START alpha=0.01, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.1\n",
      "Iteration 22, loss = 0.01044564\n",
      "Iteration 16, loss = 0.01174211\n",
      "Iteration 132, loss = 0.01990946\n",
      "Iteration 140, loss = 0.02005249\n",
      "Iteration 15, loss = 0.05756241\n",
      "Iteration 3, loss = 0.44006276\n",
      "Iteration 125, loss = 0.02211389\n",
      "Iteration 9, loss = 0.07369765\n",
      "Iteration 13, loss = 0.06731825\n",
      "Iteration 9, loss = 0.25058550\n",
      "Iteration 25, loss = 0.00962246\n",
      "Iteration 133, loss = 0.01983092\n",
      "Iteration 141, loss = 0.01985138\n",
      "Iteration 23, loss = 0.01042332\n",
      "Iteration 17, loss = 0.01141897\n",
      "Iteration 126, loss = 0.02204017\n",
      "Iteration 1, loss = 1.30831234\n",
      "Iteration 16, loss = 0.05934622\n",
      "Iteration 4, loss = 0.33656455\n",
      "Iteration 14, loss = 0.05786740\n",
      "Iteration 134, loss = 0.01995657\n",
      "Iteration 10, loss = 0.05675106\n",
      "Iteration 10, loss = 0.24541224\n",
      "Iteration 142, loss = 0.01987981\n",
      "Iteration 26, loss = 0.00976945\n",
      "Iteration 127, loss = 0.02161646\n",
      "Iteration 18, loss = 0.01097668\n",
      "Iteration 24, loss = 0.01034467\n",
      "Iteration 135, loss = 0.02003066\n",
      "Iteration 2, loss = 0.59493168\n",
      "Iteration 143, loss = 0.01980222\n",
      "Iteration 17, loss = 0.05903672\n",
      "Iteration 5, loss = 0.29290369\n",
      "Iteration 11, loss = 0.05155268\n",
      "Iteration 15, loss = 0.05279544\n",
      "Iteration 11, loss = 0.22263313\n",
      "Iteration 128, loss = 0.02141588\n",
      "Iteration 27, loss = 0.00955094\n",
      "Iteration 136, loss = 0.01980649\n",
      "Iteration 144, loss = 0.01958121\n",
      "Iteration 19, loss = 0.01077238\n",
      "Iteration 25, loss = 0.01005492\n",
      "Iteration 3, loss = 0.43945275\n",
      "Iteration 18, loss = 0.05524047\n",
      "Iteration 6, loss = 0.26404662\n",
      "Iteration 129, loss = 0.02156201\n",
      "Iteration 12, loss = 0.05475710\n",
      "Iteration 16, loss = 0.05554062\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 0.21878453\n",
      "Iteration 145, loss = 0.01949688\n",
      "Iteration 137, loss = 0.01967375\n",
      "[CV 2/3; 35/36] END alpha=0.01, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.01;, score=0.888 total time= 1.2min\n",
      "Iteration 28, loss = 0.00956956\n",
      "Iteration 20, loss = 0.01069316\n",
      "Iteration 26, loss = 0.00986527\n",
      "Iteration 130, loss = 0.02134800\n",
      "Iteration 4, loss = 0.33511233\n",
      "Iteration 146, loss = 0.01938116\n",
      "Iteration 19, loss = 0.05165321\n",
      "Iteration 7, loss = 0.24079877\n",
      "Iteration 138, loss = 0.01955084\n",
      "Iteration 13, loss = 0.06765590\n",
      "Iteration 13, loss = 0.23331659\n",
      "Iteration 29, loss = 0.00910578\n",
      "Iteration 131, loss = 0.02125480\n",
      "Iteration 21, loss = 0.01052707\n",
      "Iteration 147, loss = 0.01933562\n",
      "Iteration 27, loss = 0.00975583\n",
      "Iteration 139, loss = 0.01945116\n",
      "Iteration 5, loss = 0.28599902\n",
      "Iteration 20, loss = 0.05132730\n",
      "Iteration 8, loss = 0.23340315\n",
      "Iteration 14, loss = 0.06832647\n",
      "Iteration 14, loss = 0.24104928\n",
      "Iteration 148, loss = 0.01933302\n",
      "Iteration 132, loss = 0.02132287\n",
      "Iteration 30, loss = 0.00898636\n",
      "Iteration 140, loss = 0.01927354\n",
      "Iteration 28, loss = 0.00977094\n",
      "Iteration 22, loss = 0.01056005\n",
      "Iteration 6, loss = 0.24807347\n",
      "Iteration 21, loss = 0.06402814\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 149, loss = 0.01925214\n",
      "Iteration 9, loss = 0.22030033\n",
      "[CV 1/3; 35/36] END alpha=0.01, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.01;, score=0.886 total time= 1.6min\n",
      "Iteration 133, loss = 0.02104386\n",
      "Iteration 15, loss = 0.06719485\n",
      "Iteration 141, loss = 0.01926755\n",
      "Iteration 15, loss = 0.22516514\n",
      "Iteration 31, loss = 0.00884347\n",
      "Iteration 23, loss = 0.01017962\n",
      "Iteration 29, loss = 0.00955411\n",
      "Iteration 150, loss = 0.01924110\n",
      "Iteration 7, loss = 0.23904115\n",
      "Iteration 142, loss = 0.01920748\n",
      "Iteration 134, loss = 0.02108554\n",
      "Iteration 10, loss = 0.22518580\n",
      "Iteration 16, loss = 0.05565269\n",
      "Iteration 16, loss = 0.22733655\n",
      "Iteration 151, loss = 0.01919757\n",
      "Iteration 32, loss = 0.00873037\n",
      "Iteration 143, loss = 0.01908813\n",
      "Iteration 24, loss = 0.01002366\n",
      "Iteration 30, loss = 0.00938372\n",
      "Iteration 135, loss = 0.02114613\n",
      "Iteration 8, loss = 0.25750842\n",
      "Iteration 152, loss = 0.01921909\n",
      "Iteration 11, loss = 0.20510595\n",
      "Iteration 17, loss = 0.05590297\n",
      "Iteration 17, loss = 0.21269907\n",
      "Iteration 144, loss = 0.01918490\n",
      "Iteration 33, loss = 0.00865825\n",
      "Iteration 136, loss = 0.02101213\n",
      "Iteration 31, loss = 0.00927204\n",
      "Iteration 25, loss = 0.01016801\n",
      "Iteration 153, loss = 0.01903390\n",
      "Iteration 9, loss = 0.23339239\n",
      "Iteration 12, loss = 0.20886607\n",
      "Iteration 145, loss = 0.01905123\n",
      "Iteration 137, loss = 0.02091339\n",
      "Iteration 18, loss = 0.05497427\n",
      "Iteration 18, loss = 0.20516424\n",
      "Iteration 154, loss = 0.01907250\n",
      "Iteration 34, loss = 0.00851647\n",
      "Iteration 26, loss = 0.00996512\n",
      "Iteration 32, loss = 0.00925061\n",
      "Iteration 146, loss = 0.01905122\n",
      "Iteration 10, loss = 0.21253069\n",
      "Iteration 138, loss = 0.02077777\n",
      "Iteration 13, loss = 0.21876695\n",
      "Iteration 155, loss = 0.01891364\n",
      "Iteration 19, loss = 0.05134809\n",
      "Iteration 19, loss = 0.22800680\n",
      "Iteration 35, loss = 0.00835620\n",
      "Iteration 147, loss = 0.01899345\n",
      "Iteration 27, loss = 0.00983522\n",
      "Iteration 33, loss = 0.00906276\n",
      "Iteration 156, loss = 0.01891276\n",
      "Iteration 139, loss = 0.02098892\n",
      "Iteration 11, loss = 0.20955929\n",
      "Iteration 14, loss = 0.20845349\n",
      "Iteration 20, loss = 0.05005713\n",
      "Iteration 148, loss = 0.01890256\n",
      "Iteration 20, loss = 0.24010784\n",
      "Iteration 157, loss = 0.01897770\n",
      "Iteration 36, loss = 0.00836651\n",
      "Iteration 140, loss = 0.02300492\n",
      "Iteration 28, loss = 0.00971175\n",
      "Iteration 34, loss = 0.00927348\n",
      "Iteration 12, loss = 0.22434136\n",
      "Iteration 149, loss = 0.01886546\n",
      "Iteration 15, loss = 0.21588884\n",
      "Iteration 158, loss = 0.01874506\n",
      "Iteration 21, loss = 0.05041971\n",
      "Iteration 141, loss = 0.02208505\n",
      "Iteration 21, loss = 0.21837277\n",
      "Iteration 37, loss = 0.00834936\n",
      "Iteration 29, loss = 0.00997181\n",
      "Iteration 35, loss = 0.00904153\n",
      "Iteration 150, loss = 0.01883782\n",
      "Iteration 159, loss = 0.01875922\n",
      "Iteration 13, loss = 0.22595056\n",
      "Iteration 142, loss = 0.02149138\n",
      "Iteration 16, loss = 0.19678476\n",
      "Iteration 22, loss = 0.04670936\n",
      "Iteration 22, loss = 0.18905324\n",
      "Iteration 160, loss = 0.01869232\n",
      "Iteration 151, loss = 0.01882726\n",
      "Iteration 38, loss = 0.00824262\n",
      "Iteration 30, loss = 0.00974469\n",
      "Iteration 36, loss = 0.00894093\n",
      "Iteration 143, loss = 0.02111354\n",
      "Iteration 14, loss = 0.22235432\n",
      "Iteration 161, loss = 0.01863369\n",
      "Iteration 17, loss = 0.18546430\n",
      "Iteration 152, loss = 0.01879074\n",
      "Iteration 23, loss = 0.04590551\n",
      "Iteration 23, loss = 0.19669199\n",
      "Iteration 39, loss = 0.00817540\n",
      "Iteration 144, loss = 0.02065914\n",
      "Iteration 31, loss = 0.00956930\n",
      "Iteration 162, loss = 0.01861163\n",
      "Iteration 37, loss = 0.00872878\n",
      "Iteration 153, loss = 0.01856907\n",
      "Iteration 15, loss = 0.22277562\n",
      "Iteration 18, loss = 0.18566932\n",
      "Iteration 24, loss = 0.05391434\n",
      "Iteration 24, loss = 0.20702577\n",
      "Iteration 145, loss = 0.02033114\n",
      "Iteration 163, loss = 0.01858592\n",
      "Iteration 40, loss = 0.00800592\n",
      "Iteration 154, loss = 0.01852677\n",
      "Iteration 32, loss = 0.00931557\n",
      "Iteration 38, loss = 0.00878563\n",
      "Iteration 16, loss = 0.21404168\n",
      "Iteration 164, loss = 0.01853170\n",
      "Iteration 19, loss = 0.20000238\n",
      "Iteration 146, loss = 0.02021127\n",
      "Iteration 25, loss = 0.07180658\n",
      "Iteration 155, loss = 0.01841509\n",
      "Iteration 25, loss = 0.20843756\n",
      "Iteration 41, loss = 0.00803243\n",
      "Iteration 33, loss = 0.00926899\n",
      "Iteration 165, loss = 0.01854422\n",
      "Iteration 39, loss = 0.00886040\n",
      "Iteration 147, loss = 0.02004931\n",
      "Iteration 17, loss = 0.24837015\n",
      "Iteration 156, loss = 0.01849278\n",
      "Iteration 20, loss = 0.20006900\n",
      "Iteration 26, loss = 0.06458357\n",
      "Iteration 26, loss = 0.18475380\n",
      "Iteration 166, loss = 0.01875380\n",
      "Iteration 42, loss = 0.00815976\n",
      "Iteration 148, loss = 0.01992762\n",
      "Iteration 34, loss = 0.00900096\n",
      "Iteration 40, loss = 0.00864495\n",
      "Iteration 157, loss = 0.01834505\n",
      "Iteration 18, loss = 0.22022294\n",
      "Iteration 21, loss = 0.20280004\n",
      "Iteration 167, loss = 0.01874325\n",
      "Iteration 27, loss = 0.06130178\n",
      "Iteration 27, loss = 0.19180572\n",
      "Iteration 149, loss = 0.01986681\n",
      "Iteration 158, loss = 0.01825628\n",
      "Iteration 43, loss = 0.00787237\n",
      "Iteration 35, loss = 0.00920658\n",
      "Iteration 41, loss = 0.00864914\n",
      "Iteration 168, loss = 0.01868425\n",
      "Iteration 19, loss = 0.20090425\n",
      "Iteration 22, loss = 0.17929318\n",
      "Iteration 150, loss = 0.01978876\n",
      "Iteration 159, loss = 0.01827863\n",
      "Iteration 28, loss = 0.06364264\n",
      "Iteration 28, loss = 0.21210085\n",
      "Iteration 169, loss = 0.01857986\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 44, loss = 0.00775104\n",
      "[CV 3/3; 28/36] END alpha=0.01, hidden_layer_sizes=(100,), learning_rate_init=0.001;, score=0.909 total time= 8.1min\n",
      "Iteration 36, loss = 0.00904251\n",
      "Iteration 42, loss = 0.00853547\n",
      "Iteration 160, loss = 0.01826655\n",
      "Iteration 151, loss = 0.01970019\n",
      "Iteration 20, loss = 0.20455281\n",
      "Iteration 23, loss = 0.17179176\n",
      "Iteration 29, loss = 0.05810493\n",
      "Iteration 29, loss = 0.19877582\n",
      "Iteration 45, loss = 0.00785811\n",
      "Iteration 161, loss = 0.01834988\n",
      "Iteration 37, loss = 0.00955959\n",
      "Iteration 152, loss = 0.01988157\n",
      "Iteration 43, loss = 0.00875999\n",
      "Iteration 21, loss = 0.20602663\n",
      "Iteration 24, loss = 0.18624875\n",
      "Iteration 30, loss = 0.05563391\n",
      "Iteration 162, loss = 0.01814792\n",
      "Iteration 30, loss = 0.18628560\n",
      "Iteration 153, loss = 0.01978320\n",
      "Iteration 46, loss = 0.00790656\n",
      "Iteration 38, loss = 0.00893925\n",
      "Iteration 44, loss = 0.00867090\n",
      "Iteration 22, loss = 0.19979593\n",
      "Iteration 163, loss = 0.01804618\n",
      "Iteration 25, loss = 0.17909792\n",
      "Iteration 154, loss = 0.01975759\n",
      "Iteration 31, loss = 0.05239143\n",
      "Iteration 31, loss = 0.19103732\n",
      "Iteration 47, loss = 0.00778505\n",
      "Iteration 39, loss = 0.00872689\n",
      "Iteration 45, loss = 0.00853371\n",
      "Iteration 164, loss = 0.01800654\n",
      "Iteration 23, loss = 0.20879027\n",
      "Iteration 155, loss = 0.01972008\n",
      "Iteration 26, loss = 0.16904155\n",
      "Iteration 32, loss = 0.05222528\n",
      "Iteration 32, loss = 0.17640357\n",
      "Iteration 165, loss = 0.01818514\n",
      "Iteration 48, loss = 0.00816745\n",
      "Iteration 40, loss = 0.00857062\n",
      "Iteration 46, loss = 0.00837474\n",
      "Iteration 156, loss = 0.01958957\n",
      "Iteration 24, loss = 0.19799711\n",
      "Iteration 27, loss = 0.14874184\n",
      "Iteration 166, loss = 0.01806918\n",
      "Iteration 33, loss = 0.04864233\n",
      "Iteration 33, loss = 0.19025251\n",
      "Iteration 157, loss = 0.01949045\n",
      "Iteration 49, loss = 0.00758022\n",
      "Iteration 41, loss = 0.00841900\n",
      "Iteration 47, loss = 0.00832647\n",
      "Iteration 167, loss = 0.01795781\n",
      "Iteration 25, loss = 0.20643455\n",
      "Iteration 28, loss = 0.15694929\n",
      "Iteration 34, loss = 0.04844588\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 158, loss = 0.01939494\n",
      "[CV 3/3; 35/36] END alpha=0.01, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.01;, score=0.882 total time= 2.3min\n",
      "Iteration 34, loss = 0.20261873\n",
      "Iteration 50, loss = 0.00750751\n",
      "Iteration 42, loss = 0.00849921\n",
      "Iteration 168, loss = 0.01788162\n",
      "Iteration 48, loss = 0.00889730\n",
      "Iteration 26, loss = 0.18832232\n",
      "Iteration 159, loss = 0.01922752\n",
      "Iteration 29, loss = 0.17672420\n",
      "Iteration 169, loss = 0.01781161\n",
      "Iteration 35, loss = 0.20830789\n",
      "Iteration 51, loss = 0.00769055\n",
      "Iteration 43, loss = 0.00833658\n",
      "Iteration 49, loss = 0.00839492\n",
      "Iteration 160, loss = 0.01924169\n",
      "Iteration 27, loss = 0.21116536\n",
      "Iteration 30, loss = 0.18158625\n",
      "Iteration 170, loss = 0.01769385\n",
      "Iteration 36, loss = 0.19046625\n",
      "Iteration 161, loss = 0.01993147\n",
      "Iteration 52, loss = 0.00743312\n",
      "Iteration 44, loss = 0.00822502\n",
      "Iteration 50, loss = 0.00812772\n",
      "Iteration 171, loss = 0.01764627\n",
      "Iteration 28, loss = 0.20515810\n",
      "Iteration 31, loss = 0.17723007\n",
      "Iteration 162, loss = 0.01958475\n",
      "Iteration 37, loss = 0.16680729\n",
      "Iteration 172, loss = 0.01764341\n",
      "Iteration 53, loss = 0.00745196\n",
      "Iteration 45, loss = 0.00867916\n",
      "Iteration 51, loss = 0.00793185\n",
      "Iteration 32, loss = 0.17775138\n",
      "Iteration 29, loss = 0.21296734\n",
      "Iteration 163, loss = 0.01946708\n",
      "Iteration 173, loss = 0.01772207\n",
      "Iteration 38, loss = 0.16386034\n",
      "Iteration 54, loss = 0.00745615\n",
      "Iteration 46, loss = 0.00838266\n",
      "Iteration 52, loss = 0.00784312\n",
      "Iteration 164, loss = 0.01931283\n",
      "Iteration 174, loss = 0.01768128\n",
      "Iteration 33, loss = 0.16968493\n",
      "Iteration 30, loss = 0.19402390\n",
      "Iteration 39, loss = 0.18845501\n",
      "Iteration 55, loss = 0.00722314\n",
      "Iteration 165, loss = 0.01920526\n",
      "Iteration 47, loss = 0.00815715\n",
      "Iteration 175, loss = 0.01767785\n",
      "Iteration 53, loss = 0.00796574\n",
      "Iteration 34, loss = 0.19336014\n",
      "Iteration 31, loss = 0.19980665\n",
      "Iteration 40, loss = 0.20256990\n",
      "Iteration 166, loss = 0.01899909\n",
      "Iteration 176, loss = 0.01775233\n",
      "Iteration 56, loss = 0.00736867\n",
      "Iteration 48, loss = 0.00835513\n",
      "Iteration 54, loss = 0.00800977\n",
      "Iteration 35, loss = 0.20010921\n",
      "Iteration 32, loss = 0.21636166\n",
      "Iteration 177, loss = 0.01787336\n",
      "Iteration 167, loss = 0.01896649\n",
      "Iteration 41, loss = 0.18375015\n",
      "Iteration 57, loss = 0.00708016\n",
      "Iteration 49, loss = 0.00808708\n",
      "Iteration 55, loss = 0.00772058\n",
      "Iteration 178, loss = 0.01776061\n",
      "Iteration 168, loss = 0.01911254\n",
      "Iteration 36, loss = 0.17888306\n",
      "Iteration 33, loss = 0.21406374\n",
      "Iteration 42, loss = 0.18884751\n",
      "Iteration 179, loss = 0.01787147\n",
      "Iteration 58, loss = 0.00694333\n",
      "Iteration 50, loss = 0.00799963\n",
      "Iteration 169, loss = 0.01909954\n",
      "Iteration 56, loss = 0.00805459\n",
      "Iteration 37, loss = 0.16026993\n",
      "Iteration 34, loss = 0.18716571\n",
      "Iteration 180, loss = 0.01792030\n",
      "Iteration 43, loss = 0.18433978\n",
      "Iteration 170, loss = 0.01894160\n",
      "Iteration 59, loss = 0.00695141\n",
      "Iteration 51, loss = 0.00797422\n",
      "Iteration 57, loss = 0.00811718\n",
      "Iteration 38, loss = 0.17435518\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 181, loss = 0.01798114\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 35, loss = 0.16710742\n",
      "[CV 1/3; 28/36] END alpha=0.01, hidden_layer_sizes=(100,), learning_rate_init=0.001;, score=0.906 total time= 9.0min\n",
      "[CV 2/3; 36/36] END alpha=0.01, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.1;, score=0.876 total time= 2.4min\n",
      "Iteration 171, loss = 0.01880838\n",
      "Iteration 44, loss = 0.18480591\n",
      "Iteration 60, loss = 0.00716738\n",
      "Iteration 52, loss = 0.00782928\n",
      "Iteration 58, loss = 0.00818343\n",
      "Iteration 172, loss = 0.01892026\n",
      "Iteration 36, loss = 0.18193937\n",
      "Iteration 45, loss = 0.17013245\n",
      "Iteration 61, loss = 0.00705113\n",
      "Iteration 53, loss = 0.00781906\n",
      "Iteration 173, loss = 0.01876527\n",
      "Iteration 59, loss = 0.02219145\n",
      "Iteration 37, loss = 0.19022017\n",
      "Iteration 46, loss = 0.16272267\n",
      "Iteration 62, loss = 0.00698951\n",
      "Iteration 174, loss = 0.01858169\n",
      "Iteration 54, loss = 0.00797612\n",
      "Iteration 60, loss = 0.20441820\n",
      "Iteration 38, loss = 0.20317126\n",
      "Iteration 47, loss = 0.18180107\n",
      "Iteration 175, loss = 0.01854745\n",
      "Iteration 63, loss = 0.00711701\n",
      "Iteration 55, loss = 0.00773385\n",
      "Iteration 61, loss = 0.08813585\n",
      "Iteration 39, loss = 0.21466647\n",
      "Iteration 176, loss = 0.01842900\n",
      "Iteration 48, loss = 0.18526127\n",
      "Iteration 64, loss = 0.00703287\n",
      "Iteration 56, loss = 0.00803139\n",
      "Iteration 62, loss = 0.02438752\n",
      "Iteration 177, loss = 0.01840522\n",
      "Iteration 40, loss = 0.19351010\n",
      "Iteration 49, loss = 0.16719157\n",
      "Iteration 65, loss = 0.00965341\n",
      "Iteration 178, loss = 0.01846009\n",
      "Iteration 57, loss = 0.00802561\n",
      "Iteration 63, loss = 0.01527710\n",
      "Iteration 41, loss = 0.17500351\n",
      "Iteration 50, loss = 0.18082649\n",
      "Iteration 179, loss = 0.01854819\n",
      "Iteration 66, loss = 0.13763123\n",
      "Iteration 64, loss = 0.01346180\n",
      "Iteration 58, loss = 0.00783082\n",
      "Iteration 42, loss = 0.20241438\n",
      "Iteration 180, loss = 0.01862131\n",
      "Iteration 51, loss = 0.20948446\n",
      "Iteration 67, loss = 0.07852115\n",
      "Iteration 65, loss = 0.01272555\n",
      "Iteration 59, loss = 0.00847558\n",
      "Iteration 181, loss = 0.01838327\n",
      "Iteration 43, loss = 0.20363678\n",
      "Iteration 52, loss = 0.19322851\n",
      "Iteration 68, loss = 0.02335538\n",
      "Iteration 182, loss = 0.01843562\n",
      "Iteration 66, loss = 0.01227271\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 60, loss = 0.04620433\n",
      "[CV 2/3; 34/36] END alpha=0.01, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.001;, score=0.895 total time= 4.1min\n",
      "Iteration 44, loss = 0.22153692\n",
      "Iteration 53, loss = 0.16571318\n",
      "Iteration 183, loss = 0.01874856\n",
      "Iteration 69, loss = 0.01318819\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/3; 34/36] END alpha=0.01, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.001;, score=0.900 total time= 4.3min\n",
      "Iteration 61, loss = 0.13240495\n",
      "Iteration 45, loss = 0.20464745\n",
      "Iteration 184, loss = 0.01828418\n",
      "Iteration 54, loss = 0.18655033\n",
      "Iteration 62, loss = 0.06246526\n",
      "Iteration 185, loss = 0.01825282\n",
      "Iteration 46, loss = 0.17508606\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/3; 36/36] END alpha=0.01, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.1;, score=0.884 total time= 2.5min\n",
      "Iteration 55, loss = 0.16584234\n",
      "Iteration 186, loss = 0.01842228\n",
      "Iteration 63, loss = 0.02403506\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/3; 34/36] END alpha=0.01, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.001;, score=0.910 total time= 3.8min\n",
      "Iteration 56, loss = 0.16587484\n",
      "Iteration 187, loss = 0.01815289\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/3; 28/36] END alpha=0.01, hidden_layer_sizes=(100,), learning_rate_init=0.001;, score=0.902 total time= 9.4min\n",
      "Iteration 57, loss = 0.17032873\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/3; 36/36] END alpha=0.01, hidden_layer_sizes=(150, 100, 50), learning_rate_init=0.1;, score=0.879 total time= 3.2min\n",
      "Iteration 1, loss = 0.61196869\n",
      "Iteration 2, loss = 0.40587553\n",
      "Iteration 3, loss = 0.28614111\n",
      "Iteration 4, loss = 0.21784919\n",
      "Iteration 5, loss = 0.17552005\n",
      "Iteration 6, loss = 0.14672274\n",
      "Iteration 7, loss = 0.12668925\n",
      "Iteration 8, loss = 0.11291684\n",
      "Iteration 9, loss = 0.10209209\n",
      "Iteration 10, loss = 0.09345182\n",
      "Iteration 11, loss = 0.08698769\n",
      "Iteration 12, loss = 0.08157565\n",
      "Iteration 13, loss = 0.07678755\n",
      "Iteration 14, loss = 0.07327971\n",
      "Iteration 15, loss = 0.07029126\n",
      "Iteration 16, loss = 0.06748131\n",
      "Iteration 17, loss = 0.06481191\n",
      "Iteration 18, loss = 0.06284169\n",
      "Iteration 19, loss = 0.06093961\n",
      "Iteration 20, loss = 0.05927075\n",
      "Iteration 21, loss = 0.05754803\n",
      "Iteration 22, loss = 0.05666465\n",
      "Iteration 23, loss = 0.05540402\n",
      "Iteration 24, loss = 0.05407601\n",
      "Iteration 25, loss = 0.05278515\n",
      "Iteration 26, loss = 0.05168401\n",
      "Iteration 27, loss = 0.05090353\n",
      "Iteration 28, loss = 0.04978723\n",
      "Iteration 29, loss = 0.04934642\n",
      "Iteration 30, loss = 0.04860230\n",
      "Iteration 31, loss = 0.04745422\n",
      "Iteration 32, loss = 0.04666949\n",
      "Iteration 33, loss = 0.04585560\n",
      "Iteration 34, loss = 0.04527763\n",
      "Iteration 35, loss = 0.04462696\n",
      "Iteration 36, loss = 0.04403593\n",
      "Iteration 37, loss = 0.04291962\n",
      "Iteration 38, loss = 0.04261149\n",
      "Iteration 39, loss = 0.04205512\n",
      "Iteration 40, loss = 0.04123747\n",
      "Iteration 41, loss = 0.04079168\n",
      "Iteration 42, loss = 0.04005065\n",
      "Iteration 43, loss = 0.03951510\n",
      "Iteration 44, loss = 0.03906018\n",
      "Iteration 45, loss = 0.03908799\n",
      "Iteration 46, loss = 0.03821593\n",
      "Iteration 47, loss = 0.03753535\n",
      "Iteration 48, loss = 0.03718651\n",
      "Iteration 49, loss = 0.03677497\n",
      "Iteration 50, loss = 0.03616013\n",
      "Iteration 51, loss = 0.03571609\n",
      "Iteration 52, loss = 0.03536574\n",
      "Iteration 53, loss = 0.03485562\n",
      "Iteration 54, loss = 0.03489722\n",
      "Iteration 55, loss = 0.03412898\n",
      "Iteration 56, loss = 0.03391237\n",
      "Iteration 57, loss = 0.03334160\n",
      "Iteration 58, loss = 0.03277089\n",
      "Iteration 59, loss = 0.03254602\n",
      "Iteration 60, loss = 0.03246007\n",
      "Iteration 61, loss = 0.03202154\n",
      "Iteration 62, loss = 0.03173821\n",
      "Iteration 63, loss = 0.03143805\n",
      "Iteration 64, loss = 0.03093233\n",
      "Iteration 65, loss = 0.03051568\n",
      "Iteration 66, loss = 0.03021080\n",
      "Iteration 67, loss = 0.02995564\n",
      "Iteration 68, loss = 0.02994530\n",
      "Iteration 69, loss = 0.02959688\n",
      "Iteration 70, loss = 0.02954661\n",
      "Iteration 71, loss = 0.02926955\n",
      "Iteration 72, loss = 0.02880054\n",
      "Iteration 73, loss = 0.02854704\n",
      "Iteration 74, loss = 0.02798992\n",
      "Iteration 75, loss = 0.02818054\n",
      "Iteration 76, loss = 0.02772918\n",
      "Iteration 77, loss = 0.02786945\n",
      "Iteration 78, loss = 0.02825358\n",
      "Iteration 79, loss = 0.02746570\n",
      "Iteration 80, loss = 0.02731796\n",
      "Iteration 81, loss = 0.02689556\n",
      "Iteration 82, loss = 0.02653159\n",
      "Iteration 83, loss = 0.02611466\n",
      "Iteration 84, loss = 0.02611633\n",
      "Iteration 85, loss = 0.02619894\n",
      "Iteration 86, loss = 0.02615816\n",
      "Iteration 87, loss = 0.02619499\n",
      "Iteration 88, loss = 0.02589415\n",
      "Iteration 89, loss = 0.02546571\n",
      "Iteration 90, loss = 0.02526637\n",
      "Iteration 91, loss = 0.02564363\n",
      "Iteration 92, loss = 0.02492278\n",
      "Iteration 93, loss = 0.02474008\n",
      "Iteration 94, loss = 0.02449645\n",
      "Iteration 95, loss = 0.02423972\n",
      "Iteration 96, loss = 0.02421610\n",
      "Iteration 97, loss = 0.02397798\n",
      "Iteration 98, loss = 0.02429013\n",
      "Iteration 99, loss = 0.02407376\n",
      "Iteration 100, loss = 0.02424232\n",
      "Iteration 101, loss = 0.02415912\n",
      "Iteration 102, loss = 0.02387821\n",
      "Iteration 103, loss = 0.02364858\n",
      "Iteration 104, loss = 0.02335172\n",
      "Iteration 105, loss = 0.02347471\n",
      "Iteration 106, loss = 0.02318491\n",
      "Iteration 107, loss = 0.02307549\n",
      "Iteration 108, loss = 0.02283707\n",
      "Iteration 109, loss = 0.02276133\n",
      "Iteration 110, loss = 0.02278517\n",
      "Iteration 111, loss = 0.02268869\n",
      "Iteration 112, loss = 0.02269615\n",
      "Iteration 113, loss = 0.02263454\n",
      "Iteration 114, loss = 0.02271492\n",
      "Iteration 115, loss = 0.02231845\n",
      "Iteration 116, loss = 0.02322150\n",
      "Iteration 117, loss = 0.02314238\n",
      "Iteration 118, loss = 0.02263197\n",
      "Iteration 119, loss = 0.02223687\n",
      "Iteration 120, loss = 0.02207946\n",
      "Iteration 121, loss = 0.02182351\n",
      "Iteration 122, loss = 0.02179747\n",
      "Iteration 123, loss = 0.02163128\n",
      "Iteration 124, loss = 0.02148310\n",
      "Iteration 125, loss = 0.02140065\n",
      "Iteration 126, loss = 0.02174705\n",
      "Iteration 127, loss = 0.02154417\n",
      "Iteration 128, loss = 0.02125044\n",
      "Iteration 129, loss = 0.02129161\n",
      "Iteration 130, loss = 0.02121127\n",
      "Iteration 131, loss = 0.02120073\n",
      "Iteration 132, loss = 0.02108723\n",
      "Iteration 133, loss = 0.02082625\n",
      "Iteration 134, loss = 0.02080435\n",
      "Iteration 135, loss = 0.02074059\n",
      "Iteration 136, loss = 0.02071973\n",
      "Iteration 137, loss = 0.02052225\n",
      "Iteration 138, loss = 0.02054629\n",
      "Iteration 139, loss = 0.02058768\n",
      "Iteration 140, loss = 0.02056021\n",
      "Iteration 141, loss = 0.02049583\n",
      "Iteration 142, loss = 0.02064072\n",
      "Iteration 143, loss = 0.02092461\n",
      "Iteration 144, loss = 0.02104857\n",
      "Iteration 145, loss = 0.02149408\n",
      "Iteration 146, loss = 0.02365795\n",
      "Iteration 147, loss = 0.02562706\n",
      "Iteration 148, loss = 0.03000892\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Best parameters found:  {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.86      0.90      1500\n",
      "           1       0.87      0.94      0.91      1500\n",
      "\n",
      "    accuracy                           0.90      3000\n",
      "   macro avg       0.90      0.90      0.90      3000\n",
      "weighted avg       0.90      0.90      0.90      3000\n",
      "\n",
      "Accuracy Score: 0.9016666666666666\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (100, 50), (150, 100, 50)],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "mlp = MLPClassifier(max_iter=300, random_state=21, verbose=10)\n",
    "grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, cv=3, n_jobs=-1, verbose=10)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.83      0.89      1500\n",
      "           1       0.85      0.97      0.91      1500\n",
      "\n",
      "    accuracy                           0.90      3000\n",
      "   macro avg       0.91      0.90      0.90      3000\n",
      "weighted avg       0.91      0.90      0.90      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(C=100, kernel='rbf', gamma=0.001)\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1246  254]\n",
      " [  44 1456]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=np.unique(labels), yticklabels=np.unique(labels))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV 1/5; 1/8] START C=1, gamma=0.001, kernel=rbf................................[CV 2/5; 3/8] START C=10, gamma=0.001, kernel=rbf...............................\n",
      "[CV 4/5; 1/8] START C=1, gamma=0.001, kernel=rbf................................\n",
      "[CV 1/5; 2/8] START C=1, gamma=0.0001, kernel=rbf...............................[CV 1/5; 3/8] START C=10, gamma=0.001, kernel=rbf...............................\n",
      "\n",
      "\n",
      "[CV 5/5; 1/8] START C=1, gamma=0.001, kernel=rbf................................\n",
      "[CV 3/5; 2/8] START C=1, gamma=0.0001, kernel=rbf...............................\n",
      "[CV 4/5; 2/8] START C=1, gamma=0.0001, kernel=rbf...............................\n",
      "[CV 2/5; 1/8] START C=1, gamma=0.001, kernel=rbf................................\n",
      "[CV 2/5; 2/8] START C=1, gamma=0.0001, kernel=rbf...............................\n",
      "[CV 5/5; 2/8] START C=1, gamma=0.0001, kernel=rbf...............................\n",
      "[CV 3/5; 1/8] START C=1, gamma=0.001, kernel=rbf................................\n",
      "[CV 2/5; 3/8] END C=10, gamma=0.001, kernel=rbf;, score=0.839 total time=13.9min\n",
      "[CV 1/5; 3/8] END C=10, gamma=0.001, kernel=rbf;, score=0.839 total time=13.9min\n",
      "[CV 3/5; 3/8] START C=10, gamma=0.001, kernel=rbf...............................\n",
      "[CV 4/5; 3/8] START C=10, gamma=0.001, kernel=rbf...............................\n",
      "[CV 4/5; 1/8] END .C=1, gamma=0.001, kernel=rbf;, score=0.707 total time=16.0min\n",
      "[CV 5/5; 3/8] START C=10, gamma=0.001, kernel=rbf...............................\n",
      "[CV 5/5; 1/8] END .C=1, gamma=0.001, kernel=rbf;, score=0.745 total time=16.1min\n",
      "[CV 1/5; 4/8] START C=10, gamma=0.0001, kernel=rbf..............................\n",
      "[CV 1/5; 1/8] END .C=1, gamma=0.001, kernel=rbf;, score=0.729 total time=16.1min\n",
      "[CV 2/5; 4/8] START C=10, gamma=0.0001, kernel=rbf..............................\n",
      "[CV 2/5; 1/8] END .C=1, gamma=0.001, kernel=rbf;, score=0.746 total time=16.1min\n",
      "[CV 3/5; 4/8] START C=10, gamma=0.0001, kernel=rbf..............................\n",
      "[CV 3/5; 1/8] END .C=1, gamma=0.001, kernel=rbf;, score=0.748 total time=16.2min\n",
      "[CV 4/5; 4/8] START C=10, gamma=0.0001, kernel=rbf..............................\n",
      "[CV 5/5; 2/8] END C=1, gamma=0.0001, kernel=rbf;, score=0.636 total time=17.8min\n",
      "[CV 5/5; 4/8] START C=10, gamma=0.0001, kernel=rbf..............................\n",
      "[CV 4/5; 2/8] END C=1, gamma=0.0001, kernel=rbf;, score=0.615 total time=17.8min\n",
      "[CV 2/5; 2/8] END C=1, gamma=0.0001, kernel=rbf;, score=0.625 total time=17.8min\n",
      "[CV 1/5; 5/8] START C=100, gamma=0.001, kernel=rbf..............................\n",
      "[CV 3/5; 2/8] END C=1, gamma=0.0001, kernel=rbf;, score=0.634 total time=17.8min\n",
      "[CV 2/5; 5/8] START C=100, gamma=0.001, kernel=rbf..............................\n",
      "[CV 3/5; 5/8] START C=100, gamma=0.001, kernel=rbf..............................\n",
      "[CV 1/5; 2/8] END C=1, gamma=0.0001, kernel=rbf;, score=0.613 total time=17.9min\n",
      "[CV 4/5; 5/8] START C=100, gamma=0.001, kernel=rbf..............................\n",
      "[CV 4/5; 3/8] END C=10, gamma=0.001, kernel=rbf;, score=0.829 total time=10.2min\n",
      "[CV 5/5; 5/8] START C=100, gamma=0.001, kernel=rbf..............................\n",
      "[CV 3/5; 3/8] END C=10, gamma=0.001, kernel=rbf;, score=0.846 total time=10.4min\n",
      "[CV 1/5; 6/8] START C=100, gamma=0.0001, kernel=rbf.............................\n",
      "[CV 4/5; 5/8] END C=100, gamma=0.001, kernel=rbf;, score=0.902 total time= 8.8min\n",
      "[CV 2/5; 6/8] START C=100, gamma=0.0001, kernel=rbf.............................\n",
      "[CV 1/5; 5/8] END C=100, gamma=0.001, kernel=rbf;, score=0.914 total time= 8.8min\n",
      "[CV 3/5; 6/8] START C=100, gamma=0.0001, kernel=rbf.............................\n",
      "[CV 3/5; 5/8] END C=100, gamma=0.001, kernel=rbf;, score=0.900 total time= 8.8min\n",
      "[CV 4/5; 6/8] START C=100, gamma=0.0001, kernel=rbf.............................\n",
      "[CV 2/5; 5/8] END C=100, gamma=0.001, kernel=rbf;, score=0.900 total time= 8.8min\n",
      "[CV 5/5; 6/8] START C=100, gamma=0.0001, kernel=rbf.............................\n",
      "[CV 5/5; 3/8] END C=10, gamma=0.001, kernel=rbf;, score=0.839 total time=11.5min\n",
      "[CV 1/5; 7/8] START C=1000, gamma=0.001, kernel=rbf.............................\n",
      "[CV 4/5; 4/8] END C=10, gamma=0.0001, kernel=rbf;, score=0.701 total time=14.3min\n",
      "[CV 1/5; 4/8] END C=10, gamma=0.0001, kernel=rbf;, score=0.729 total time=14.4min\n",
      "[CV 2/5; 7/8] START C=1000, gamma=0.001, kernel=rbf.............................\n",
      "[CV 3/5; 7/8] START C=1000, gamma=0.001, kernel=rbf.............................\n",
      "[CV 2/5; 4/8] END C=10, gamma=0.0001, kernel=rbf;, score=0.745 total time=14.5min\n",
      "[CV 4/5; 7/8] START C=1000, gamma=0.001, kernel=rbf.............................\n",
      "[CV 5/5; 5/8] END C=100, gamma=0.001, kernel=rbf;, score=0.914 total time= 6.5min\n",
      "[CV 5/5; 7/8] START C=1000, gamma=0.001, kernel=rbf.............................\n",
      "[CV 3/5; 4/8] END C=10, gamma=0.0001, kernel=rbf;, score=0.744 total time=14.5min\n",
      "[CV 1/5; 8/8] START C=1000, gamma=0.0001, kernel=rbf............................\n",
      "[CV 5/5; 4/8] END C=10, gamma=0.0001, kernel=rbf;, score=0.747 total time=15.0min\n",
      "[CV 2/5; 8/8] START C=1000, gamma=0.0001, kernel=rbf............................\n",
      "[CV 1/5; 7/8] END C=1000, gamma=0.001, kernel=rbf;, score=0.933 total time= 5.8min\n",
      "[CV 3/5; 8/8] START C=1000, gamma=0.0001, kernel=rbf............................\n",
      "[CV 1/5; 6/8] END C=100, gamma=0.0001, kernel=rbf;, score=0.839 total time= 9.8min\n",
      "[CV 4/5; 8/8] START C=1000, gamma=0.0001, kernel=rbf............................\n",
      "[CV 4/5; 6/8] END C=100, gamma=0.0001, kernel=rbf;, score=0.826 total time= 9.3min\n",
      "[CV 5/5; 8/8] START C=1000, gamma=0.0001, kernel=rbf............................\n",
      "[CV 2/5; 6/8] END C=100, gamma=0.0001, kernel=rbf;, score=0.841 total time= 9.4min\n",
      "[CV 3/5; 6/8] END C=100, gamma=0.0001, kernel=rbf;, score=0.844 total time= 9.4min\n",
      "[CV 5/5; 6/8] END C=100, gamma=0.0001, kernel=rbf;, score=0.835 total time= 9.4min\n",
      "[CV 2/5; 7/8] END C=1000, gamma=0.001, kernel=rbf;, score=0.913 total time= 5.6min\n",
      "[CV 3/5; 7/8] END C=1000, gamma=0.001, kernel=rbf;, score=0.922 total time= 5.7min\n",
      "[CV 4/5; 7/8] END C=1000, gamma=0.001, kernel=rbf;, score=0.916 total time= 5.6min\n",
      "[CV 5/5; 7/8] END C=1000, gamma=0.001, kernel=rbf;, score=0.922 total time= 5.6min\n",
      "[CV 1/5; 8/8] END C=1000, gamma=0.0001, kernel=rbf;, score=0.909 total time= 6.1min\n",
      "[CV 2/5; 8/8] END C=1000, gamma=0.0001, kernel=rbf;, score=0.891 total time= 4.5min\n",
      "[CV 3/5; 8/8] END C=1000, gamma=0.0001, kernel=rbf;, score=0.897 total time= 4.0min\n",
      "[CV 4/5; 8/8] END C=1000, gamma=0.0001, kernel=rbf;, score=0.896 total time= 3.4min\n",
      "[CV 5/5; 8/8] END C=1000, gamma=0.0001, kernel=rbf;, score=0.906 total time= 1.9min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"‚ñ∏\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"‚ñæ\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=SVC(), n_jobs=-1,\n",
       "             param_grid=[{&#x27;C&#x27;: [1, 10, 100, 1000], &#x27;gamma&#x27;: [0.001, 0.0001],\n",
       "                          &#x27;kernel&#x27;: [&#x27;rbf&#x27;]}],\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=5, estimator=SVC(), n_jobs=-1,\n",
       "             param_grid=[{&#x27;C&#x27;: [1, 10, 100, 1000], &#x27;gamma&#x27;: [0.001, 0.0001],\n",
       "                          &#x27;kernel&#x27;: [&#x27;rbf&#x27;]}],\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=10)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: SVC</label><div class=\"sk-toggleable__content fitted\"><pre>SVC()</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SVC()</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(), n_jobs=-1,\n",
       "             param_grid=[{'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001],\n",
       "                          'kernel': ['rbf']}],\n",
       "             scoring='accuracy', verbose=10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [\n",
    "    {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    "]\n",
    "\n",
    "svc = SVC()\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy', verbose=10, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best cross-validation score: 0.92\n"
     ]
    }
   ],
   "source": [
    "best_svm = grid_search.best_estimator_\n",
    "y_pred = best_svm.predict(X_test)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9146666666666666\n",
      "Confusion Matrix:\n",
      "[[1299  201]\n",
      " [  55 1445]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=np.unique(labels), yticklabels=np.unique(labels))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
